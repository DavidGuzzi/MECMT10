{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1421ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateid01</th>\n",
       "      <th>dateid</th>\n",
       "      <th>r3</th>\n",
       "      <th>r6</th>\n",
       "      <th>tb3ms</th>\n",
       "      <th>tb6ms</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TB3MS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TB6MS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1934-01-01</td>\n",
       "      <td>1934-03-31 23:59:59.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52666666666666664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1934-04-01</td>\n",
       "      <td>1934-06-30 23:59:59.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15333333333333332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dateid01                   dateid  r3  r6                tb3ms  tb6ms   y\n",
       "0         NaN                      NaN NaN NaN                TB3MS    NaN NaN\n",
       "1       TB6MS                      NaN NaN NaN                  NaN    NaN NaN\n",
       "2         NaN                      NaN NaN NaN                  NaN    NaN NaN\n",
       "3  1934-01-01  1934-03-31 23:59:59.999 NaN NaN  0.52666666666666664    NaN NaN\n",
       "4  1934-04-01  1934-06-30 23:59:59.999 NaN NaN  0.15333333333333332    NaN NaN"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\HP\\OneDrive\\Escritorio\\David Guzzi\\DiTella\\MEC\\Materias\\2025\\2025 1T\\[MT10] Series de Tiempo\\Clases prácticas\\Prácticas\\Práctica 4-20250516\\rp.txt\"\n",
    "df = pd.read_csv(path, delimiter=\"\\t\", decimal=\".\")\n",
    "df.drop(columns=[\"Name\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7005a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 348 entries, 0 to 347\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   dateid01  346 non-null    object \n",
      " 1   dateid    345 non-null    object \n",
      " 2   r3        98 non-null     float64\n",
      " 3   r6        98 non-null     float64\n",
      " 4   tb3ms     346 non-null    object \n",
      " 5   tb6ms     246 non-null    float64\n",
      " 6   y         97 non-null     float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 19.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb60eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   dateid01  97 non-null     object \n",
      " 1   dateid    97 non-null     object \n",
      " 2   r3        97 non-null     float64\n",
      " 3   r6        97 non-null     float64\n",
      " 4   tb3ms     97 non-null     object \n",
      " 5   tb6ms     97 non-null     float64\n",
      " 6   y         97 non-null     float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f35b106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy.stats import norm, chi2\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class ARCHModelEViewsStyle:\n",
    "    def __init__(self, y, p=4, backcast_lambda=0.7):\n",
    "        self.y = np.asarray(y, float)\n",
    "        self.n = len(y)\n",
    "        self.p = p\n",
    "        self.lam = backcast_lambda\n",
    "        self.k = 1 + 1 + p\n",
    "\n",
    "    def _compute_sigma2(self, θ):\n",
    "        μ, logω, *logα = θ\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(logα)\n",
    "        e = self.y - μ\n",
    "        σ2 = np.empty(self.n)\n",
    "        w = (1 - self.lam)*self.lam**np.arange(self.p)\n",
    "        bc = (w @ (e[:self.p][::-1]**2)) / w.sum()\n",
    "        for t in range(self.n):\n",
    "            σ2[t] = bc if t < self.p else ω + (α * e[t-self.p:t][::-1]**2).sum()\n",
    "        return σ2, e\n",
    "\n",
    "    def loglike(self, θ):\n",
    "        σ2, e = self._compute_sigma2(θ)\n",
    "        return -0.5 * np.sum(np.log(2*np.pi) + np.log(σ2) + e**2/σ2)\n",
    "\n",
    "    def fit(self, maxiter=500, tol=1e-4):\n",
    "        μ0 = self.y.mean()\n",
    "        logω0 = np.log(0.1*self.y.var())\n",
    "        logα0 = np.log(np.full(self.p, 0.1))\n",
    "        x0 = np.r_[μ0, logω0, logα0]\n",
    "\n",
    "        res = minimize(lambda th: -self.loglike(th), x0,\n",
    "                       method=\"BFGS\",\n",
    "                       options={\"maxiter\": maxiter, \"gtol\": tol, \"disp\": False})\n",
    "        self.params = res.x\n",
    "        self.llf = -res.fun\n",
    "        self.nit = res.nit\n",
    "\n",
    "        σ2, e = self._compute_sigma2(self.params)\n",
    "        self.cond_vol2 = σ2\n",
    "        self.resid = e\n",
    "        SSR = (e**2).sum()\n",
    "        σ̂ = np.sqrt(SSR/self.n)\n",
    "\n",
    "        # ==== OPG ====\n",
    "        # calcular score sólo de t=p..n-1\n",
    "        score = np.zeros((self.n - self.p, self.k))\n",
    "        μ, logω, *logα = self.params\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(logα)\n",
    "        for idx, t in enumerate(range(self.p, self.n)):\n",
    "            st = (e[t]**2/σ2[t] - 1)/(2*σ2[t])\n",
    "            score[idx, 0] = e[t]/σ2[t]\n",
    "            score[idx, 1] = st*ω\n",
    "            for j in range(self.p):\n",
    "                score[idx, 2+j] = st*α[j]*(e[t-j-1]**2)\n",
    "\n",
    "        M = score.T @ score\n",
    "        df = self.n - self.p\n",
    "        cov_log = np.linalg.inv(M/df)/df\n",
    "\n",
    "        # transformamos parámetros y errors al nivel\n",
    "        coef = np.r_[μ, ω, α]\n",
    "        se_log = np.sqrt(np.diag(cov_log))\n",
    "        se = np.empty(self.k)\n",
    "        se[0] = se_log[0]\n",
    "        se[1] = se_log[1]*ω\n",
    "        for j in range(self.p):\n",
    "            se[2+j] = se_log[2+j]*α[j]\n",
    "\n",
    "        z = coef/se\n",
    "        pval = 2*(1 - norm.cdf(np.abs(z)))\n",
    "\n",
    "        # métricas\n",
    "        R2 = 1 - SSR/((self.y - self.y.mean())**2).sum()\n",
    "        aic = -2*self.llf/self.n + 2*self.k/self.n\n",
    "        bic = -2*self.llf/self.n + np.log(self.n)*self.k/self.n\n",
    "        hqc = -2*self.llf/self.n + 2*np.log(np.log(self.n))*self.k/self.n\n",
    "        dw = durbin_watson(e)\n",
    "\n",
    "        self.summary = dict(\n",
    "            coef=coef, se=se, z=z, p=pval,\n",
    "            llf=self.llf, aic=aic, bic=bic, hqc=hqc,\n",
    "            R2=R2, SSR=SSR, sigma_hat=σ̂, dw=dw\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def print_summary(self):\n",
    "        s = self.summary\n",
    "        print(\"Dependent Variable: Y\")\n",
    "        print(\"Method: ML ARCH - Normal distribution (BFGS / Marquardt steps)\")\n",
    "        print(\"Sample (adjusted): 1960Q1 1984Q1\")\n",
    "        print(f\"Included observations: {self.n} after adjustments\")\n",
    "        print(f\"Convergence achieved after {self.nit} iterations\")\n",
    "        print(\"Coefficient covariance computed using outer product of gradients\")\n",
    "        print(f\"Presample variance: backcast (parameter = {self.lam})\")\n",
    "        eq = \"GARCH = C(2)\"\n",
    "        for i in range(self.p):\n",
    "            eq += f\" + C({3+i})*RESID(-{i+1})^2\"\n",
    "        print(eq + \"\\n\")\n",
    "\n",
    "        print(\"Variable\\tCoefficient\\tStd. Error\\tz-Statistic\\tProb.\")\n",
    "        # Primero la media\n",
    "        print(f\"C\\t{s['coef'][0]:.6f}\\t{s['se'][0]:.6f}\\t{s['z'][0]:.6f}\\t{s['p'][0]:.4f}\\n\")\n",
    "        print(\"\\tVariance Equation\")\n",
    "        print(f\"C\\t{s['coef'][1]:.6f}\\t{s['se'][1]:.6f}\\t{s['z'][1]:.6f}\\t{s['p'][1]:.4f}\")\n",
    "        for j in range(self.p):\n",
    "            lab = f\"RESID(-{j+1})^2\"\n",
    "            i = 2+j\n",
    "            print(f\"{lab}\\t{s['coef'][i]:.6f}\\t{s['se'][i]:.6f}\\t{s['z'][i]:.6f}\\t{s['p'][i]:.4f}\")\n",
    "\n",
    "        print(f\"\\nR-squared\\t{s['R2']:.6f}\\tMean dependent var\\t{self.y.mean():.6f}\")\n",
    "        print(f\"Adjusted R-squared\\t{s['R2']:.6f}\\tS.D. dependent var\\t{self.y.std():.6f}\")\n",
    "        print(f\"S.E. of regression\\t{s['sigma_hat']:.6f}\\tAkaike info criterion\\t{s['aic']:.6f}\")\n",
    "        print(f\"Sum squared resid\\t{s['SSR']:.6f}\\tSchwarz criterion\\t{s['bic']:.6f}\")\n",
    "        print(f\"Log likelihood\\t{s['llf']:.6f}\\tHannan-Quinn criter.\\t{s['hqc']:.6f}\")\n",
    "        print(f\"Durbin-Watson stat\\t{s['dw']:.6f}\")\n",
    "    \n",
    "    def resid_correlogram(self, nlags=36, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Correlograma de los residuos estandarizados al cuadrado,\n",
    "        replicando exactamente el View > Residual Diagnostics >\n",
    "        Correlogram Squared Residuals de EViews.\n",
    "        \"\"\"\n",
    "        # 1) Saca la volatilidad condicional estimada sigma_t^2\n",
    "        #    Si en tu fit() guardas self.cond_vol2, úsalo.\n",
    "        #    Aquí asumo que lo guardaste en self.cond_vol2:\n",
    "        sigma2 = self.cond_vol2  \n",
    "        # 2) Calcula residuos estandarizados\n",
    "        z = self.resid / np.sqrt(sigma2)\n",
    "        # 3) Squared standardized residuals\n",
    "        x = z**2\n",
    "        n = len(x)\n",
    "\n",
    "        # ACF omitiendo lag 0\n",
    "        ac = acf(x, nlags=nlags, fft=False)[1:]\n",
    "        # PACF por Yule-Walker\n",
    "        pac = pacf(x, nlags=nlags, method='yw')[1:]\n",
    "\n",
    "        # Q-Stat con la fórmula de EViews\n",
    "        Q = np.zeros(nlags)\n",
    "        pvals = np.zeros(nlags)\n",
    "        for m in range(1, nlags+1):\n",
    "            s = np.sum([ac[i-1]**2/(n - i) for i in range(1, m+1)])\n",
    "            Qm = n*(n+2)*s\n",
    "            Q[m-1] = Qm\n",
    "            pvals[m-1] = 1 - chi2.cdf(Qm, df=m)\n",
    "\n",
    "        # Imprime al estilo EViews\n",
    "        print(f\"Sample: 1960Q1 1984Q2   Included observations: {n}\\n\")\n",
    "        print(\"Lag\\t   AC      PAC     Q-Stat    Prob*\")\n",
    "        crit = norm.ppf(1 - alpha/2)/np.sqrt(n)\n",
    "        for i in range(nlags):\n",
    "            star_ac  = \"*\" if abs(ac[i])  > crit else \" \"\n",
    "            star_pac = \"*\" if abs(pac[i]) > crit else \" \"\n",
    "            print(f\"{i+1:>3}\\t{ac[i]:>7.3f}{star_ac}\\t\"\n",
    "                  f\"{pac[i]:>7.3f}{star_pac}\\t\"\n",
    "                  f\"{Q[i]:>7.4f}\\t{pvals[i]:>6.3f}\")\n",
    "        print(\"\\n*Probabilities may not be valid for this equation specification.\")\n",
    "\n",
    "    def plot_conditional_variance(self, dates=None):\n",
    "        \"\"\"\n",
    "        Grafica la varianza condicional estimada (sigma^2) al estilo\n",
    "        GARCH graphs > Conditional variance de EViews.\n",
    "        Si se proporciona 'dates', las usa como eje x; si no, usa índices.\n",
    "        \"\"\"\n",
    "        sigma2 = self.cond_vol2\n",
    "        n = len(sigma2)\n",
    "\n",
    "        # Preparar eje x\n",
    "        if dates is not None:\n",
    "            x = dates\n",
    "        else:\n",
    "            x = np.arange(n)\n",
    "\n",
    "        plt.figure()  # gráfico independiente\n",
    "        plt.plot(x, sigma2)\n",
    "        plt.title(\"Conditional Variance\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"σ²\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24ed50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "159de5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variable: Y\n",
      "Method: ML ARCH - Normal distribution (BFGS / Marquardt steps)\n",
      "Sample (adjusted): 1960Q1 1984Q1\n",
      "Included observations: 97 after adjustments\n",
      "Convergence achieved after 39 iterations\n",
      "Coefficient covariance computed using outer product of gradients\n",
      "Presample variance: backcast (parameter = 0.7)\n",
      "GARCH = C(2) + C(3)*RESID(-1)^2 + C(4)*RESID(-2)^2 + C(5)*RESID(-3)^2 + C(6)*RESID(-4)^2\n",
      "\n",
      "Variable\tCoefficient\tStd. Error\tz-Statistic\tProb.\n",
      "C\t0.054305\t0.012602\t4.309221\t0.0000\n",
      "\n",
      "\tVariance Equation\n",
      "C\t0.002777\t0.001923\t1.444053\t0.1487\n",
      "RESID(-1)^2\t0.628140\t0.194594\t3.227960\t0.0012\n",
      "RESID(-2)^2\t0.000000\t0.126337\t0.000000\t1.0000\n",
      "RESID(-3)^2\t0.095199\t0.140289\t0.678594\t0.4974\n",
      "RESID(-4)^2\t0.711920\t0.233344\t3.050949\t0.0023\n",
      "\n",
      "R-squared\t-0.005836\tMean dependent var\t0.073290\n",
      "Adjusted R-squared\t-0.005836\tS.D. dependent var\t0.248519\n",
      "S.E. of regression\t0.249243\tAkaike info criterion\t-0.524712\n",
      "Sum squared resid\t6.025847\tSchwarz criterion\t-0.365452\n",
      "Log likelihood\t31.448549\tHannan-Quinn criter.\t-0.460315\n",
      "Durbin-Watson stat\t1.415334\n"
     ]
    }
   ],
   "source": [
    "y = df_clean['y'].astype(float)\n",
    "model = ARCHModelEViewsStyle(y, p=4, backcast_lambda=0.7)\n",
    "res = model.fit()\n",
    "res.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "341e0220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1960Q1 1984Q2   Included observations: 97\n",
      "\n",
      "Lag\t   AC      PAC     Q-Stat    Prob*\n",
      "  1\t -0.000 \t -0.000 \t 0.0000\t 0.999\n",
      "  2\t  0.060 \t  0.061 \t 0.3661\t 0.833\n",
      "  3\t -0.010 \t -0.010 \t 0.3761\t 0.945\n",
      "  4\t  0.083 \t  0.083 \t 1.0920\t 0.896\n",
      "  5\t  0.026 \t  0.029 \t 1.1637\t 0.948\n",
      "  6\t  0.028 \t  0.020 \t 1.2444\t 0.975\n",
      "  7\t -0.133 \t -0.147 \t 3.1349\t 0.872\n",
      "  8\t -0.040 \t -0.055 \t 3.3080\t 0.914\n",
      "  9\t -0.095 \t -0.096 \t 4.2838\t 0.892\n",
      " 10\t -0.013 \t -0.018 \t 4.3037\t 0.933\n",
      " 11\t  0.062 \t  0.108 \t 4.7398\t 0.943\n",
      " 12\t -0.073 \t -0.066 \t 5.3491\t 0.945\n",
      " 13\t -0.147 \t -0.161 \t 7.8088\t 0.856\n",
      " 14\t -0.108 \t -0.141 \t 9.1522\t 0.821\n",
      " 15\t  0.066 \t  0.075 \t 9.6607\t 0.841\n",
      " 16\t  0.181 \t  0.243*\t13.5573\t 0.632\n",
      " 17\t -0.096 \t -0.110 \t14.6679\t 0.619\n",
      " 18\t -0.069 \t -0.101 \t15.2424\t 0.645\n",
      " 19\t  0.015 \t  0.005 \t15.2705\t 0.705\n",
      " 20\t -0.033 \t -0.104 \t15.4043\t 0.753\n",
      " 21\t  0.116 \t  0.124 \t17.1081\t 0.705\n",
      " 22\t -0.010 \t -0.009 \t17.1213\t 0.757\n",
      " 23\t -0.127 \t -0.171 \t19.2202\t 0.688\n",
      " 24\t  0.037 \t  0.085 \t19.4007\t 0.730\n",
      " 25\t  0.093 \t  0.189 \t20.5599\t 0.717\n",
      " 26\t  0.086 \t  0.068 \t21.5503\t 0.713\n",
      " 27\t  0.053 \t -0.064 \t21.9349\t 0.741\n",
      " 28\t -0.054 \t -0.061 \t22.3480\t 0.765\n",
      " 29\t  0.042 \t  0.165 \t22.5991\t 0.795\n",
      " 30\t -0.074 \t -0.144 \t23.3821\t 0.799\n",
      " 31\t  0.053 \t -0.035 \t23.7861\t 0.819\n",
      " 32\t  0.047 \t  0.050 \t24.1087\t 0.840\n",
      " 33\t -0.100 \t -0.118 \t25.6027\t 0.817\n",
      " 34\t -0.104 \t -0.052 \t27.2438\t 0.788\n",
      " 35\t -0.000 \t  0.036 \t27.2438\t 0.822\n",
      " 36\t  0.116 \t  0.244*\t29.3772\t 0.775\n",
      "\n",
      "*Probabilities may not be valid for this equation specification.\n"
     ]
    }
   ],
   "source": [
    "res.resid_correlogram(nlags=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b820d661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.plot_conditional_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70801e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm, chi2\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ARCHModelEViewsStyle:\n",
    "    def __init__(self, y, p=4, backcast_lambda=0.7, arch_in_mean=False):\n",
    "        self.y = np.asarray(y, float)\n",
    "        self.n = len(self.y)\n",
    "        self.p = p\n",
    "        self.lam = backcast_lambda\n",
    "        self.arch_in_mean = arch_in_mean\n",
    "        # número de parámetros: μ [+ γ si arch_in_mean] + ω + α₁…αₚ\n",
    "        self.k = 1 + (1 if arch_in_mean else 0) + 1 + p\n",
    "\n",
    "    def _compute_sigma2(self, θ):\n",
    "        # parámetros en log para ω y α\n",
    "        idx = 0\n",
    "        μ = θ[idx]; idx += 1\n",
    "        if self.arch_in_mean:\n",
    "            γ = θ[idx]; idx += 1\n",
    "        else:\n",
    "            γ = 0.0\n",
    "        logω = θ[idx]; idx += 1\n",
    "        logα = θ[idx: idx+self.p]\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(logα)\n",
    "\n",
    "        # residuos y varianza condicional\n",
    "        e = self.y - (μ + γ * np.sqrt(self._backcast(e=np.zeros(self.p)) if False else 0))\n",
    "        σ2 = np.empty(self.n)\n",
    "        # backcast inicial usando primeros p residuos (aquí zeros)\n",
    "        bc = self._backcast(e, α, ω)\n",
    "        for t in range(self.n):\n",
    "            if t < self.p:\n",
    "                σ2[t] = bc\n",
    "            else:\n",
    "                σ2[t] = ω + np.sum(α * e[t-self.p:t][::-1]**2)\n",
    "        # si arch_in_mean, recalcula e = y - μ - γ*σ\n",
    "        if self.arch_in_mean:\n",
    "            e = self.y - μ - γ*np.sqrt(σ2)\n",
    "        return σ2, e\n",
    "\n",
    "    def _backcast(self, e, α=None, ω=None):\n",
    "        # cálculo simple de backcast: promedio ponderado de primeros p resid²\n",
    "        w = (1 - self.lam) * self.lam**np.arange(self.p)\n",
    "        vals = (e[:self.p][::-1]**2) if len(e)>=self.p else np.zeros(self.p)\n",
    "        return np.dot(w, vals) / w.sum()\n",
    "\n",
    "    def loglike(self, θ):\n",
    "        σ2, e = self._compute_sigma2(θ)\n",
    "        return -0.5 * np.sum(np.log(2*np.pi) + np.log(σ2) + e**2/σ2)\n",
    "\n",
    "    def fit(self, maxiter=500, tol=1e-8):\n",
    "        # init\n",
    "        μ0 = self.y.mean()\n",
    "        x0 = [μ0]\n",
    "        if self.arch_in_mean:\n",
    "            x0.append(0.0)  # γ₀\n",
    "        x0 += [np.log(0.1*self.y.var())]  # logω\n",
    "        x0 += [np.log(0.1)] * self.p      # logα\n",
    "        x0 = np.r_[x0]\n",
    "\n",
    "        # optimiza\n",
    "        res = minimize(lambda th: -self.loglike(th), x0,\n",
    "                       method=\"BFGS\",\n",
    "                       options={\"maxiter\": maxiter, \"gtol\": tol, \"disp\": False})\n",
    "        self.params = res.x\n",
    "        self.llf    = -res.fun\n",
    "        self.nit    = res.nit\n",
    "\n",
    "        # calcula var cond y residuos\n",
    "        σ2, e = self._compute_sigma2(self.params)\n",
    "        self.cond_vol2 = σ2\n",
    "        self.resid     = e\n",
    "\n",
    "        # SSR, sigma_hat\n",
    "        SSR = np.sum(e**2)\n",
    "        sigma_hat = np.sqrt(SSR/self.n)\n",
    "\n",
    "        # OPG para covarianzas\n",
    "        score = np.zeros((self.n-self.p, self.k))\n",
    "        idx = 0\n",
    "        μ = self.params[idx]; idx+=1\n",
    "        if self.arch_in_mean:\n",
    "            γ = self.params[idx]; idx+=1\n",
    "        else:\n",
    "            γ = 0.0\n",
    "        logω = self.params[idx]; idx+=1\n",
    "        logα = self.params[idx:idx+self.p]\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(logα)\n",
    "\n",
    "        for i,t in enumerate(range(self.p, self.n)):\n",
    "            st = (e[t]**2/σ2[t] - 1)/(2*σ2[t])\n",
    "            # derivada wrt μ\n",
    "            score[i,0] = (self.y[t] - μ - γ*np.sqrt(σ2[t])) * (1/σ2[t])\n",
    "            col = 1\n",
    "            if self.arch_in_mean:\n",
    "                score[i,col] = np.sqrt(σ2[t]); col+=1\n",
    "            score[i,col] = st*ω; col+=1\n",
    "            for j in range(self.p):\n",
    "                score[i,col+j] = st*α[j]*(e[t-j-1]**2)\n",
    "        M  = score.T @ score\n",
    "        df = self.n - self.p\n",
    "        cov_log = np.linalg.inv(M/df)/df\n",
    "\n",
    "        # transforma errores\n",
    "        coef = []\n",
    "        se_log = np.sqrt(np.diag(cov_log))\n",
    "        se = np.zeros(self.k)\n",
    "\n",
    "        idx = 0\n",
    "        coef.append(μ)\n",
    "        se[idx] = se_log[idx]; idx+=1\n",
    "        if self.arch_in_mean:\n",
    "            coef.append(γ)\n",
    "            se[idx] = se_log[idx]; idx+=1\n",
    "        coef.append(ω)\n",
    "        se[idx] = se_log[idx]*ω; idx+=1\n",
    "        for j in range(self.p):\n",
    "            coef.append(α[j])\n",
    "            se[idx] = se_log[idx]*α[j]; idx+=1\n",
    "\n",
    "        coef = np.array(coef)\n",
    "        z    = coef/se\n",
    "        pval = 2*(1-norm.cdf(np.abs(z)))\n",
    "\n",
    "        # métricas\n",
    "        R2  = 1 - SSR/np.sum((self.y-self.y.mean())**2)\n",
    "        aic = -2*self.llf/self.n + 2*self.k/self.n\n",
    "        bic = -2*self.llf/self.n + np.log(self.n)*self.k/self.n\n",
    "        hqc = -2*self.llf/self.n + 2*np.log(np.log(self.n))*self.k/self.n\n",
    "        dw  = durbin_watson(e)\n",
    "\n",
    "        self.summary = dict(\n",
    "            coef=coef, se=se, z=z, p=pval,\n",
    "            llf=self.llf, aic=aic, bic=bic, hqc=hqc,\n",
    "            R2=R2, SSR=SSR, sigma_hat=sigma_hat, dw=dw\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def print_summary(self):\n",
    "        s = self.summary\n",
    "        print(\"Dependent Variable: Y\")\n",
    "        print(\"Method: ML ARCH - Normal distribution (BFGS / Marquardt steps)\")\n",
    "        print(f\"Sample (adjusted): 1960Q1 1984Q1\")\n",
    "        print(f\"Included observations: {self.n} after adjustments\")\n",
    "        print(f\"Convergence achieved after {self.nit} iterations\")\n",
    "        print(\"Coefficient covariance computed using outer product of gradients\")\n",
    "        print(f\"Presample variance: backcast (parameter = {self.lam})\\n\")\n",
    "\n",
    "        # Mean Equation\n",
    "        print(\"    Mean Equation\")\n",
    "        print(\"Variable\\tCoefficient\\tStd. Error\\tz-Statistic\\tProb.\")\n",
    "        # μ\n",
    "        print(f\"C\\t{s['coef'][0]:.6f}\\t{s['se'][0]:.6f}\\t{s['z'][0]:.6f}\\t{s['p'][0]:.4f}\")\n",
    "        # ARCH-in-Mean γ\n",
    "        print(f\"ARCH-M\\t{s['coef'][1]:.6f}\\t{s['se'][1]:.6f}\\t{s['z'][1]:.6f}\\t{s['p'][1]:.4f}\\n\")\n",
    "\n",
    "        # Variance Equation\n",
    "        print(\"    Variance Equation\")\n",
    "        print(\"Variable\\tCoefficient\\tStd. Error\\tz-Statistic\\tProb.\")\n",
    "        # ω\n",
    "        print(f\"C\\t{s['coef'][2]:.6f}\\t{s['se'][2]:.6f}\\t{s['z'][2]:.6f}\\t{s['p'][2]:.4f}\")\n",
    "        # α’s\n",
    "        for j in range(self.p):\n",
    "            idx = 3 + j\n",
    "            print(f\"RESID(-{j+1})^2\\t{s['coef'][idx]:.6f}\\t{s['se'][idx]:.6f}\\t\"\n",
    "                f\"{s['z'][idx]:.6f}\\t{s['p'][idx]:.4f}\")\n",
    "\n",
    "        # Estadísticas finales\n",
    "        print(f\"\\nR-squared\\t{s['R2']:.6f}\\tMean dep. var\\t{self.y.mean():.6f}\")\n",
    "        print(f\"Adjusted R2\\t{s['R2']:.6f}\\tS.D. dep. var\\t{self.y.std():.6f}\")\n",
    "        print(f\"S.E. regression\\t{s['sigma_hat']:.6f}\\tAIC\\t{s['aic']:.6f}\")\n",
    "        print(f\"Sum sq. resid\\t{s['SSR']:.6f}\\tBIC\\t{s['bic']:.6f}\")\n",
    "        print(f\"Log likelihood\\t{s['llf']:.6f}\\tHQC\\t{s['hqc']:.6f}\")\n",
    "        print(f\"Durbin-Watson stat\\t{s['dw']:.6f}\")\n",
    "\n",
    "\n",
    "    def plot_conditional_variance(self):\n",
    "        plt.figure(figsize=(8,3))\n",
    "        plt.plot(self.cond_vol2, label=\"σ²ₜ (condicional)\")\n",
    "        plt.title(\"Conditional Variance\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def resid_correlogram(self, nlags=36, alpha=0.05):\n",
    "        # Correlograma de z²\n",
    "        σ2 = self.cond_vol2\n",
    "        z  = self.resid/np.sqrt(σ2)\n",
    "        x  = z**2; n = len(x)\n",
    "        ac = acf(x, nlags=nlags, fft=False)[1:]\n",
    "        pac = pacf(x, nlags=nlags, method='yw')[1:]\n",
    "\n",
    "        Q, pvals = np.zeros(nlags), np.zeros(nlags)\n",
    "        for m in range(1,nlags+1):\n",
    "            s = np.sum([ac[i-1]**2/(n-i) for i in range(1,m+1)])\n",
    "            Qm = n*(n+2)*s\n",
    "            Q[m-1], pvals[m-1] = Qm, 1-chi2.cdf(Qm,df=m)\n",
    "\n",
    "        print(f\"Sample: 1960Q1 1984Q1   Observations: {n}\\n\")\n",
    "        print(\"Lag\\t   AC      PAC     Q-Stat    Prob*\")\n",
    "        crit = norm.ppf(1-alpha/2)/np.sqrt(n)\n",
    "        for i in range(nlags):\n",
    "            sa = \"*\" if abs(ac[i])>crit else \" \"\n",
    "            sp = \"*\" if abs(pac[i])>crit else \" \"\n",
    "            print(f\"{i+1:>3}\\t{ac[i]:>7.3f}{sa}\\t{pac[i]:>7.3f}{sp}\\t\"\n",
    "                  f\"{Q[i]:>7.4f}\\t{pvals[i]:>6.3f}\")\n",
    "        print(\"\\n*Probabilities may not be valid for this specification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "672ad209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variable: Y\n",
      "Method: ML ARCH - Normal distribution (BFGS / Marquardt steps)\n",
      "Sample (adjusted): 1960Q1 1984Q1\n",
      "Included observations: 97 after adjustments\n",
      "Convergence achieved after 40 iterations\n",
      "Coefficient covariance computed using outer product of gradients\n",
      "Presample variance: backcast (parameter = 0.7)\n",
      "\n",
      "    Mean Equation\n",
      "Variable\tCoefficient\tStd. Error\tz-Statistic\tProb.\n",
      "C\t0.056331\t0.012545\t4.490213\t0.0000\n",
      "ARCH-M\t-0.026476\t0.341643\t-0.077497\t0.9382\n",
      "\n",
      "    Variance Equation\n",
      "Variable\tCoefficient\tStd. Error\tz-Statistic\tProb.\n",
      "C\t0.002691\t0.001952\t1.378660\t0.1680\n",
      "RESID(-1)^2\t0.633051\t0.195986\t3.230077\t0.0012\n",
      "RESID(-2)^2\t0.000000\t0.124113\t0.000000\t1.0000\n",
      "RESID(-3)^2\t0.095093\t0.139602\t0.681174\t0.4958\n",
      "RESID(-4)^2\t0.717117\t0.240610\t2.980410\t0.0029\n",
      "\n",
      "R-squared\t-0.013107\tMean dep. var\t0.073290\n",
      "Adjusted R2\t-0.013107\tS.D. dep. var\t0.248519\n",
      "S.E. regression\t0.250142\tAIC\t-0.504452\n",
      "Sum sq. resid\t6.069409\tBIC\t-0.318648\n",
      "Log likelihood\t31.465899\tHQC\t-0.429322\n",
      "Durbin-Watson stat\t1.415094\n"
     ]
    }
   ],
   "source": [
    "y = df_clean['y'].astype(float)\n",
    "model = ARCHModelEViewsStyle(y, p=4, arch_in_mean=True)\n",
    "res   = model.fit()\n",
    "res.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7262dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm, chi2\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from datetime import datetime\n",
    "\n",
    "class ARCHModelEViewsStyle:\n",
    "    def __init__(self, y, p=4, backcast_lambda=0.7, arch_in_mean=None):\n",
    "        \"\"\"\n",
    "        arch_in_mean: None, 'Std. Dev.', 'Variance', or 'Log(Var)'\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(y, float)\n",
    "        self.n = len(y)\n",
    "        self.p = p\n",
    "        self.lam = backcast_lambda\n",
    "        self.arch_in_mean = arch_in_mean\n",
    "        # número de parámetros: μ + [γ] + logω + p·logα\n",
    "        self.k = 1 + (1 if arch_in_mean else 0) + 1 + p\n",
    "\n",
    "    def _compute_sigma2(self, θ):\n",
    "        # descodifica parámetros\n",
    "        i = 0\n",
    "        μ = θ[i]; i+=1\n",
    "        if self.arch_in_mean:\n",
    "            γ = θ[i]; i+=1\n",
    "        else:\n",
    "            γ = 0.0\n",
    "        logω = θ[i]; i+=1\n",
    "        logα = θ[i:]\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(logα)\n",
    "        # residuos preliminares\n",
    "        e0 = self.y - μ\n",
    "        # backcast...\n",
    "        w = (1 - self.lam)*self.lam**np.arange(self.p)\n",
    "        bc = (w @ (e0[:self.p][::-1]**2)) / w.sum()\n",
    "        σ2 = np.empty(self.n)\n",
    "        for t in range(self.n):\n",
    "            σ2[t] = bc if t < self.p else ω + (α * e0[t-self.p:t][::-1]**2).sum()\n",
    "        # ahora el residual final según arch_in_mean\n",
    "        if self.arch_in_mean == 'Variance':\n",
    "            e = self.y - μ - γ*σ2\n",
    "        elif self.arch_in_mean == 'Std. Dev.':\n",
    "            e = self.y - μ - γ*np.sqrt(σ2)\n",
    "        elif self.arch_in_mean == 'Log(Var)':\n",
    "            e = self.y - μ - γ*np.log(σ2)\n",
    "        else:\n",
    "            e = e0\n",
    "        return σ2, e\n",
    "\n",
    "    def loglike(self, θ):\n",
    "        σ2, e = self._compute_sigma2(θ)\n",
    "        return -0.5 * np.sum(np.log(2*np.pi) + np.log(σ2) + e**2/σ2)\n",
    "\n",
    "    def fit(self, maxiter=500, tol=1e-4):\n",
    "        # valores iniciales\n",
    "        init = [self.y.mean()]\n",
    "        if self.arch_in_mean:\n",
    "            init.append(0.0)\n",
    "        init.append(np.log(0.1*self.y.var()))\n",
    "        init += list(np.log(np.full(self.p, 0.1)))\n",
    "        x0 = np.array(init)\n",
    "\n",
    "        res = minimize(lambda th: -self.loglike(th), x0,\n",
    "                       method=\"BFGS\",\n",
    "                       options={\"maxiter\": maxiter, \"gtol\": tol, \"disp\": False})\n",
    "        self.params = res.x\n",
    "        self.llf = -res.fun\n",
    "        self.nit = res.nit\n",
    "\n",
    "        # sigma2 y residuos definitivos\n",
    "        σ2, e = self._compute_sigma2(self.params)\n",
    "        self.cond_vol2 = σ2\n",
    "        self.resid = e\n",
    "        SSR = (e**2).sum()\n",
    "        σhat = np.sqrt(SSR/self.n)\n",
    "\n",
    "        # ==== OPG para covarianza de coeficientes ====\n",
    "        score = np.zeros((self.n - self.p, self.k))\n",
    "        μ, *rest = self.params\n",
    "        idx = 1\n",
    "        if self.arch_in_mean:\n",
    "            γ = rest[0]; idx += 1\n",
    "        else:\n",
    "            γ = 0.0\n",
    "        logω = rest[idx-1]; αlogs = rest[idx:]\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(αlogs)\n",
    "        for row, t in enumerate(range(self.p, self.n)):\n",
    "            # derivada de loglike wrt μ: e[t]/σ2[t]\n",
    "            score[row, 0] = e[t]/σ2[t]\n",
    "            col = 1\n",
    "            # derivada de loglike wrt gamma:\n",
    "            if self.arch_in_mean:\n",
    "                if self.arch_in_mean=='Variance':\n",
    "                    # d e / dγ = -σ2\n",
    "                    score[row, 1] = (-σ2[t]/σ2[t] + (e[t]*0))/1.0\n",
    "                elif self.arch_in_mean=='Std. Dev.':\n",
    "                    # de/dγ = -sqrt(σ2)\n",
    "                    score[row, 1] = -np.sqrt(σ2[t]) * ( -0.5*e[t]/σ2[t] )\n",
    "                elif self.arch_in_mean=='Log(Var)':\n",
    "                    # de/dγ = -log(σ2)\n",
    "                    score[row, 1] = -np.log(σ2[t]) * ( e[t]/σ2[t] )\n",
    "                col = 2\n",
    "            else:\n",
    "                col = 1\n",
    "            # wrt logω\n",
    "            s = (e[t]**2/σ2[t] - 1)/(2*σ2[t])\n",
    "            score[row, col] = s*ω; col += 1\n",
    "            # wrt cada logα\n",
    "            for j in range(self.p):\n",
    "                score[row, col] = s*α[j]*(e[t-j-1]**2)\n",
    "                col += 1\n",
    "        M = score.T @ score\n",
    "        df = self.n - self.p\n",
    "        cov_log = np.linalg.inv(M/df)/df\n",
    "\n",
    "        # convertimos a nivel\n",
    "        coefs = []\n",
    "        se = np.zeros(self.k)\n",
    "        i = 0\n",
    "        # μ\n",
    "        coefs.append(self.params[0]); se[0] = np.sqrt(np.linalg.inv(M)[0,0])\n",
    "        i = 1\n",
    "        if self.arch_in_mean:\n",
    "            # γ\n",
    "            coefs.append(self.params[1])\n",
    "            se[1] = np.sqrt(cov_log[1,1])*np.exp(0)  # log->nivel no aplica\n",
    "            i += 1\n",
    "        # ω\n",
    "        coefs.append(np.exp(self.params[i]))\n",
    "        se[i] = np.sqrt(cov_log[i,i])*coefs[-1]; i += 1\n",
    "        # α\n",
    "        for j in range(self.p):\n",
    "            coefs.append(np.exp(self.params[i+j]))\n",
    "            se[i+j] = np.sqrt(cov_log[i+j,i+j])*coefs[-1]\n",
    "        coefs = np.array(coefs)\n",
    "\n",
    "        z = coefs/se\n",
    "        pval = 2*(1 - norm.cdf(np.abs(z)))\n",
    "\n",
    "        # métricas\n",
    "        R2 = 1 - SSR/((self.y - self.y.mean())**2).sum()\n",
    "        aic = -2*self.llf/self.n + 2*self.k/self.n\n",
    "        bic = -2*self.llf/self.n + np.log(self.n)*self.k/self.n\n",
    "        hqc = -2*self.llf/self.n + 2*np.log(np.log(self.n))*self.k/self.n\n",
    "        dw = durbin_watson(e)\n",
    "\n",
    "        self.summary = dict(\n",
    "            coef=coefs, se=se, z=z, p=pval,\n",
    "            llf=self.llf, aic=aic, bic=bic, hqc=hqc,\n",
    "            R2=R2, SSR=SSR, sigma_hat=σhat, dw=dw\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def print_summary(self):\n",
    "        s = self.summary\n",
    "        print(\"Dependent Variable: Y\")\n",
    "        print(\"Method: ML ARCH - Normal distribution (BFGS / Marquardt steps)\")\n",
    "        now = datetime.now()\n",
    "        print(f\"Date: {now:%m/%d/%y}   Time: {now:%H:%M}\")\n",
    "        print(\"Sample (adjusted): 1960Q1 1984Q1\")\n",
    "        print(f\"Included observations: {self.n} after adjustments\")\n",
    "        print(f\"Convergence achieved after {self.nit} iterations\")\n",
    "        print(\"Coefficient covariance computed using outer product of gradients\")\n",
    "        print(f\"Presample variance: backcast (parameter = {self.lam})\\n\")\n",
    "\n",
    "        # Mean Equation\n",
    "        print(\"    Mean Equation\")\n",
    "        print(\"Variable\\tCoefficient\\tStd. Error\\tz-Statistic\\tProb.\")\n",
    "        idx = 0\n",
    "        # siempre la C\n",
    "        print(f\"C\\t{s['coef'][idx]:.6f}\\t{s['se'][idx]:.6f}\\t\"\n",
    "              f\"{s['z'][idx]:.6f}\\t{s['p'][idx]:.4f}\")\n",
    "        idx += 1\n",
    "\n",
    "        # ARCH-in-Mean\n",
    "        if self.arch_in_mean == 'Std. Dev.':\n",
    "            label = \"ARCH-M (Std.Dev.)\"\n",
    "        elif self.arch_in_mean == 'Variance':\n",
    "            label = \"ARCH-M (Var)\"\n",
    "        elif self.arch_in_mean == 'Log(Var)':\n",
    "            label = \"ARCH-M (LogVar)\"\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        if label:\n",
    "            print(f\"{label}\\t{s['coef'][idx]:.6f}\\t{s['se'][idx]:.6f}\\t\"\n",
    "                  f\"{s['z'][idx]:.6f}\\t{s['p'][idx]:.4f}\")\n",
    "            idx += 1\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Variance Equation\n",
    "        print(\"    Variance Equation\")\n",
    "        print(\"Variable\\tCoefficient\\tStd. Error\\tz-Statistic\\tProb.\")\n",
    "        # ω\n",
    "        print(f\"C\\t{s['coef'][idx]:.6f}\\t{s['se'][idx]:.6f}\\t\"\n",
    "              f\"{s['z'][idx]:.6f}\\t{s['p'][idx]:.4f}\")\n",
    "        idx += 1\n",
    "        # α\n",
    "        for j in range(self.p):\n",
    "            print(f\"RESID(-{j+1})^2\\t{s['coef'][idx]:.6f}\\t\"\n",
    "                  f\"{s['se'][idx]:.6f}\\t{s['z'][idx]:.6f}\\t{s['p'][idx]:.4f}\")\n",
    "            idx += 1\n",
    "\n",
    "        # Estadísticas finales\n",
    "        print(f\"\\nR-squared\\t{s['R2']:.6f}\\tMean dep. var\\t{self.y.mean():.6f}\")\n",
    "        print(f\"Adjusted R2\\t{s['R2']:.6f}\\tS.D. dep. var\\t{self.y.std():.6f}\")\n",
    "        print(f\"S.E. of regression\\t{s['sigma_hat']:.6f}\\tAIC\\t{s['aic']:.6f}\")\n",
    "        print(f\"Sum sq. resid\\t{s['SSR']:.6f}\\tBIC\\t{s['bic']:.6f}\")\n",
    "        print(f\"Log likelihood\\t{s['llf']:.6f}\\tHQC\\t{s['hqc']:.6f}\")\n",
    "        print(f\"Durbin-Watson stat\\t{s['dw']:.6f}\")\n",
    "\n",
    "    def resid_correlogram(self, nlags=36, alpha=0.05):\n",
    "        x = (self.resid / np.sqrt(self.cond_vol2))**2\n",
    "        n = len(x)\n",
    "        ac = acf(x, nlags=nlags, fft=False)[1:]\n",
    "        pac = pacf(x, nlags=nlags, method='yw')[1:]\n",
    "        Q, pvals = np.zeros(nlags), np.zeros(nlags)\n",
    "        for m in range(1, nlags+1):\n",
    "            s = np.sum([ac[i-1]**2/(n - i) for i in range(1, m+1)])\n",
    "            Q[m-1] = n*(n+2)*s\n",
    "            pvals[m-1] = 1 - chi2.cdf(Q[m-1], df=m)\n",
    "\n",
    "        print(f\"Sample: 1960Q1 1984Q2   Included observations: {n}\\n\")\n",
    "        print(\"Lag\\t   AC      PAC     Q-Stat    Prob*\")\n",
    "        crit = norm.ppf(1 - alpha/2)/np.sqrt(n)\n",
    "        for i in range(nlags):\n",
    "            star_ac  = \"*\" if abs(ac[i])  > crit else \" \"\n",
    "            star_pac = \"*\" if abs(pac[i]) > crit else \" \"\n",
    "            print(f\"{i+1:>3}\\t{ac[i]:>7.3f}{star_ac}\\t\"\n",
    "                  f\"{pac[i]:>7.3f}{star_pac}\\t\"\n",
    "                  f\"{Q[i]:>7.4f}\\t{pvals[i]:>6.3f}\")\n",
    "        print(\"\\n*Probabilities may not be valid for this equation specification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c242749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variable: Y\n",
      "Method: ML ARCH - Normal distribution (BFGS / Marquardt steps)\n",
      "Date: 06/19/25   Time: 22:59\n",
      "Sample (adjusted): 1960Q1 1984Q1\n",
      "Included observations: 97 after adjustments\n",
      "Convergence achieved after 35 iterations\n",
      "Coefficient covariance computed using outer product of gradients\n",
      "Presample variance: backcast (parameter = 0.7)\n",
      "\n",
      "    Mean Equation\n",
      "Variable\tCoefficient\tStd. Error\tz-Statistic\tProb.\n",
      "C\t0.056331\t0.026766\t2.104576\t0.0353\n",
      "ARCH-M (Std.Dev.)\t-0.026476\t0.470307\t-0.056296\t0.9551\n",
      "\n",
      "    Variance Equation\n",
      "Variable\tCoefficient\tStd. Error\tz-Statistic\tProb.\n",
      "C\t0.002691\t0.001947\t1.382307\t0.1669\n",
      "RESID(-1)^2\t0.633052\t0.195033\t3.245864\t0.0012\n",
      "RESID(-2)^2\t0.000000\t0.125152\t0.000000\t1.0000\n",
      "RESID(-3)^2\t0.095093\t0.140871\t0.675037\t0.4997\n",
      "RESID(-4)^2\t0.717117\t0.248491\t2.885890\t0.0039\n",
      "\n",
      "R-squared\t-0.013107\tMean dep. var\t0.073290\n",
      "Adjusted R2\t-0.013107\tS.D. dep. var\t0.248519\n",
      "S.E. of regression\t0.250142\tAIC\t-0.504452\n",
      "Sum sq. resid\t6.069409\tBIC\t-0.318648\n",
      "Log likelihood\t31.465899\tHQC\t-0.429322\n",
      "Durbin-Watson stat\t1.415094\n"
     ]
    }
   ],
   "source": [
    "y = df_clean['y'].astype(float)\n",
    "model = ARCHModelEViewsStyle(y, p=4, arch_in_mean='Std. Dev.')\n",
    "res   = model.fit()\n",
    "res.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f836f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, least_squares\n",
    "from scipy.stats import norm, chi2\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from datetime import datetime\n",
    "\n",
    "class ARCHModelEViewsStyle:\n",
    "    def __init__(self, y, p=4, backcast_lambda=0.7, arch_in_mean=None):\n",
    "        \"\"\"\n",
    "        arch_in_mean: None, 'Std. Dev.', 'Variance', or 'Log(Var)'\n",
    "        Replica EViews GARCH(p,0) con ARCH-in-Mean opcional.\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(y, float)\n",
    "        self.n = len(self.y)\n",
    "        self.p = p\n",
    "        self.lam = backcast_lambda\n",
    "        self.arch_in_mean = arch_in_mean\n",
    "        # número de parámetros: μ + [γ] + logω + p·logα\n",
    "        self.k = 1 + (1 if arch_in_mean else 0) + 1 + p\n",
    "        # bounds: μ libre, γ libre, logω ∈ [log(1e-12), +∞), cada logα ∈ [log(1e-12), +∞)\n",
    "        self.bounds = [(None,None)]\n",
    "        if arch_in_mean:\n",
    "            self.bounds.append((None,None))\n",
    "        self.bounds += [(np.log(1e-12), None)] * (1 + p)\n",
    "\n",
    "    def _backcast(self, e0):\n",
    "        w = (1 - self.lam) * self.lam**np.arange(self.p)\n",
    "        return (w @ e0[:self.p][::-1]**2) / w.sum()\n",
    "\n",
    "    def _compute_sigma2(self, θ):\n",
    "        # decodifica parámetros\n",
    "        i = 0\n",
    "        μ = θ[i]; i += 1\n",
    "        γ = θ[i] if self.arch_in_mean else 0.0\n",
    "        if self.arch_in_mean: i += 1\n",
    "        logω = θ[i]; i += 1\n",
    "        logα = θ[i:]\n",
    "        ω = np.exp(logω)\n",
    "        α = np.exp(logα)\n",
    "        # residuos preliminares para backcast\n",
    "        e0 = self.y - μ\n",
    "        bc = self._backcast(e0)\n",
    "        σ2 = np.empty(self.n)\n",
    "        for t in range(self.n):\n",
    "            if t < self.p:\n",
    "                σ2[t] = bc\n",
    "            else:\n",
    "                σ2[t] = ω + np.sum(α * e0[t-self.p:t][::-1]**2)\n",
    "        # residual final según arch_in_mean\n",
    "        if self.arch_in_mean == 'Variance':\n",
    "            e = self.y - μ - γ*σ2\n",
    "        elif self.arch_in_mean == 'Std. Dev.':\n",
    "            e = self.y - μ - γ*np.sqrt(σ2)\n",
    "        elif self.arch_in_mean == 'Log(Var)':\n",
    "            e = self.y - μ - γ*np.log(σ2)\n",
    "        else:\n",
    "            e = e0\n",
    "        return σ2, e\n",
    "\n",
    "    def loglike(self, θ):\n",
    "        σ2, e = self._compute_sigma2(θ)\n",
    "        # penalizamos si aparece non-finite\n",
    "        if not np.isfinite(σ2).all() or not np.isfinite(e).all():\n",
    "            return 1e20\n",
    "        return -0.5 * np.sum(np.log(2*np.pi) + np.log(σ2) + e**2/σ2)\n",
    "\n",
    "    def fit(self, maxiter=500, tol=1e-6):\n",
    "        # 1) crear x0\n",
    "        init = [self.y.mean()]\n",
    "        if self.arch_in_mean:\n",
    "            init.append(0.0)\n",
    "        init.append(np.log(0.1*self.y.var()))\n",
    "        init += [np.log(0.1)] * self.p\n",
    "        x0 = np.array(init)\n",
    "        # 2) primer pase: BFGS\n",
    "        res = minimize(lambda th: -self.loglike(th), x0,\n",
    "                       method=\"L-BFGS-B\", bounds=self.bounds,\n",
    "                       options={\"maxiter\": maxiter, \"ftol\": tol, \"disp\": False})\n",
    "        # 3) si no converge, intentar LM sobre residuales estandarizados\n",
    "        if not res.success:\n",
    "            def std_res(th):\n",
    "                σ2, e = self._compute_sigma2(th)\n",
    "                return e/np.sqrt(σ2)\n",
    "            try:\n",
    "                lm = least_squares(std_res, res.x, method='lm',\n",
    "                                   max_nfev=10*self.k)\n",
    "                res = lm  # lm.x, lm.cost\n",
    "                self.llf = -0.5*(lm.cost*2)  # cost = 0.5*sum(r^2)\n",
    "            except:\n",
    "                pass\n",
    "        # 4) guardar resultados\n",
    "        self.params = res.x\n",
    "        if not hasattr(self, 'llf'):\n",
    "            self.llf = self.loglike(self.params)\n",
    "        self.nit = res.nit if hasattr(res, 'nit') else res.nfev\n",
    "        # 5) sigma2 y resid finales\n",
    "        σ2, e = self._compute_sigma2(self.params)\n",
    "        self.cond_vol2 = σ2\n",
    "        self.resid = e\n",
    "        SSR = np.sum(e**2)\n",
    "        # 6) covarianza por hessiana inversa\n",
    "        if hasattr(res, 'hess_inv'):\n",
    "            Hinv = res.hess_inv.todense() if hasattr(res.hess_inv, 'todense') else res.hess_inv\n",
    "            cov_log = Hinv\n",
    "        else:\n",
    "            cov_log = np.linalg.inv(-res.hess_inv)  # fallback\n",
    "        # 7) convertir a coef y se\n",
    "        coefs = []; se = np.zeros(self.k)\n",
    "        i = 0\n",
    "        # μ\n",
    "        coefs.append(self.params[0]); se[0] = np.sqrt(cov_log[0,0])\n",
    "        i = 1\n",
    "        if self.arch_in_mean:\n",
    "            coefs.append(self.params[1]); se[1] = np.sqrt(cov_log[1,1]); i+=1\n",
    "        # ω\n",
    "        wcoef = np.exp(self.params[i])\n",
    "        coefs.append(wcoef); se[i] = np.sqrt(cov_log[i,i])*wcoef; i+=1\n",
    "        # α\n",
    "        for j in range(self.p):\n",
    "            acoef = np.exp(self.params[i+j])\n",
    "            coefs.append(acoef)\n",
    "            se[i+j] = np.sqrt(cov_log[i+j,i+j])*acoef\n",
    "        coefs = np.array(coefs)\n",
    "        # z y p\n",
    "        z = coefs / se\n",
    "        pval = 2*(1 - norm.cdf(np.abs(z)))\n",
    "        # métricas\n",
    "        sigma_hat = np.sqrt(SSR/self.n)\n",
    "        R2 = 1 - SSR/np.sum((self.y-self.y.mean())**2)\n",
    "        aic = -2*self.llf + 2*self.k\n",
    "        bic = -2*self.llf + np.log(self.n)*self.k\n",
    "        hqc = -2*self.llf + 2*np.log(np.log(self.n))*self.k\n",
    "        dw = durbin_watson(e)\n",
    "        # guardar resumen\n",
    "        self.summary = dict(coef=coefs, se=se, z=z, p=pval,\n",
    "                            llf=self.llf, aic=aic, bic=bic, hqc=hqc,\n",
    "                            R2=R2, SSR=SSR, sigma_hat=sigma_hat, dw=dw)\n",
    "        return self\n",
    "\n",
    "    def print_summary(self):\n",
    "        s = self.summary\n",
    "        now = datetime.now()\n",
    "        print(\"Dependent Variable: Y\")\n",
    "        print(\"Method: ML ARCH – Normal distribution (L-BFGS-B)\")\n",
    "        print(f\"Date: {now:%m/%d/%y}   Time: {now:%H:%M}\")\n",
    "        print(f\"Included observations: {self.n}\")\n",
    "        print(f\"Iterations: {self.nit}\")\n",
    "        print(f\"Backcast λ = {self.lam}\\n\")\n",
    "        # Mean Equation\n",
    "        print(\"    Mean Equation\")\n",
    "        print(\"Variable       Coef.     Std.Err.   z-Stat    Prob.\")\n",
    "        idx = 0\n",
    "        print(f\"C {s['coef'][idx]:8.6f} {s['se'][idx]:12.6f} {s['z'][idx]:9.6f} {s['p'][idx]:6.4f}\")\n",
    "        idx += 1\n",
    "        if self.arch_in_mean:\n",
    "            label = {'Variance':'GARCH','Std. Dev.':'StdDev','Log(Var)':'LogVar'}[self.arch_in_mean]\n",
    "            print(f\"{label:10s} {s['coef'][idx]:8.6f} {s['se'][idx]:12.6f} {s['z'][idx]:9.6f} {s['p'][idx]:6.4f}\")\n",
    "            idx += 1\n",
    "        # Variance Equation\n",
    "        print(\"\\n    Variance Equation\")\n",
    "        print(\"Variable       Coef.     Std.Err.   z-Stat    Prob.\")\n",
    "        print(f\"ω {s['coef'][idx]:8.6f} {s['se'][idx]:12.6f} {s['z'][idx]:9.6f} {s['p'][idx]:6.4f}\")\n",
    "        idx += 1\n",
    "        for j in range(self.p):\n",
    "            print(f\"RESID(-{j+1})^2 {s['coef'][idx]:8.6f} {s['se'][idx]:12.6f} {s['z'][idx]:9.6f} {s['p'][idx]:6.4f}\")\n",
    "            idx += 1\n",
    "        # Final\n",
    "        print(f\"\\nLog likelihood   {s['llf']:10.6f}\")\n",
    "        print(f\"AIC              {s['aic']:10.6f}\")\n",
    "        print(f\"BIC              {s['bic']:10.6f}\")\n",
    "        print(f\"HQC              {s['hqc']:10.6f}\")\n",
    "        print(f\"R-squared        {s['R2']:10.6f}\")\n",
    "        print(f\"S.E. regression  {s['sigma_hat']:10.6f}\")\n",
    "        print(f\"Durbin-Watson    {s['dw']:10.6f}\")\n",
    "\n",
    "    def resid_correlogram(self, nlags=36, alpha=0.05):\n",
    "        x = (self.resid / np.sqrt(self.cond_vol2))**2\n",
    "        n = len(x)\n",
    "        ac = acf(x, nlags=nlags, fft=False)[1:]\n",
    "        pac = pacf(x, nlags=nlags, method='yw')[1:]\n",
    "        Q = np.zeros(nlags); pvals = np.zeros(nlags)\n",
    "        for m in range(1, nlags+1):\n",
    "            s = np.sum([ac[i-1]**2/(n - i) for i in range(1, m+1)])\n",
    "            Q[m-1] = n*(n+2)*s\n",
    "            pvals[m-1] = 1 - chi2.cdf(Q[m-1], df=m)\n",
    "        print(f\"Included observations: {n}\\n\")\n",
    "        print(\"Lag    AC      PAC     Q-Stat    Prob*\")\n",
    "        crit = norm.ppf(1 - alpha/2)/np.sqrt(n)\n",
    "        for i in range(nlags):\n",
    "            print(f\"{i+1:3d} {ac[i]:7.3f} {pac[i]:7.3f} {Q[i]:9.4f} {pvals[i]:7.3f}\")\n",
    "        print(\"\\n*Probabilities may not be valid for this eqn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cf72317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variable: Y\n",
      "Method: ML ARCH – Normal distribution (L-BFGS-B)\n",
      "Date: 06/19/25   Time: 23:12\n",
      "Included observations: 97\n",
      "Iterations: 90\n",
      "Backcast λ = 0.7\n",
      "\n",
      "    Mean Equation\n",
      "Variable       Coef.     Std.Err.   z-Stat    Prob.\n",
      "C 0.051119     0.132285  0.386433 0.6992\n",
      "GARCH      0.242231     3.201265  0.075667 0.9397\n",
      "\n",
      "    Variance Equation\n",
      "Variable       Coef.     Std.Err.   z-Stat    Prob.\n",
      "ω 0.002852     0.031986  0.089149 0.9290\n",
      "RESID(-1)^2 0.618327     1.257847  0.491576 0.6230\n",
      "RESID(-2)^2 0.000005     0.000960  0.005479 0.9956\n",
      "RESID(-3)^2 0.094672     0.452643  0.209154 0.8343\n",
      "RESID(-4)^2 0.700010     2.820107  0.248221 0.8040\n",
      "\n",
      "Log likelihood    31.673194\n",
      "AIC              -49.346387\n",
      "BIC              -31.323410\n",
      "HQC              -42.058778\n",
      "R-squared          0.003687\n",
      "S.E. regression    0.248060\n",
      "Durbin-Watson      1.336027\n"
     ]
    }
   ],
   "source": [
    "y = df_clean['y'].astype(float)\n",
    "model = ARCHModelEViewsStyle(y, p=4, arch_in_mean='Variance')\n",
    "res   = model.fit()\n",
    "res.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c858d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min:19:51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f7cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:19: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method bfgs is: gtol, norm, epsilon. The list of unsupported keyword arguments passed include: tol. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.325267\n",
      "         Iterations: 63\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 68\n",
      "ARCH(4) via ML Normal, backcast λ=0.7\n",
      "Dependent Variable: y\n",
      "Method: ML   Date: 2025-06-03   Time: 20:06:22\n",
      "Sample: 1 97   Included observations: 97\n",
      "\n",
      "Parameter Estimates:\n",
      "            coef  std.err        z    P>|z|\n",
      "C       0.054073 0.011532 4.689009 0.000003\n",
      "omega   0.002774 0.002315 1.198450 0.230742\n",
      "ARCH(1) 0.628225 0.217841 2.883871 0.003928\n",
      "ARCH(2) 0.000000 0.000025 0.000088 0.999930\n",
      "ARCH(3) 0.095218 0.096038 0.991456 0.321463\n",
      "ARCH(4) 0.712061 0.265404 2.682927 0.007298\n",
      "\n",
      "Goodness-of-fit statistics:\n",
      "R-squared            -0.005979    Mean dependent var    0.073290\n",
      "Adjusted R-squared   -0.005979    S.D. dependent var    0.249810\n",
      "S.E. of regression   0.257347    Akaike info criterion -0.526822\n",
      "Sum squared resid    6.026707    Schwarz criterion    -0.367562\n",
      "Log likelihood       31.55087    Hannan-Quinn criter.  -0.462425\n",
      "Durbin-Watson stat   1.415132\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std.err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>5.407327e-02</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>4.689009</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>omega</th>\n",
       "      <td>2.773813e-03</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>1.198450</td>\n",
       "      <td>0.230742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(1)</th>\n",
       "      <td>6.282252e-01</td>\n",
       "      <td>0.217841</td>\n",
       "      <td>2.883871</td>\n",
       "      <td>0.003928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(2)</th>\n",
       "      <td>2.221093e-09</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.999930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(3)</th>\n",
       "      <td>9.521758e-02</td>\n",
       "      <td>0.096038</td>\n",
       "      <td>0.991456</td>\n",
       "      <td>0.321463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(4)</th>\n",
       "      <td>7.120606e-01</td>\n",
       "      <td>0.265404</td>\n",
       "      <td>2.682927</td>\n",
       "      <td>0.007298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 coef   std.err         z     P>|z|\n",
       "C        5.407327e-02  0.011532  4.689009  0.000003\n",
       "omega    2.773813e-03  0.002315  1.198450  0.230742\n",
       "ARCH(1)  6.282252e-01  0.217841  2.883871  0.003928\n",
       "ARCH(2)  2.221093e-09  0.000025  0.000088  0.999930\n",
       "ARCH(3)  9.521758e-02  0.096038  0.991456  0.321463\n",
       "ARCH(4)  7.120606e-01  0.265404  2.682927  0.007298"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats\n",
    "\n",
    "class ARCH(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, p=4, backcast_lambda=0.7, **kwds):\n",
    "        self.y = np.asarray(endog, dtype=float)\n",
    "        self.n = len(self.y)\n",
    "        self.p = int(p)\n",
    "        self.lam = float(backcast_lambda)\n",
    "        super().__init__(self.y, **kwds)\n",
    "\n",
    "    def loglike(self, theta):\n",
    "        mu        = theta[0]\n",
    "        omega     = np.exp(theta[1])\n",
    "        alphas    = np.exp(theta[2:2 + self.p])\n",
    "        e         = self.y - mu\n",
    "        n         = self.n\n",
    "\n",
    "        # dynamic backcast for initial h\n",
    "        sigma2_bar = np.mean(e**2)\n",
    "        i          = np.arange(1, n+1)\n",
    "        w          = (1 - self.lam) * self.lam**(i - 1)\n",
    "        h0         = np.dot(w, e[::-1]**2) + self.lam**n * sigma2_bar\n",
    "\n",
    "        # Recursion ARCH(p)\n",
    "        h = np.empty(n)\n",
    "        h[:self.p] = h0\n",
    "        for t in range(self.p, n):\n",
    "            e_lags = e[t-self.p:t][::-1]\n",
    "            h[t]   = omega + np.dot(alphas, e_lags**2)\n",
    "\n",
    "        ll = -0.5 * np.sum(np.log(2 * np.pi) + np.log(h) + e**2 / h)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, start_params=None, **fit_kwargs):\n",
    "        if start_params is None:\n",
    "            n = self.n\n",
    "            yt = self.y\n",
    "            mu0 = sm.OLS(yt, np.ones((n,1))).fit().params[0]\n",
    "            eps = yt - mu0\n",
    "            sq  = eps**2\n",
    "            lags = np.column_stack([np.roll(sq, i) for i in range(1, self.p+1)])\n",
    "            Yv, Xv = sq[self.p:], np.column_stack([np.ones(n-self.p), lags[self.p:]])\n",
    "            params = sm.OLS(Yv, Xv).fit().params\n",
    "            omega0 = max(params[0], 1e-6)\n",
    "            alpha0 = np.maximum(params[1:], 1e-6)\n",
    "            start_params = np.r_[mu0, np.log(omega0), np.log(alpha0)]\n",
    "        return super().fit(start_params=start_params, **fit_kwargs)\n",
    "\n",
    "    def summary_eviews(self, res):\n",
    "        theta_hat = res.params\n",
    "        mu_hat    = theta_hat[0]\n",
    "        omega_hat = np.exp(theta_hat[1])\n",
    "        alpha_hat = np.exp(theta_hat[2:2+self.p])\n",
    "\n",
    "        se       = res.bse\n",
    "        se_mu    = se[0]\n",
    "        se_omega = se[1] * omega_hat\n",
    "        se_alpha = se[2:2+self.p] * alpha_hat\n",
    "\n",
    "        coefs = [mu_hat, omega_hat] + alpha_hat.tolist()\n",
    "        ses   = [se_mu, se_omega] + se_alpha.tolist()\n",
    "        z     = np.array(coefs) / np.array(ses)\n",
    "        pvals = 2 * (1 - stats.norm.cdf(np.abs(z)))\n",
    "\n",
    "        names = ['C', 'omega'] + [f'ARCH({i})' for i in range(1, self.p+1)]\n",
    "        df = pd.DataFrame({\n",
    "            'coef': coefs,\n",
    "            'std.err': ses,\n",
    "            'z': z,\n",
    "            'P>|z|': pvals\n",
    "        }, index=names)\n",
    "\n",
    "        # Goodness-of-fit statistics\n",
    "        y = self.y\n",
    "        nobs = self.n\n",
    "        resid = y - mu_hat\n",
    "        ssr = np.sum(resid**2)\n",
    "        tss = np.sum((y - np.mean(y))**2)\n",
    "        mean_y = np.mean(y)\n",
    "        sd_y = np.std(y, ddof=1)\n",
    "        df_resid = nobs - (2 + self.p)\n",
    "        ser = np.sqrt(ssr/df_resid)\n",
    "        k = len(theta_hat)\n",
    "        llf = res.llf\n",
    "        aic = -2*llf/nobs + 2*k/nobs\n",
    "        bic = -2*llf/nobs + k*np.log(nobs)/nobs\n",
    "        hqic = -2*llf/nobs + 2*k*np.log(np.log(nobs))/nobs\n",
    "        dw = np.sum(np.diff(resid)**2)/ssr\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"ARCH({self.p}) via ML Normal, backcast λ={self.lam}\")\n",
    "        print(\"Dependent Variable: y\")\n",
    "        print(f\"Method: ML   Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}   Time: {pd.Timestamp.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"Sample: 1 {nobs}   Included observations: {nobs}\")\n",
    "        print(\"\\nParameter Estimates:\")\n",
    "        print(df.to_string(float_format=\"%.6f\"))\n",
    "\n",
    "        print(\"\\nGoodness-of-fit statistics:\")\n",
    "        print(f\"R-squared            {1-ssr/tss:.6f}    Mean dependent var    {mean_y:.6f}\")\n",
    "        print(f\"Adjusted R-squared   {1-ssr/tss:.6f}    S.D. dependent var    {sd_y:.6f}\")\n",
    "        print(f\"S.E. of regression   {ser:.6f}    Akaike info criterion {aic:.6f}\")\n",
    "        print(f\"Sum squared resid    {ssr:.6f}    Schwarz criterion    {bic:.6f}\")\n",
    "        print(f\"Log likelihood       {llf:.5f}    Hannan-Quinn criter.  {hqic:.6f}\")\n",
    "        print(f\"Durbin-Watson stat   {dw:.6f}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "# Uso:\n",
    "y = df_clean['y'].astype(float)\n",
    "mod = ARCH(y, p=4, backcast_lambda=0.7)\n",
    "res = mod.fit(method='bfgs', disp=True, maxiter=1000, tol=1e-9)\n",
    "mod.summary_eviews(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49e979fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4876\\2658326651.py:255: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  h_proxy = pd.Series(sq_residuals).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variable: Y\n",
      "Method: ML ARCH - Normal distribution (GARCH(1,4))\n",
      "Date: 06/03/25   Time: 21:28\n",
      "Sample: 1 97\n",
      "Included observations: 97\n",
      "Convergence achieved after iterations\n",
      "Coefficient covariance computed using outer product of gradients\n",
      "Presample variance: backcast (parameter = 0.7)\n",
      "\n",
      "======================================================================\n",
      "Variable     Coefficient  Std. Error   z-Statistic  Prob.       \n",
      "======================================================================\n",
      "C            0.084557     0.022407     3.773633     0.0002      \n",
      "OMEGA        0.000001     0.001855     0.000539     0.9996      \n",
      "ARCH(1)      0.000001     0.064445     0.000016     1.0000      \n",
      "ARCH(2)      0.000001     0.083593     0.000012     1.0000      \n",
      "ARCH(3)      0.000001     0.204451     0.000005     1.0000      \n",
      "ARCH(4)      0.224600     0.250639     0.896109     0.3702      \n",
      "GARCH(1)     0.889245     0.061889     14.368362    0.0000      \n",
      "======================================================================\n",
      "\n",
      "R-squared            -0.002055     Mean dependent var 0.073290\n",
      "Adjusted R-squared   -0.002055     S.D. dependent var 0.249810\n",
      "S.E. of regression   0.258268     Akaike info criterion -0.056453\n",
      "Sum squared resid    6.003199     Schwarz criterion    0.129351\n",
      "Log likelihood           9.74     Hannan-Quinn criter. 0.018677\n",
      "Durbin-Watson stat   1.420674\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "\n",
    "class ARCHGARCH(GenericLikelihoodModel):\n",
    "    \"\"\"\n",
    "    Implementación de modelos ARCH/GARCH siguiendo la metodología de EViews 10\n",
    "    Soporta distribuciones Normal, t-Student y GED\n",
    "    \"\"\"\n",
    "    def __init__(self, endog, p=0, q=1, dist='normal', backcast_lambda=0.7, \n",
    "                 mean='constant', stationarity_constraint=False, **kwds):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        endog : array-like\n",
    "            Serie temporal dependiente\n",
    "        p : int\n",
    "            Orden GARCH (retardos de h_t)\n",
    "        q : int  \n",
    "            Orden ARCH (retardos de epsilon^2)\n",
    "        dist : str\n",
    "            Distribución: 'normal', 't', 'ged'\n",
    "        backcast_lambda : float\n",
    "            Parámetro lambda para backcast (default 0.7)\n",
    "        mean : str\n",
    "            Especificación de la media: 'constant', 'zero'\n",
    "        stationarity_constraint : bool\n",
    "            Si imponer restricción de estacionariedad sum(alpha + beta) < 1\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(endog, dtype=float)\n",
    "        self.n = len(self.y)\n",
    "        self.p = int(p)  # GARCH orden\n",
    "        self.q = int(q)  # ARCH orden\n",
    "        self.dist = dist.lower()\n",
    "        self.lam = float(backcast_lambda)\n",
    "        self.mean_spec = mean\n",
    "        self.stationarity = stationarity_constraint\n",
    "        \n",
    "        # Validaciones\n",
    "        if self.dist not in ['normal', 't', 'ged']:\n",
    "            raise ValueError(\"dist debe ser 'normal', 't', o 'ged'\")\n",
    "        if not (0 < self.lam < 1):\n",
    "            warnings.warn(\"backcast_lambda fuera del rango típico (0,1)\")\n",
    "            \n",
    "        super().__init__(self.y, **kwds)\n",
    "\n",
    "    def _backcast_variance(self, residuals):\n",
    "        \"\"\"\n",
    "        Cálculo de varianza inicial usando método backcast de EViews\n",
    "        Implementa la fórmula: h_0 = sum((1-λ)λ^(i-1) * ε_{n-i+1}^2) + λ^n * σ²\n",
    "        \"\"\"\n",
    "        n = len(residuals)\n",
    "        sigma2_bar = np.mean(residuals**2)\n",
    "        \n",
    "        # Pesos exponenciales decrecientes\n",
    "        i_vals = np.arange(1, n + 1)\n",
    "        weights = (1 - self.lam) * self.lam**(i_vals - 1)\n",
    "        \n",
    "        # Aplicar pesos a residuos en orden reverso\n",
    "        weighted_sum = np.dot(weights, residuals[::-1]**2)\n",
    "        \n",
    "        # Término adicional con peso λ^n\n",
    "        h0 = weighted_sum + self.lam**n * sigma2_bar\n",
    "        \n",
    "        return max(h0, 1e-8)  # Asegurar positividad\n",
    "\n",
    "    def _compute_variance(self, params, residuals):\n",
    "        \"\"\"\n",
    "        Calcula la serie de varianzas condicionales h_t\n",
    "        \"\"\"\n",
    "        n = len(residuals)\n",
    "        \n",
    "        # Extraer parámetros transformados\n",
    "        if self.mean_spec == 'constant':\n",
    "            omega = np.exp(params[1])  # Transformación log para ω > 0\n",
    "            alphas = np.exp(params[2:2+self.q]) if self.q > 0 else np.array([])\n",
    "            betas = np.exp(params[2+self.q:2+self.q+self.p]) if self.p > 0 else np.array([])\n",
    "        else:  # mean = 'zero'\n",
    "            omega = np.exp(params[0])\n",
    "            alphas = np.exp(params[1:1+self.q]) if self.q > 0 else np.array([])\n",
    "            betas = np.exp(params[1+self.q:1+self.q+self.p]) if self.p > 0 else np.array([])\n",
    "        \n",
    "        # Verificar restricción de estacionariedad si está activa\n",
    "        if self.stationarity and (np.sum(alphas) + np.sum(betas)) >= 1:\n",
    "            return np.full(n, 1e8)  # Penalizar fuertemente\n",
    "        \n",
    "        # Inicializar h_t\n",
    "        h = np.zeros(n)\n",
    "        h0 = self._backcast_variance(residuals)\n",
    "        \n",
    "        # Para los primeros max(p,q) valores, usar h0\n",
    "        max_lag = max(self.p, self.q)\n",
    "        h[:max_lag] = h0\n",
    "        \n",
    "        # Recursión GARCH: h_t = ω + Σα_i*ε²_{t-i} + Σβ_j*h_{t-j}\n",
    "        for t in range(max_lag, n):\n",
    "            h_val = omega\n",
    "            \n",
    "            # Términos ARCH (ε²_{t-i})\n",
    "            if self.q > 0:\n",
    "                for i in range(1, self.q + 1):\n",
    "                    if t - i >= 0:\n",
    "                        h_val += alphas[i-1] * residuals[t-i]**2\n",
    "            \n",
    "            # Términos GARCH (h_{t-j})\n",
    "            if self.p > 0:\n",
    "                for j in range(1, self.p + 1):\n",
    "                    if t - j >= 0:\n",
    "                        h_val += betas[j-1] * h[t-j]\n",
    "            \n",
    "            h[t] = max(h_val, 1e-8)  # Asegurar positividad\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def _log_likelihood_normal(self, residuals, h):\n",
    "        \"\"\"Log-verosimilitud para distribución normal\"\"\"\n",
    "        return -0.5 * np.sum(np.log(2 * np.pi) + np.log(h) + residuals**2 / h)\n",
    "\n",
    "    def _log_likelihood_t(self, residuals, h, nu):\n",
    "        \"\"\"Log-verosimilitud para distribución t-Student\"\"\"\n",
    "        n = len(residuals)\n",
    "        ll = 0.0\n",
    "        \n",
    "        for t in range(n):\n",
    "            ll += (stats.loggamma((nu + 1) / 2) - stats.loggamma(nu / 2) \n",
    "                   - 0.5 * np.log((nu - 2) * np.pi)\n",
    "                   - 0.5 * np.log(h[t])\n",
    "                   - ((nu + 1) / 2) * np.log(1 + residuals[t]**2 / ((nu - 2) * h[t])))\n",
    "        \n",
    "        return ll\n",
    "\n",
    "    def _log_likelihood_ged(self, residuals, h, nu):\n",
    "        \"\"\"Log-verosimilitud para GED (Generalized Error Distribution)\"\"\"\n",
    "        n = len(residuals)\n",
    "        lambda_ged = np.sqrt(2**(-2/nu) * stats.gamma(1/nu) / stats.gamma(3/nu))\n",
    "        \n",
    "        ll = 0.0\n",
    "        for t in range(n):\n",
    "            ll += (np.log(nu) - (1 + 1/nu) * np.log(2) - stats.loggamma(1/nu)\n",
    "                   - 0.5 * np.log(h[t]) - np.log(lambda_ged)\n",
    "                   - 0.5 * (np.abs(residuals[t] / (lambda_ged * np.sqrt(h[t]))))**nu)\n",
    "        \n",
    "        return ll\n",
    "\n",
    "    def loglike(self, params):\n",
    "        \"\"\"\n",
    "        Función de log-verosimilitud principal\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extraer parámetro de media\n",
    "            if self.mean_spec == 'constant':\n",
    "                mu = params[0]\n",
    "                dist_params = params[2 + self.p + self.q:]\n",
    "            else:  # mean = 'zero'\n",
    "                mu = 0.0\n",
    "                dist_params = params[1 + self.p + self.q:]\n",
    "            \n",
    "            # Calcular residuos\n",
    "            residuals = self.y - mu\n",
    "            \n",
    "            # Calcular varianzas condicionales\n",
    "            h = self._compute_variance(params, residuals)\n",
    "            \n",
    "            # Verificar validez de h\n",
    "            if np.any(h <= 0) or np.any(~np.isfinite(h)):\n",
    "                return -1e10\n",
    "            \n",
    "            # Calcular log-verosimilitud según distribución\n",
    "            if self.dist == 'normal':\n",
    "                ll = self._log_likelihood_normal(residuals, h)\n",
    "            elif self.dist == 't':\n",
    "                nu = 2.01 + np.exp(dist_params[0])  # ν > 2 para t-Student\n",
    "                ll = self._log_likelihood_t(residuals, h, nu)\n",
    "            elif self.dist == 'ged':\n",
    "                nu = 0.1 + np.exp(dist_params[0])  # ν > 0 para GED\n",
    "                ll = self._log_likelihood_ged(residuals, h, nu)\n",
    "            \n",
    "            return ll if np.isfinite(ll) else -1e10\n",
    "            \n",
    "        except:\n",
    "            return -1e10\n",
    "\n",
    "    def fit(self, start_params=None, method='bfgs', maxiter=1000, tol=1e-9, **fit_kwargs):\n",
    "        \"\"\"\n",
    "        Estimación por ML usando BFGS con pasos tipo Marquardt\n",
    "        \"\"\"\n",
    "        if start_params is None:\n",
    "            start_params = self._get_start_params()\n",
    "        \n",
    "        # Configurar optimizador con características de EViews\n",
    "        if method.lower() == 'bfgs':\n",
    "            # BFGS con pasos Marquardt-type (damping)\n",
    "            result = minimize(\n",
    "                lambda x: -self.loglike(x),\n",
    "                start_params,\n",
    "                method='L-BFGS-B',\n",
    "                options={\n",
    "                    'maxiter': maxiter,\n",
    "                    'ftol': tol,\n",
    "                    'gtol': tol\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Convertir resultado a formato statsmodels\n",
    "            class MockResult:\n",
    "                def __init__(self, result, model):\n",
    "                    self.params = result.x\n",
    "                    self.llf = -result.fun\n",
    "                    self.model = model\n",
    "                    self.nobs = model.n\n",
    "                    self.df_resid = model.n - len(result.x)\n",
    "                    self.success = result.success\n",
    "                    \n",
    "                    # Calcular matriz de covarianza usando OPG\n",
    "                    self.bse, self.cov_params_opg = model._compute_opg_covariance(self.params)\n",
    "            \n",
    "            return MockResult(result, self)\n",
    "        \n",
    "        else:\n",
    "            return super().fit(start_params=start_params, method=method, \n",
    "                             maxiter=maxiter, **fit_kwargs)\n",
    "\n",
    "    def _get_start_params(self):\n",
    "        \"\"\"Valores iniciales para los parámetros\"\"\"\n",
    "        n = self.n\n",
    "        y = self.y\n",
    "        \n",
    "        # Estimación inicial de la media\n",
    "        if self.mean_spec == 'constant':\n",
    "            mu0 = np.mean(y)\n",
    "            params = [mu0]\n",
    "        else:\n",
    "            params = []\n",
    "        \n",
    "        # Residuos iniciales\n",
    "        residuals = y - (params[0] if self.mean_spec == 'constant' else 0)\n",
    "        sq_residuals = residuals**2\n",
    "        \n",
    "        # Regresión auxiliar para parámetros de varianza\n",
    "        if self.q > 0 or self.p > 0:\n",
    "            # Crear matriz de regresores para GARCH\n",
    "            max_lag = max(self.p, self.q)\n",
    "            X_list = [np.ones(n - max_lag)]  # Constante\n",
    "            \n",
    "            # Retardos de residuos al cuadrado (ARCH)\n",
    "            for i in range(1, self.q + 1):\n",
    "                X_list.append(sq_residuals[max_lag - i:-i])\n",
    "            \n",
    "            # Retardos de varianza (aproximados por media móvil)\n",
    "            if self.p > 0:\n",
    "                h_proxy = pd.Series(sq_residuals).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "                for j in range(1, self.p + 1):\n",
    "                    X_list.append(h_proxy.iloc[max_lag - j:-j].values)\n",
    "            \n",
    "            X = np.column_stack(X_list)\n",
    "            y_reg = sq_residuals[max_lag:]\n",
    "            \n",
    "            # Regresión OLS para valores iniciales\n",
    "            try:\n",
    "                ols_params = np.linalg.lstsq(X, y_reg, rcond=None)[0]\n",
    "                omega0 = max(ols_params[0], 1e-6)\n",
    "                alpha0 = np.maximum(ols_params[1:1+self.q], 1e-6) if self.q > 0 else []\n",
    "                beta0 = np.maximum(ols_params[1+self.q:1+self.q+self.p], 1e-6) if self.p > 0 else []\n",
    "            except:\n",
    "                omega0 = np.var(residuals) * 0.1\n",
    "                alpha0 = [0.1] * self.q if self.q > 0 else []\n",
    "                beta0 = [0.8] * self.p if self.p > 0 else []\n",
    "        else:\n",
    "            omega0 = np.var(residuals)\n",
    "            alpha0, beta0 = [], []\n",
    "        \n",
    "        # Transformar a espacio log para asegurar positividad\n",
    "        params.extend([np.log(omega0)])\n",
    "        params.extend([np.log(a) for a in alpha0])\n",
    "        params.extend([np.log(b) for b in beta0])\n",
    "        \n",
    "        # Parámetros de distribución\n",
    "        if self.dist == 't':\n",
    "            params.append(np.log(8 - 2.01))  # ν ≈ 8 inicial\n",
    "        elif self.dist == 'ged':\n",
    "            params.append(np.log(1.9))  # ν ≈ 2 inicial (cercano a normal)\n",
    "        \n",
    "        return np.array(params)\n",
    "\n",
    "    def _compute_opg_covariance(self, params):\n",
    "        \"\"\"\n",
    "        Cálculo de matriz de covarianza usando OPG (Outer Product of Gradients)\n",
    "        como hace EViews por defecto\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        k = len(params)\n",
    "        \n",
    "        # Calcular gradientes por diferencias finitas\n",
    "        eps = 1e-6\n",
    "        gradients = np.zeros((n, k))\n",
    "        \n",
    "        # Log-likelihood individual para cada observación\n",
    "        def ll_individual(theta, t):\n",
    "            if self.mean_spec == 'constant':\n",
    "                mu = theta[0]\n",
    "                residual = self.y[t] - mu\n",
    "            else:\n",
    "                residual = self.y[t]\n",
    "            \n",
    "            # Calcular h_t (simplificado para una observación)\n",
    "            h_t = self._compute_variance(theta, self.y - (theta[0] if self.mean_spec == 'constant' else 0))[t]\n",
    "            \n",
    "            if self.dist == 'normal':\n",
    "                return -0.5 * (np.log(2 * np.pi) + np.log(h_t) + residual**2 / h_t)\n",
    "            else:\n",
    "                # Para simplicidad, usar aproximación normal en gradientes\n",
    "                return -0.5 * (np.log(2 * np.pi) + np.log(h_t) + residual**2 / h_t)\n",
    "        \n",
    "        # Calcular gradientes numéricamente\n",
    "        for j in range(k):\n",
    "            theta_plus = params.copy()\n",
    "            theta_minus = params.copy()\n",
    "            theta_plus[j] += eps\n",
    "            theta_minus[j] -= eps\n",
    "            \n",
    "            for t in range(n):\n",
    "                try:\n",
    "                    ll_plus = ll_individual(theta_plus, t)\n",
    "                    ll_minus = ll_individual(theta_minus, t)\n",
    "                    gradients[t, j] = (ll_plus - ll_minus) / (2 * eps)\n",
    "                except:\n",
    "                    gradients[t, j] = 0\n",
    "        \n",
    "        # Matriz OPG\n",
    "        try:\n",
    "            opg_matrix = gradients.T @ gradients\n",
    "            inv_opg = np.linalg.inv(opg_matrix)\n",
    "            se = np.sqrt(np.diag(inv_opg))\n",
    "            return se, inv_opg\n",
    "        except:\n",
    "            # Si falla, usar identidad escalada\n",
    "            se = np.ones(k) * 0.1\n",
    "            cov = np.eye(k) * 0.01\n",
    "            return se, cov\n",
    "\n",
    "    def summary_eviews(self, result):\n",
    "        \"\"\"\n",
    "        Resumen en formato similar a EViews\n",
    "        \"\"\"\n",
    "        params = result.params\n",
    "        \n",
    "        # Extraer parámetros transformados\n",
    "        param_names = []\n",
    "        param_values = []\n",
    "        param_se = []\n",
    "        \n",
    "        idx = 0\n",
    "        if self.mean_spec == 'constant':\n",
    "            param_names.append('C')\n",
    "            param_values.append(params[idx])\n",
    "            param_se.append(result.bse[idx])\n",
    "            idx += 1\n",
    "        \n",
    "        # Omega (constante de varianza)\n",
    "        omega_val = np.exp(params[idx])\n",
    "        param_names.append('OMEGA')\n",
    "        param_values.append(omega_val)\n",
    "        param_se.append(result.bse[idx] * omega_val)  # Delta method\n",
    "        idx += 1\n",
    "        \n",
    "        # Parámetros ARCH\n",
    "        for i in range(self.q):\n",
    "            alpha_val = np.exp(params[idx])\n",
    "            param_names.append(f'ARCH({i+1})')\n",
    "            param_values.append(alpha_val)\n",
    "            param_se.append(result.bse[idx] * alpha_val)\n",
    "            idx += 1\n",
    "        \n",
    "        # Parámetros GARCH\n",
    "        for j in range(self.p):\n",
    "            beta_val = np.exp(params[idx])\n",
    "            param_names.append(f'GARCH({j+1})')\n",
    "            param_values.append(beta_val)\n",
    "            param_se.append(result.bse[idx] * beta_val)\n",
    "            idx += 1\n",
    "        \n",
    "        # Parámetros de distribución\n",
    "        if self.dist == 't':\n",
    "            nu_val = 2.01 + np.exp(params[idx])\n",
    "            param_names.append('DF_PARAM')\n",
    "            param_values.append(nu_val)\n",
    "            param_se.append(result.bse[idx] * np.exp(params[idx]))\n",
    "        elif self.dist == 'ged':\n",
    "            nu_val = 0.1 + np.exp(params[idx])\n",
    "            param_names.append('GED_PARAM')\n",
    "            param_values.append(nu_val)\n",
    "            param_se.append(result.bse[idx] * np.exp(params[idx]))\n",
    "        \n",
    "        # Estadísticos z y p-valores\n",
    "        z_stats = np.array(param_values) / np.array(param_se)\n",
    "        p_values = 2 * (1 - stats.norm.cdf(np.abs(z_stats)))\n",
    "        \n",
    "        # DataFrame de resultados\n",
    "        results_df = pd.DataFrame({\n",
    "            'Coefficient': param_values,\n",
    "            'Std. Error': param_se,\n",
    "            'z-Statistic': z_stats,\n",
    "            'Prob.': p_values\n",
    "        }, index=param_names)\n",
    "        \n",
    "        # Estadísticos de bondad de ajuste\n",
    "        y = self.y\n",
    "        nobs = self.n\n",
    "        if self.mean_spec == 'constant':\n",
    "            resid = y - params[0]\n",
    "            mean_y = params[0]\n",
    "        else:\n",
    "            resid = y\n",
    "            mean_y = 0\n",
    "        \n",
    "        ssr = np.sum(resid**2)\n",
    "        tss = np.sum((y - np.mean(y))**2)\n",
    "        r_squared = 1 - ssr/tss\n",
    "        \n",
    "        k = len(params)\n",
    "        aic = -2 * result.llf / nobs + 2 * k / nobs\n",
    "        bic = -2 * result.llf / nobs + k * np.log(nobs) / nobs\n",
    "        hqic = -2 * result.llf / nobs + 2 * k * np.log(np.log(nobs)) / nobs\n",
    "        \n",
    "        # Imprimir resumen estilo EViews\n",
    "        model_name = f\"{'GARCH' if self.p > 0 else 'ARCH'}({self.p},{self.q})\" if self.p > 0 else f\"ARCH({self.q})\"\n",
    "        dist_name = {'normal': 'Normal', 't': 't-distribution', 'ged': 'GED'}[self.dist]\n",
    "        \n",
    "        print(f\"Dependent Variable: Y\")\n",
    "        print(f\"Method: ML ARCH - {dist_name} distribution ({model_name})\")\n",
    "        print(f\"Date: {pd.Timestamp.now().strftime('%m/%d/%y')}   Time: {pd.Timestamp.now().strftime('%H:%M')}\")\n",
    "        print(f\"Sample: 1 {nobs}\")\n",
    "        print(f\"Included observations: {nobs}\")\n",
    "        print(f\"Convergence achieved after iterations\")\n",
    "        print(f\"Coefficient covariance computed using outer product of gradients\")\n",
    "        print(f\"Presample variance: backcast (parameter = {self.lam})\")\n",
    "        print()\n",
    "        \n",
    "        # Tabla de coeficientes\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Variable':<12} {'Coefficient':<12} {'Std. Error':<12} {'z-Statistic':<12} {'Prob.':<12}\")\n",
    "        print(\"=\" * 70)\n",
    "        for name in results_df.index:\n",
    "            row = results_df.loc[name]\n",
    "            print(f\"{name:<12} {row['Coefficient']:<12.6f} {row['Std. Error']:<12.6f} \"\n",
    "                  f\"{row['z-Statistic']:<12.6f} {row['Prob.']:<12.4f}\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # Estadísticos de ajuste\n",
    "        print(f\"R-squared            {r_squared:>8.6f}     Mean dependent var {np.mean(y):>8.6f}\")\n",
    "        print(f\"Adjusted R-squared   {r_squared:>8.6f}     S.D. dependent var {np.std(y, ddof=1):>8.6f}\")\n",
    "        print(f\"S.E. of regression   {np.sqrt(ssr/(nobs-k)):>8.6f}     Akaike info criterion {aic:>8.6f}\")\n",
    "        print(f\"Sum squared resid    {ssr:>8.6f}     Schwarz criterion    {bic:>8.6f}\")\n",
    "        print(f\"Log likelihood       {result.llf:>8.2f}     Hannan-Quinn criter. {hqic:>8.6f}\")\n",
    "        \n",
    "        # Durbin-Watson\n",
    "        dw = np.sum(np.diff(resid)**2) / ssr\n",
    "        print(f\"Durbin-Watson stat   {dw:>8.6f}\")\n",
    "\n",
    "# # Ejemplo de uso\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Simular datos con heterocedasticidad\n",
    "#     np.random.seed(42)\n",
    "#     n = 500\n",
    "    \n",
    "#     # Modelo DGP: GARCH(1,1)\n",
    "#     omega_true, alpha_true, beta_true = 0.1, 0.15, 0.8\n",
    "#     h = np.zeros(n)\n",
    "#     y = np.zeros(n)\n",
    "#     h[0] = omega_true / (1 - alpha_true - beta_true)\n",
    "    \n",
    "#     for t in range(n):\n",
    "#         if t > 0:\n",
    "#             h[t] = omega_true + alpha_true * (y[t-1] - 0.05)**2 + beta_true * h[t-1]\n",
    "#         y[t] = 0.05 + np.sqrt(h[t]) * np.random.normal()\n",
    "    \n",
    "#     # Estimar GARCH(1,1) con distribución normal\n",
    "#     print(\"Estimando GARCH(1,1) con distribución Normal...\")\n",
    "#     model = ARCHGARCH(y, p=1, q=1, dist='normal', backcast_lambda=0.7)\n",
    "#     result = model.fit(method='bfgs', maxiter=1000)\n",
    "    \n",
    "#     if result.success:\n",
    "#         summary_df = model.summary_eviews(result)\n",
    "#     else:\n",
    "#         print(\"La optimización no convergió exitosamente\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"Estimando ARCH(2) con distribución t-Student...\")\n",
    "#     model2 = ARCHGARCH(y, p=0, q=2, dist='t', backcast_lambda=0.7)\n",
    "#     result2 = model2.fit(method='bfgs', maxiter=1000)\n",
    "    \n",
    "#     if result2.success:\n",
    "#         summary_df2 = model2.summary_eviews(result2)\n",
    "#     else:\n",
    "#         print(\"La optimización no convergió exitosamente\")\n",
    "\n",
    "\n",
    "# GARCH(1,1) con distribución normal\n",
    "model = ARCHGARCH(df_clean['y'], p=1, q=4, dist='normal')\n",
    "result = model.fit()\n",
    "model.summary_eviews(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0af587b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variable: Y\n",
      "Method: ML ARCH - Normal distribution (ARCH(4))\n",
      "Date: 06/03/25   Time: 21:33\n",
      "Sample: 1 97\n",
      "Included observations: 97\n",
      "Convergence achieved after iterations\n",
      "Coefficient covariance computed using outer product of gradients\n",
      "Presample variance: backcast (parameter = 0.7)\n",
      "\n",
      "======================================================================\n",
      "Variable     Coefficient  Std. Error   z-Statistic  Prob.       \n",
      "======================================================================\n",
      "C            0.051074     0.010384     4.918427     0.0000      \n",
      "OMEGA        0.001801     0.001505     1.197297     0.2312      \n",
      "RESID(-1)^2  0.777751     0.178281     4.362501     0.0000      \n",
      "RESID(-2)^2  -0.130620    0.073332     -1.781220    0.0749      \n",
      "RESID(-3)^2  0.186838     0.145143     1.287267     0.1980      \n",
      "RESID(-4)^2  0.858993     0.235631     3.645506     0.0003      \n",
      "======================================================================\n",
      "\n",
      "R-squared            -0.007992     Mean dependent var 0.073290\n",
      "Adjusted R-squared   -0.007992     S.D. dependent var 0.249810\n",
      "S.E. of regression   0.257604     Akaike info criterion -0.532567\n",
      "Sum squared resid    6.038763     Schwarz criterion    -0.373306\n",
      "Log likelihood          31.83     Hannan-Quinn criter. -0.468170\n",
      "Durbin-Watson stat   1.412307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>z-Statistic</th>\n",
       "      <th>Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.051074</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>4.918427</td>\n",
       "      <td>8.724226e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMEGA</th>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>1.197297</td>\n",
       "      <td>2.311909e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESID(-1)^2</th>\n",
       "      <td>0.777751</td>\n",
       "      <td>0.178281</td>\n",
       "      <td>4.362501</td>\n",
       "      <td>1.285843e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESID(-2)^2</th>\n",
       "      <td>-0.130620</td>\n",
       "      <td>0.073332</td>\n",
       "      <td>-1.781220</td>\n",
       "      <td>7.487648e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESID(-3)^2</th>\n",
       "      <td>0.186838</td>\n",
       "      <td>0.145143</td>\n",
       "      <td>1.287267</td>\n",
       "      <td>1.980013e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESID(-4)^2</th>\n",
       "      <td>0.858993</td>\n",
       "      <td>0.235631</td>\n",
       "      <td>3.645506</td>\n",
       "      <td>2.668661e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Coefficient  Std. Error  z-Statistic         Prob.\n",
       "C               0.051074    0.010384     4.918427  8.724226e-07\n",
       "OMEGA           0.001801    0.001505     1.197297  2.311909e-01\n",
       "RESID(-1)^2     0.777751    0.178281     4.362501  1.285843e-05\n",
       "RESID(-2)^2    -0.130620    0.073332    -1.781220  7.487648e-02\n",
       "RESID(-3)^2     0.186838    0.145143     1.287267  1.980013e-01\n",
       "RESID(-4)^2     0.858993    0.235631     3.645506  2.668661e-04"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "\n",
    "class ARCHGARCH(GenericLikelihoodModel):\n",
    "    \"\"\"\n",
    "    Implementación de modelos ARCH/GARCH siguiendo la metodología de EViews 10\n",
    "    Soporta distribuciones Normal, t-Student y GED\n",
    "    \"\"\"\n",
    "    def __init__(self, endog, p=0, q=1, dist='normal', backcast_lambda=0.7, \n",
    "                 mean='constant', stationarity_constraint=False, force_positive=True, **kwds):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        endog : array-like\n",
    "            Serie temporal dependiente\n",
    "        p : int\n",
    "            Orden GARCH (retardos de h_t)\n",
    "        q : int  \n",
    "            Orden ARCH (retardos de epsilon^2)\n",
    "        dist : str\n",
    "            Distribución: 'normal', 't', 'ged'\n",
    "        backcast_lambda : float\n",
    "            Parámetro lambda para backcast (default 0.7)\n",
    "        mean : str\n",
    "            Especificación de la media: 'constant', 'zero'\n",
    "        stationarity_constraint : bool\n",
    "            Si imponer restricción de estacionariedad sum(alpha + beta) < 1\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(endog, dtype=float)\n",
    "        self.n = len(self.y)\n",
    "        self.p = int(p)  # GARCH orden\n",
    "        self.q = int(q)  # ARCH orden\n",
    "        self.dist = dist.lower()\n",
    "        self.lam = float(backcast_lambda)\n",
    "        self.mean_spec = mean\n",
    "        self.stationarity = stationarity_constraint\n",
    "        self.force_positive = force_positive  # Nueva opción\n",
    "        \n",
    "        # Validaciones\n",
    "        if self.dist not in ['normal', 't', 'ged']:\n",
    "            raise ValueError(\"dist debe ser 'normal', 't', o 'ged'\")\n",
    "        if not (0 < self.lam < 1):\n",
    "            warnings.warn(\"backcast_lambda fuera del rango típico (0,1)\")\n",
    "            \n",
    "        super().__init__(self.y, **kwds)\n",
    "\n",
    "    def _backcast_variance(self, residuals):\n",
    "        \"\"\"\n",
    "        Cálculo de varianza inicial usando método backcast de EViews\n",
    "        Implementa la fórmula: h_0 = sum((1-λ)λ^(i-1) * ε_{n-i+1}^2) + λ^n * σ²\n",
    "        \"\"\"\n",
    "        n = len(residuals)\n",
    "        sigma2_bar = np.mean(residuals**2)\n",
    "        \n",
    "        # Pesos exponenciales decrecientes\n",
    "        i_vals = np.arange(1, n + 1)\n",
    "        weights = (1 - self.lam) * self.lam**(i_vals - 1)\n",
    "        \n",
    "        # Aplicar pesos a residuos en orden reverso\n",
    "        weighted_sum = np.dot(weights, residuals[::-1]**2)\n",
    "        \n",
    "        # Término adicional con peso λ^n\n",
    "        h0 = weighted_sum + self.lam**n * sigma2_bar\n",
    "        \n",
    "        return max(h0, 1e-8)  # Asegurar positividad\n",
    "\n",
    "    def _compute_variance(self, params, residuals):\n",
    "        \"\"\"\n",
    "        Calcula la serie de varianzas condicionales h_t\n",
    "        \"\"\"\n",
    "        n = len(residuals)\n",
    "        \n",
    "        # Extraer parámetros (con o sin transformación)\n",
    "        if self.mean_spec == 'constant':\n",
    "            if self.force_positive:\n",
    "                omega = np.exp(params[1])  # Transformación log para ω > 0\n",
    "                alphas = np.exp(params[2:2+self.q]) if self.q > 0 else np.array([])\n",
    "                betas = np.exp(params[2+self.q:2+self.q+self.p]) if self.p > 0 else np.array([])\n",
    "            else:\n",
    "                # EViews permite coeficientes negativos\n",
    "                omega = params[1]**2  # Solo omega debe ser positivo\n",
    "                alphas = params[2:2+self.q] if self.q > 0 else np.array([])\n",
    "                betas = params[2+self.q:2+self.q+self.p] if self.p > 0 else np.array([])\n",
    "        else:  # mean = 'zero'\n",
    "            if self.force_positive:\n",
    "                omega = np.exp(params[0])\n",
    "                alphas = np.exp(params[1:1+self.q]) if self.q > 0 else np.array([])\n",
    "                betas = np.exp(params[1+self.q:1+self.q+self.p]) if self.p > 0 else np.array([])\n",
    "            else:\n",
    "                omega = params[0]**2\n",
    "                alphas = params[1:1+self.q] if self.q > 0 else np.array([])\n",
    "                betas = params[1+self.q:1+self.q+self.p] if self.p > 0 else np.array([])\n",
    "        \n",
    "        # Verificar restricción de estacionariedad si está activa\n",
    "        if self.stationarity and (np.sum(alphas) + np.sum(betas)) >= 1:\n",
    "            return np.full(n, 1e8)  # Penalizar fuertemente\n",
    "        \n",
    "        # Inicializar h_t\n",
    "        h = np.zeros(n)\n",
    "        h0 = self._backcast_variance(residuals)\n",
    "        \n",
    "        # Para los primeros max(p,q) valores, usar h0\n",
    "        max_lag = max(self.p, self.q)\n",
    "        h[:max_lag] = h0\n",
    "        \n",
    "        # Recursión GARCH: h_t = ω + Σα_i*ε²_{t-i} + Σβ_j*h_{t-j}\n",
    "        for t in range(max_lag, n):\n",
    "            h_val = omega\n",
    "            \n",
    "            # Términos ARCH (ε²_{t-i})\n",
    "            if self.q > 0:\n",
    "                for i in range(1, self.q + 1):\n",
    "                    if t - i >= 0:\n",
    "                        h_val += alphas[i-1] * residuals[t-i]**2\n",
    "            \n",
    "            # Términos GARCH (h_{t-j})\n",
    "            if self.p > 0:\n",
    "                for j in range(1, self.p + 1):\n",
    "                    if t - j >= 0:\n",
    "                        h_val += betas[j-1] * h[t-j]\n",
    "            \n",
    "            h[t] = max(h_val, 1e-8)  # Asegurar positividad\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def _log_likelihood_normal(self, residuals, h):\n",
    "        \"\"\"Log-verosimilitud para distribución normal\"\"\"\n",
    "        return -0.5 * np.sum(np.log(2 * np.pi) + np.log(h) + residuals**2 / h)\n",
    "\n",
    "    def _log_likelihood_t(self, residuals, h, nu):\n",
    "        \"\"\"Log-verosimilitud para distribución t-Student\"\"\"\n",
    "        n = len(residuals)\n",
    "        ll = 0.0\n",
    "        \n",
    "        for t in range(n):\n",
    "            ll += (stats.loggamma((nu + 1) / 2) - stats.loggamma(nu / 2) \n",
    "                   - 0.5 * np.log((nu - 2) * np.pi)\n",
    "                   - 0.5 * np.log(h[t])\n",
    "                   - ((nu + 1) / 2) * np.log(1 + residuals[t]**2 / ((nu - 2) * h[t])))\n",
    "        \n",
    "        return ll\n",
    "\n",
    "    def _log_likelihood_ged(self, residuals, h, nu):\n",
    "        \"\"\"Log-verosimilitud para GED (Generalized Error Distribution)\"\"\"\n",
    "        n = len(residuals)\n",
    "        lambda_ged = np.sqrt(2**(-2/nu) * stats.gamma(1/nu) / stats.gamma(3/nu))\n",
    "        \n",
    "        ll = 0.0\n",
    "        for t in range(n):\n",
    "            ll += (np.log(nu) - (1 + 1/nu) * np.log(2) - stats.loggamma(1/nu)\n",
    "                   - 0.5 * np.log(h[t]) - np.log(lambda_ged)\n",
    "                   - 0.5 * (np.abs(residuals[t] / (lambda_ged * np.sqrt(h[t]))))**nu)\n",
    "        \n",
    "        return ll\n",
    "\n",
    "    def loglike(self, params):\n",
    "        \"\"\"\n",
    "        Función de log-verosimilitud principal\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extraer parámetro de media\n",
    "            if self.mean_spec == 'constant':\n",
    "                mu = params[0]\n",
    "                dist_params = params[2 + self.p + self.q:]\n",
    "            else:  # mean = 'zero'\n",
    "                mu = 0.0\n",
    "                dist_params = params[1 + self.p + self.q:]\n",
    "            \n",
    "            # Calcular residuos\n",
    "            residuals = self.y - mu\n",
    "            \n",
    "            # Calcular varianzas condicionales\n",
    "            h = self._compute_variance(params, residuals)\n",
    "            \n",
    "            # Verificar validez de h\n",
    "            if np.any(h <= 0) or np.any(~np.isfinite(h)):\n",
    "                return -1e10\n",
    "            \n",
    "            # Calcular log-verosimilitud según distribución\n",
    "            if self.dist == 'normal':\n",
    "                ll = self._log_likelihood_normal(residuals, h)\n",
    "            elif self.dist == 't':\n",
    "                nu = 2.01 + np.exp(dist_params[0])  # ν > 2 para t-Student\n",
    "                ll = self._log_likelihood_t(residuals, h, nu)\n",
    "            elif self.dist == 'ged':\n",
    "                nu = 0.1 + np.exp(dist_params[0])  # ν > 0 para GED\n",
    "                ll = self._log_likelihood_ged(residuals, h, nu)\n",
    "            \n",
    "            return ll if np.isfinite(ll) else -1e10\n",
    "            \n",
    "        except:\n",
    "            return -1e10\n",
    "\n",
    "    def fit(self, start_params=None, method='bfgs', maxiter=1000, tol=1e-9, **fit_kwargs):\n",
    "        \"\"\"\n",
    "        Estimación por ML usando BFGS con pasos tipo Marquardt\n",
    "        \"\"\"\n",
    "        if start_params is None:\n",
    "            start_params = self._get_start_params()\n",
    "        \n",
    "        # Configurar optimizador con características de EViews\n",
    "        if method.lower() == 'bfgs':\n",
    "            # BFGS con pasos Marquardt-type (damping)\n",
    "            result = minimize(\n",
    "                lambda x: -self.loglike(x),\n",
    "                start_params,\n",
    "                method='L-BFGS-B',\n",
    "                options={\n",
    "                    'maxiter': maxiter,\n",
    "                    'ftol': tol,\n",
    "                    'gtol': tol\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Convertir resultado a formato statsmodels\n",
    "            class MockResult:\n",
    "                def __init__(self, result, model):\n",
    "                    self.params = result.x\n",
    "                    self.llf = -result.fun\n",
    "                    self.model = model\n",
    "                    self.nobs = model.n\n",
    "                    self.df_resid = model.n - len(result.x)\n",
    "                    self.success = result.success\n",
    "                    \n",
    "                    # Calcular matriz de covarianza usando OPG\n",
    "                    self.bse, self.cov_params_opg = model._compute_opg_covariance(self.params)\n",
    "            \n",
    "            return MockResult(result, self)\n",
    "        \n",
    "        else:\n",
    "            return super().fit(start_params=start_params, method=method, \n",
    "                             maxiter=maxiter, **fit_kwargs)\n",
    "\n",
    "    def _get_start_params(self):\n",
    "        \"\"\"Valores iniciales para los parámetros\"\"\"\n",
    "        n = self.n\n",
    "        y = self.y\n",
    "        \n",
    "        # Estimación inicial de la media\n",
    "        if self.mean_spec == 'constant':\n",
    "            mu0 = np.mean(y)\n",
    "            params = [mu0]\n",
    "        else:\n",
    "            params = []\n",
    "        \n",
    "        # Residuos iniciales\n",
    "        residuals = y - (params[0] if self.mean_spec == 'constant' else 0)\n",
    "        sq_residuals = residuals**2\n",
    "        \n",
    "        # Regresión auxiliar para parámetros de varianza\n",
    "        if self.q > 0 or self.p > 0:\n",
    "            # Crear matriz de regresores para GARCH\n",
    "            max_lag = max(self.p, self.q)\n",
    "            X_list = [np.ones(n - max_lag)]  # Constante\n",
    "            \n",
    "            # Retardos de residuos al cuadrado (ARCH)\n",
    "            for i in range(1, self.q + 1):\n",
    "                X_list.append(sq_residuals[max_lag - i:-i])\n",
    "            \n",
    "            # Retardos de varianza (aproximados por media móvil)\n",
    "            if self.p > 0:\n",
    "                h_proxy = pd.Series(sq_residuals).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "                for j in range(1, self.p + 1):\n",
    "                    X_list.append(h_proxy.iloc[max_lag - j:-j].values)\n",
    "            \n",
    "            X = np.column_stack(X_list)\n",
    "            y_reg = sq_residuals[max_lag:]\n",
    "            \n",
    "            # Regresión OLS para valores iniciales\n",
    "            try:\n",
    "                ols_params = np.linalg.lstsq(X, y_reg, rcond=None)[0]\n",
    "                omega0 = max(ols_params[0], 1e-6)\n",
    "                alpha0 = np.maximum(ols_params[1:1+self.q], 1e-6) if self.q > 0 else []\n",
    "                beta0 = np.maximum(ols_params[1+self.q:1+self.q+self.p], 1e-6) if self.p > 0 else []\n",
    "            except:\n",
    "                omega0 = np.var(residuals) * 0.1\n",
    "                alpha0 = [0.1] * self.q if self.q > 0 else []\n",
    "                beta0 = [0.8] * self.p if self.p > 0 else []\n",
    "        else:\n",
    "            omega0 = np.var(residuals)\n",
    "            alpha0, beta0 = [], []\n",
    "        \n",
    "        # Transformar a espacio apropiado\n",
    "        if self.force_positive:\n",
    "            params.extend([np.log(omega0)])\n",
    "            params.extend([np.log(a) for a in alpha0])\n",
    "            params.extend([np.log(b) for b in beta0])\n",
    "        else:\n",
    "            # EViews style - solo omega positivo\n",
    "            params.extend([np.sqrt(omega0)])  # omega = param^2\n",
    "            params.extend(alpha0)  # alphas pueden ser negativos\n",
    "            params.extend(beta0)   # betas pueden ser negativos\n",
    "        \n",
    "        # Parámetros de distribución\n",
    "        if self.dist == 't':\n",
    "            params.append(np.log(8 - 2.01))  # ν ≈ 8 inicial\n",
    "        elif self.dist == 'ged':\n",
    "            params.append(np.log(1.9))  # ν ≈ 2 inicial (cercano a normal)\n",
    "        \n",
    "        return np.array(params)\n",
    "\n",
    "    def _compute_opg_covariance(self, params):\n",
    "        \"\"\"\n",
    "        Cálculo de matriz de covarianza usando OPG (Outer Product of Gradients)\n",
    "        como hace EViews por defecto\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        k = len(params)\n",
    "        \n",
    "        # Calcular gradientes por diferencias finitas\n",
    "        eps = 1e-6\n",
    "        gradients = np.zeros((n, k))\n",
    "        \n",
    "        # Log-likelihood individual para cada observación\n",
    "        def ll_individual(theta, t):\n",
    "            if self.mean_spec == 'constant':\n",
    "                mu = theta[0]\n",
    "                residual = self.y[t] - mu\n",
    "            else:\n",
    "                residual = self.y[t]\n",
    "            \n",
    "            # Calcular h_t (simplificado para una observación)\n",
    "            h_t = self._compute_variance(theta, self.y - (theta[0] if self.mean_spec == 'constant' else 0))[t]\n",
    "            \n",
    "            if self.dist == 'normal':\n",
    "                return -0.5 * (np.log(2 * np.pi) + np.log(h_t) + residual**2 / h_t)\n",
    "            else:\n",
    "                # Para simplicidad, usar aproximación normal en gradientes\n",
    "                return -0.5 * (np.log(2 * np.pi) + np.log(h_t) + residual**2 / h_t)\n",
    "        \n",
    "        # Calcular gradientes numéricamente\n",
    "        for j in range(k):\n",
    "            theta_plus = params.copy()\n",
    "            theta_minus = params.copy()\n",
    "            theta_plus[j] += eps\n",
    "            theta_minus[j] -= eps\n",
    "            \n",
    "            for t in range(n):\n",
    "                try:\n",
    "                    ll_plus = ll_individual(theta_plus, t)\n",
    "                    ll_minus = ll_individual(theta_minus, t)\n",
    "                    gradients[t, j] = (ll_plus - ll_minus) / (2 * eps)\n",
    "                except:\n",
    "                    gradients[t, j] = 0\n",
    "        \n",
    "        # Matriz OPG\n",
    "        try:\n",
    "            opg_matrix = gradients.T @ gradients\n",
    "            inv_opg = np.linalg.inv(opg_matrix)\n",
    "            se = np.sqrt(np.diag(inv_opg))\n",
    "            return se, inv_opg\n",
    "        except:\n",
    "            # Si falla, usar identidad escalada\n",
    "            se = np.ones(k) * 0.1\n",
    "            cov = np.eye(k) * 0.01\n",
    "            return se, cov\n",
    "\n",
    "    def summary_eviews(self, result):\n",
    "        \"\"\"\n",
    "        Resumen en formato similar a EViews\n",
    "        \"\"\"\n",
    "        params = result.params\n",
    "        \n",
    "        # Extraer parámetros transformados\n",
    "        param_names = []\n",
    "        param_values = []\n",
    "        param_se = []\n",
    "        \n",
    "        idx = 0\n",
    "        if self.mean_spec == 'constant':\n",
    "            param_names.append('C')\n",
    "            param_values.append(params[idx])\n",
    "            param_se.append(result.bse[idx])\n",
    "            idx += 1\n",
    "        \n",
    "        # Omega (constante de varianza)\n",
    "        if self.force_positive:\n",
    "            omega_val = np.exp(params[idx])\n",
    "            param_se.append(result.bse[idx] * omega_val)  # Delta method\n",
    "        else:\n",
    "            omega_val = params[idx]**2\n",
    "            param_se.append(result.bse[idx] * 2 * abs(params[idx]))  # Delta method\n",
    "        param_names.append('OMEGA')\n",
    "        param_values.append(omega_val)\n",
    "        idx += 1\n",
    "        \n",
    "        # Parámetros ARCH\n",
    "        for i in range(self.q):\n",
    "            if self.force_positive:\n",
    "                alpha_val = np.exp(params[idx])\n",
    "                param_se.append(result.bse[idx] * alpha_val)\n",
    "            else:\n",
    "                alpha_val = params[idx]\n",
    "                param_se.append(result.bse[idx])\n",
    "            param_names.append(f'RESID(-{i+1})^2')\n",
    "            param_values.append(alpha_val)\n",
    "            idx += 1\n",
    "        \n",
    "        # Parámetros GARCH\n",
    "        for j in range(self.p):\n",
    "            if self.force_positive:\n",
    "                beta_val = np.exp(params[idx])\n",
    "                param_se.append(result.bse[idx] * beta_val)\n",
    "            else:\n",
    "                beta_val = params[idx]\n",
    "                param_se.append(result.bse[idx])\n",
    "            param_names.append(f'GARCH(-{j+1})')\n",
    "            param_values.append(beta_val)\n",
    "            idx += 1\n",
    "        \n",
    "        # Parámetros de distribución\n",
    "        if self.dist == 't':\n",
    "            nu_val = 2.01 + np.exp(params[idx])\n",
    "            param_names.append('DF_PARAM')\n",
    "            param_values.append(nu_val)\n",
    "            param_se.append(result.bse[idx] * np.exp(params[idx]))\n",
    "        elif self.dist == 'ged':\n",
    "            nu_val = 0.1 + np.exp(params[idx])\n",
    "            param_names.append('GED_PARAM')\n",
    "            param_values.append(nu_val)\n",
    "            param_se.append(result.bse[idx] * np.exp(params[idx]))\n",
    "        \n",
    "        # Estadísticos z y p-valores\n",
    "        z_stats = np.array(param_values) / np.array(param_se)\n",
    "        p_values = 2 * (1 - stats.norm.cdf(np.abs(z_stats)))\n",
    "        \n",
    "        # DataFrame de resultados\n",
    "        results_df = pd.DataFrame({\n",
    "            'Coefficient': param_values,\n",
    "            'Std. Error': param_se,\n",
    "            'z-Statistic': z_stats,\n",
    "            'Prob.': p_values\n",
    "        }, index=param_names)\n",
    "        \n",
    "        # Estadísticos de bondad de ajuste\n",
    "        y = self.y\n",
    "        nobs = self.n\n",
    "        if self.mean_spec == 'constant':\n",
    "            resid = y - params[0]\n",
    "            mean_y = params[0]\n",
    "        else:\n",
    "            resid = y\n",
    "            mean_y = 0\n",
    "        \n",
    "        ssr = np.sum(resid**2)\n",
    "        tss = np.sum((y - np.mean(y))**2)\n",
    "        r_squared = 1 - ssr/tss\n",
    "        \n",
    "        k = len(params)\n",
    "        aic = -2 * result.llf / nobs + 2 * k / nobs\n",
    "        bic = -2 * result.llf / nobs + k * np.log(nobs) / nobs\n",
    "        hqic = -2 * result.llf / nobs + 2 * k * np.log(np.log(nobs)) / nobs\n",
    "        \n",
    "        # Imprimir resumen estilo EViews\n",
    "        model_name = f\"{'GARCH' if self.p > 0 else 'ARCH'}({self.p},{self.q})\" if self.p > 0 else f\"ARCH({self.q})\"\n",
    "        dist_name = {'normal': 'Normal', 't': 't-distribution', 'ged': 'GED'}[self.dist]\n",
    "        \n",
    "        print(f\"Dependent Variable: Y\")\n",
    "        print(f\"Method: ML ARCH - {dist_name} distribution ({model_name})\")\n",
    "        print(f\"Date: {pd.Timestamp.now().strftime('%m/%d/%y')}   Time: {pd.Timestamp.now().strftime('%H:%M')}\")\n",
    "        print(f\"Sample: 1 {nobs}\")\n",
    "        print(f\"Included observations: {nobs}\")\n",
    "        print(f\"Convergence achieved after iterations\")\n",
    "        print(f\"Coefficient covariance computed using outer product of gradients\")\n",
    "        print(f\"Presample variance: backcast (parameter = {self.lam})\")\n",
    "        print()\n",
    "        \n",
    "        # Tabla de coeficientes\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Variable':<12} {'Coefficient':<12} {'Std. Error':<12} {'z-Statistic':<12} {'Prob.':<12}\")\n",
    "        print(\"=\" * 70)\n",
    "        for name in results_df.index:\n",
    "            row = results_df.loc[name]\n",
    "            print(f\"{name:<12} {row['Coefficient']:<12.6f} {row['Std. Error']:<12.6f} \"\n",
    "                  f\"{row['z-Statistic']:<12.6f} {row['Prob.']:<12.4f}\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # Estadísticos de ajuste\n",
    "        print(f\"R-squared            {r_squared:>8.6f}     Mean dependent var {np.mean(y):>8.6f}\")\n",
    "        print(f\"Adjusted R-squared   {r_squared:>8.6f}     S.D. dependent var {np.std(y, ddof=1):>8.6f}\")\n",
    "        print(f\"S.E. of regression   {np.sqrt(ssr/(nobs-k)):>8.6f}     Akaike info criterion {aic:>8.6f}\")\n",
    "        print(f\"Sum squared resid    {ssr:>8.6f}     Schwarz criterion    {bic:>8.6f}\")\n",
    "        print(f\"Log likelihood       {result.llf:>8.2f}     Hannan-Quinn criter. {hqic:>8.6f}\")\n",
    "        \n",
    "        # Durbin-Watson\n",
    "        dw = np.sum(np.diff(resid)**2) / ssr\n",
    "        print(f\"Durbin-Watson stat   {dw:>8.6f}\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "# # Ejemplo de uso\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Simular datos con heterocedasticidad\n",
    "#     np.random.seed(42)\n",
    "#     n = 500\n",
    "    \n",
    "#     # Modelo DGP: GARCH(1,1)\n",
    "#     omega_true, alpha_true, beta_true = 0.1, 0.15, 0.8\n",
    "#     h = np.zeros(n)\n",
    "#     y = np.zeros(n)\n",
    "#     h[0] = omega_true / (1 - alpha_true - beta_true)\n",
    "    \n",
    "#     for t in range(n):\n",
    "#         if t > 0:\n",
    "#             h[t] = omega_true + alpha_true * (y[t-1] - 0.05)**2 + beta_true * h[t-1]\n",
    "#         y[t] = 0.05 + np.sqrt(h[t]) * np.random.normal()\n",
    "    \n",
    "#     # Estimar GARCH(1,4) como en EViews (permite coeficientes negativos)\n",
    "#     print(\"Estimando GARCH(1,4) estilo EViews (permite coef. negativos)...\")\n",
    "#     model = ARCHGARCH(y, p=1, q=4, dist='normal', backcast_lambda=0.7, force_positive=False)\n",
    "#     result = model.fit(method='bfgs', maxiter=1000)\n",
    "    \n",
    "#     if result.success:\n",
    "#         summary_df = model.summary_eviews(result)\n",
    "#     else:\n",
    "#         print(\"La optimización no convergió exitosamente\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"Estimando ARCH(2) con distribución t-Student...\")\n",
    "#     model2 = ARCHGARCH(y, p=0, q=2, dist='t', backcast_lambda=0.7)\n",
    "#     result2 = model2.fit(method='bfgs', maxiter=1000)\n",
    "    \n",
    "#     if result2.success:\n",
    "#         summary_df2 = model2.summary_eviews(result2)\n",
    "#     else:\n",
    "#         print(\"La optimización no convergió exitosamente\")\n",
    "\n",
    "\n",
    "# Para replicar tu ejemplo de EViews: GARCH(1,4)\n",
    "model = ARCHGARCH(df_clean['y'], p=0, q=4, dist='normal', \n",
    "                  backcast_lambda=0.7, force_positive=False)\n",
    "result = model.fit()\n",
    "model.summary_eviews(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fda4e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:19: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method bfgs is: gtol, norm, epsilon. The list of unsupported keyword arguments passed include: tol. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.325267\n",
      "         Iterations: 63\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 68\n",
      "ARCH(4) via ML   Normal distribution   (BFGS / Marquardt steps)\n",
      "Date: 2025-06-03   Time: 21:04:21\n",
      "Sample (adjusted): 1     97\n",
      "Included observations: 97\n",
      "Coefficient covariance computed using outer product of gradients\n",
      "Presample variance: backcast (parameter = 0.7)\n",
      "\n",
      "Variable   Coefficient   Std. Error   z-Statistic   Prob.\n",
      "                coef     std.err            z    P>|z|\n",
      "C           0.054073    0.011532     4.689009 0.000003\n",
      "omega       0.002774    0.002315     1.198450 0.230742\n",
      "ARCH(1)     0.628225    0.217841     2.883871 0.003928\n",
      "ARCH(2)     0.000000    0.000025     0.000088 0.999930\n",
      "ARCH(3)     0.095218    0.096038     0.991456 0.321463\n",
      "ARCH(4)     0.712061    0.265404     2.682927 0.007298\n",
      "\n",
      "R-squared               -0.005979    Mean dependent var       0.073290\n",
      "Adjusted R-squared      -0.005979    S.D. dependent var       0.249810\n",
      "S.E. of regression       0.257347    Akaike info criterion  -51.101732\n",
      "Sum squared resid        6.026707    Schwarz criterion     -35.653466\n",
      "Log likelihood           31.55087    Hannan-Quinn criter.   -44.855210\n",
      "Durbin-Watson stat       1.415132\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std.err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>5.407327e-02</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>4.689009</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>omega</th>\n",
       "      <td>2.773813e-03</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>1.198450</td>\n",
       "      <td>0.230742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(1)</th>\n",
       "      <td>6.282252e-01</td>\n",
       "      <td>0.217841</td>\n",
       "      <td>2.883871</td>\n",
       "      <td>0.003928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(2)</th>\n",
       "      <td>2.221093e-09</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.999930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(3)</th>\n",
       "      <td>9.521758e-02</td>\n",
       "      <td>0.096038</td>\n",
       "      <td>0.991456</td>\n",
       "      <td>0.321463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCH(4)</th>\n",
       "      <td>7.120606e-01</td>\n",
       "      <td>0.265404</td>\n",
       "      <td>2.682927</td>\n",
       "      <td>0.007298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 coef   std.err         z     P>|z|\n",
       "C        5.407327e-02  0.011532  4.689009  0.000003\n",
       "omega    2.773813e-03  0.002315  1.198450  0.230742\n",
       "ARCH(1)  6.282252e-01  0.217841  2.883871  0.003928\n",
       "ARCH(2)  2.221093e-09  0.000025  0.000088  0.999930\n",
       "ARCH(3)  9.521758e-02  0.096038  0.991456  0.321463\n",
       "ARCH(4)  7.120606e-01  0.265404  2.682927  0.007298"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats\n",
    "\n",
    "class ARCH(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, p=4, backcast_lambda=0.7, **kwds):\n",
    "        #-------------------------------\n",
    "        # 1. Guardar datos y parámetros\n",
    "        #-------------------------------\n",
    "        self.y       = np.asarray(endog, dtype=float)\n",
    "        self.n       = len(self.y)\n",
    "        self.p       = int(p)\n",
    "        self.lam     = float(backcast_lambda)\n",
    "        super().__init__(self.y, **kwds)\n",
    "\n",
    "    def loglike(self, theta):\n",
    "        \"\"\"\n",
    "        EViews: Log-Likelihood para ARCH(p) con distribución normal.\n",
    "        - backcast con λ para el presample.\n",
    "        - parámetros ω, αi parametrizados en log (para garantizar >0).\n",
    "        \"\"\"\n",
    "        mu     = theta[0]\n",
    "        omega  = np.exp(theta[1])\n",
    "        alphas = np.exp(theta[2:2 + self.p])\n",
    "\n",
    "        e = self.y - mu\n",
    "        n = self.n\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 2. Backcast \"presample\" (t=0,…,p-1) tal C25\n",
    "        # ---------------------------------------\n",
    "        sigma2_bar = np.mean(e**2)  # varianza no condicionada\n",
    "        # pesos (1−λ) λ^(i−1), con i=1..n\n",
    "        i = np.arange(1, n + 1)\n",
    "        w = (1 - self.lam) * (self.lam ** (i - 1))\n",
    "        # suma ponderada de residuos pasados + λ^n * varianza\n",
    "        h0 = np.dot(w, e[::-1]**2) + (self.lam ** n) * sigma2_bar\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 3. Inicializar h[t] = h0 para t=0..p-1\n",
    "        # ---------------------------------------\n",
    "        h = np.empty(n)\n",
    "        h[:self.p] = h0\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 4. Recursión ARCH(p): h[t] = ω + Σ α_i e[t−i]^2\n",
    "        # ---------------------------------------\n",
    "        for t in range(self.p, n):\n",
    "            recent_e2 = e[t-self.p:t]**2\n",
    "            # e_lags = [e[t-1]^2, e[t-2]^2, …, e[t-p]^2] en ese orden\n",
    "            h[t] = omega + np.dot(alphas, recent_e2[::-1])\n",
    "            # evitar varianzas ≤ 0\n",
    "            h[t] = np.maximum(h[t], 1e-12)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 5. Log-verosimilitud total (normal)\n",
    "        # ---------------------------------------\n",
    "        ll = -0.5 * np.sum(np.log(2 * np.pi) + np.log(h) + (e**2) / h)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, start_params=None, **fit_kwargs):\n",
    "        \"\"\"\n",
    "        Ajusta usando ML (GenericLikelihoodModel), pero forzando cov_type='opg'\n",
    "        para que los errores estándar salgan por Outer Product of Gradients.\n",
    "        \"\"\"\n",
    "        if start_params is None:\n",
    "            # 1) Estimar mu0 por OLS de y = mu + u\n",
    "            n = self.n\n",
    "            yt = self.y\n",
    "            mu0 = sm.OLS(yt, np.ones((n, 1))).fit().params[0]\n",
    "\n",
    "            # 2) Residuales y regresión ARCH(OLS) para inicializar ω y αs\n",
    "            eps = yt - mu0\n",
    "            sq  = eps**2\n",
    "            lags = np.column_stack([np.roll(sq, i) for i in range(1, self.p + 1)])\n",
    "            Yv = sq[self.p:]\n",
    "            Xv = np.column_stack([np.ones(n - self.p), lags[self.p:]])\n",
    "            params = sm.OLS(Yv, Xv).fit().params\n",
    "            omega0 = max(params[0], 1e-6)\n",
    "            alpha0 = np.maximum(params[1:], 1e-6)\n",
    "\n",
    "            start_params = np.r_[mu0, np.log(omega0), np.log(alpha0)]\n",
    "\n",
    "        # Llamada a super().fit, indicando cov_type='opg'\n",
    "        return super().fit(start_params=start_params, **fit_kwargs)\n",
    "\n",
    "    def summary_eviews(self, res):\n",
    "        \"\"\"\n",
    "        Imprime el resumen en el mismo estilo exacto de EViews C25:\n",
    "        - Cabecera con backcast y OPG.\n",
    "        - Tabla de coeficientes.\n",
    "        - Estadísticos AIC/BIC/HQIC sin normalizar por n.\n",
    "        \"\"\"\n",
    "        theta_hat = res.params\n",
    "        mu_hat    = theta_hat[0]\n",
    "        omega_hat = np.exp(theta_hat[1])\n",
    "        alpha_hat = np.exp(theta_hat[2:2 + self.p])\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 1. Coeficientes y Std.Err (res.bse ya contiene OPG si fit(...) usó cov_type='opg')\n",
    "        # -------------------------------------------------------\n",
    "        se_all = res.bse  # bse corresponde a la transformación correcta\n",
    "        se_mu    = se_all[0]\n",
    "        se_omega = se_all[1] * omega_hat\n",
    "        se_alpha = se_all[2:2 + self.p] * alpha_hat\n",
    "\n",
    "        coefs = [mu_hat, omega_hat] + alpha_hat.tolist()\n",
    "        ses   = [se_mu, se_omega] + se_alpha.tolist()\n",
    "        z     = np.array(coefs) / np.array(ses)\n",
    "        pvals = 2 * (1 - stats.norm.cdf(np.abs(z)))\n",
    "\n",
    "        names = ['C', 'omega'] + [f'ARCH({i})' for i in range(1, self.p + 1)]\n",
    "        df = pd.DataFrame({\n",
    "            'coef': coefs,\n",
    "            'std.err': ses,\n",
    "            'z': z,\n",
    "            'P>|z|': pvals\n",
    "        }, index=names)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 2. Cálculo de Bondad de ajuste (sin normalizar por n, como EViews)\n",
    "        # -------------------------------------------------------\n",
    "        y      = self.y\n",
    "        nobs   = self.n\n",
    "        resid  = y - mu_hat\n",
    "        ssr    = np.sum(resid**2)\n",
    "        tss    = np.sum((y - np.mean(y))**2)\n",
    "        mean_y = np.mean(y)\n",
    "        sd_y   = np.std(y, ddof=1)\n",
    "        k      = len(theta_hat)  # número de parámetros\n",
    "\n",
    "        llf = res.llf  # valor de log-likelihood (ya será positivo, porque GenericLikelihoodModel maximiza)\n",
    "\n",
    "        # EViews usa: AIC = −2 LL + 2 k; BIC = −2 LL + k log(n); HQIC = −2 LL + 2 k log(log n)\n",
    "        aic   = -2 * llf + 2 * k\n",
    "        bic   = -2 * llf + k * np.log(nobs)\n",
    "        hqic  = -2 * llf + 2 * k * np.log(np.log(nobs))\n",
    "\n",
    "        # Durbin–Watson = Σ Δresid² / Σ resid²\n",
    "        dw = np.sum(np.diff(resid)**2) / ssr\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 3. Impresión exacta al estilo EViews (Cap. 25)\n",
    "        # -------------------------------------------------------\n",
    "        print(f\"ARCH({self.p}) via ML   Normal distribution   (BFGS / Marquardt steps)\")\n",
    "        print(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}   Time: {pd.Timestamp.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"Sample (adjusted): 1     {nobs}\")\n",
    "        print(f\"Included observations: {nobs}\")\n",
    "        # print(f\"Convergence achieved after {res.mle_retvals['iterations']} iterations\")\n",
    "        print(\"Coefficient covariance computed using outer product of gradients\")\n",
    "        print(f\"Presample variance: backcast (parameter = {self.lam})\\n\")\n",
    "\n",
    "        print(\"Variable   Coefficient   Std. Error   z-Statistic   Prob.\")\n",
    "        print(df.to_string(formatters={\n",
    "            'coef':     '{:>12.6f}'.format,\n",
    "            'std.err':  '{:>11.6f}'.format,\n",
    "            'z':        '{:>12.6f}'.format,\n",
    "            'P>|z|':    '{:>8.6f}'.format,\n",
    "        }))\n",
    "\n",
    "        print(f\"\\nR-squared            {1 - ssr/tss:>12.6f}    Mean dependent var    {mean_y:>11.6f}\")\n",
    "        print(f\"Adjusted R-squared   {1 - ssr/tss:>12.6f}    S.D. dependent var    {sd_y:>11.6f}\")\n",
    "        print(f\"S.E. of regression   {np.sqrt(ssr/(nobs - k)):>12.6f}    Akaike info criterion {aic:>11.6f}\")\n",
    "        print(f\"Sum squared resid    {ssr:>12.6f}    Schwarz criterion    {bic:>11.6f}\")\n",
    "        print(f\"Log likelihood       {llf:>12.5f}    Hannan-Quinn criter.  {hqic:>11.6f}\")\n",
    "        print(f\"Durbin-Watson stat   {dw:>12.6f}\\n\")\n",
    "\n",
    "        return df\n",
    "\n",
    "# -----------------------------------------\n",
    "# Cómo usarlo para replicar exactamente EViews\n",
    "# -----------------------------------------\n",
    "y = df_clean['y'].astype(float)\n",
    "mod = ARCH(y, p=4, backcast_lambda=0.7)\n",
    "\n",
    "# -----------------\n",
    "# 1) Ajustar con cov_type='opg' para OPG\n",
    "# -----------------\n",
    "res = mod.fit(method='bfgs', disp=True, maxiter=1000, tol=1e-9)\n",
    "\n",
    "# -----------------\n",
    "# 2) Mostrar resumen idéntico al de EViews C25\n",
    "# -----------------\n",
    "mod.summary_eviews(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "562cef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:726: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -31.550866\n",
      "         Iterations: 37\n",
      "         Function evaluations: 529\n",
      "         Gradient evaluations: 74\n",
      "| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  | penalty  |CG stop|\n",
      "|-------|-------|-------|-------------|----------|----------|----------|----------|-------|\n",
      "|   1   |   1   |   0   | -2.4051e+01 | 1.00e+00 | 2.11e+01 | 0.00e+00 | 1.00e+00 |   0   |\n",
      "|   2   |   2   |   1   | -2.4051e+01 | 1.07e-01 | 2.11e+01 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|   3   |   3   |   2   | -2.4051e+01 | 1.09e-02 | 2.11e+01 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|   4   |   4   |   3   | -2.4091e+01 | 1.09e-02 | 1.63e+01 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|   5   |   5   |   5   | -2.4169e+01 | 2.19e-02 | 5.73e+00 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|   6   |   6   |   7   | -2.4282e+01 | 1.53e-01 | 4.54e+00 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|   7   |   7   |   8   | -2.4905e+01 | 1.07e+00 | 3.34e+00 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|   8   |   8   |  11   | -2.5639e+01 | 2.53e+00 | 3.27e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|   9   |   9   |  14   | -2.5860e+01 | 2.53e+00 | 3.31e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  10   |  10   |  18   | -2.7415e+01 | 6.21e+00 | 5.31e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  11   |  11   |  22   | -2.9262e+01 | 6.21e+00 | 3.25e+01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  12   |  12   |  26   | -2.9262e+01 | 7.37e-01 | 3.25e+01 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  13   |  13   |  31   | -3.0458e+01 | 3.65e+00 | 3.24e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  14   |  14   |  35   | -3.0719e+01 | 3.65e+00 | 1.01e+01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  15   |  15   |  40   | -3.0791e+01 | 3.65e+00 | 5.88e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  16   |  16   |  44   | -3.0822e+01 | 3.65e+00 | 1.99e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  17   |  17   |  48   | -3.0872e+01 | 3.65e+00 | 4.43e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  18   |  18   |  51   | -3.0882e+01 | 3.65e+00 | 5.88e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  19   |  19   |  56   | -3.1058e+01 | 3.65e+00 | 1.89e+01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  20   |  20   |  58   | -3.1086e+01 | 3.65e+00 | 9.69e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  21   |  21   |  60   | -3.1105e+01 | 3.65e+00 | 3.49e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  22   |  22   |  65   | -3.1130e+01 | 3.65e+00 | 4.61e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  23   |  23   |  68   | -3.1136e+01 | 3.65e+00 | 5.07e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  24   |  24   |  71   | -3.1162e+01 | 3.65e+00 | 4.05e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  25   |  25   |  75   | -3.1190e+01 | 3.65e+00 | 4.83e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  26   |  26   |  79   | -3.1303e+01 | 3.65e+00 | 5.76e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  27   |  27   |  84   | -3.1386e+01 | 3.65e+00 | 3.08e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  28   |  28   |  89   | -3.1482e+01 | 3.65e+00 | 4.91e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  29   |  29   |  92   | -3.1498e+01 | 3.65e+00 | 1.82e+00 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  30   |  30   |  95   | -3.1502e+01 | 3.65e+00 | 1.81e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  31   |  31   |  99   | -3.1505e+01 | 3.65e+00 | 9.36e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  32   |  32   |  103  | -3.1509e+01 | 3.65e+00 | 2.70e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  33   |  33   |  109  | -3.1519e+01 | 3.65e+00 | 9.44e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  34   |  34   |  110  | -3.1519e+01 | 3.65e+00 | 4.55e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  35   |  35   |  116  | -3.1524e+01 | 3.65e+00 | 8.81e-02 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  36   |  36   |  118  | -3.1525e+01 | 3.65e+00 | 8.86e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  37   |  37   |  121  | -3.1527e+01 | 3.65e+00 | 5.64e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  38   |  38   |  123  | -3.1528e+01 | 3.65e+00 | 3.83e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  39   |  39   |  126  | -3.1531e+01 | 3.65e+00 | 1.05e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  40   |  40   |  130  | -3.1532e+01 | 3.65e+00 | 5.23e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  41   |  41   |  136  | -3.1536e+01 | 3.65e+00 | 2.86e+00 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  42   |  42   |  137  | -3.1537e+01 | 3.65e+00 | 2.50e-01 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  43   |  43   |  139  | -3.1537e+01 | 3.65e+00 | 2.14e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  44   |  44   |  145  | -3.1544e+01 | 4.78e+00 | 3.21e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  45   |  45   |  146  | -3.1544e+01 | 4.78e+00 | 1.61e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  46   |  46   |  152  | -3.1547e+01 | 4.78e+00 | 7.69e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  47   |  47   |  153  | -3.1547e+01 | 4.78e+00 | 1.69e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  48   |  48   |  155  | -3.1547e+01 | 4.78e+00 | 3.52e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  49   |  49   |  161  | -3.1549e+01 | 4.83e+00 | 2.59e-02 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  50   |  50   |  162  | -3.1549e+01 | 4.83e+00 | 5.99e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  51   |  51   |  168  | -3.1549e+01 | 4.83e+00 | 3.64e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  52   |  52   |  169  | -3.1549e+01 | 4.83e+00 | 1.34e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  53   |  53   |  173  | -3.1549e+01 | 4.83e+00 | 3.71e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  54   |  54   |  179  | -3.1550e+01 | 4.83e+00 | 2.96e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  55   |  55   |  180  | -3.1550e+01 | 4.83e+00 | 3.12e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  56   |  56   |  182  | -3.1550e+01 | 4.83e+00 | 7.59e-04 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  57   |  57   |  188  | -3.1550e+01 | 4.87e+00 | 1.60e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  58   |  58   |  189  | -3.1550e+01 | 4.87e+00 | 9.16e-04 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  59   |  59   |  195  | -3.1551e+01 | 4.87e+00 | 2.53e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  60   |  60   |  196  | -3.1551e+01 | 4.87e+00 | 4.69e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  61   |  61   |  199  | -3.1551e+01 | 4.87e+00 | 9.95e-04 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  62   |  62   |  205  | -3.1551e+01 | 4.87e+00 | 4.68e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  63   |  63   |  206  | -3.1551e+01 | 4.87e+00 | 4.35e-02 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  64   |  64   |  207  | -3.1551e+01 | 4.87e+00 | 5.78e-04 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  65   |  65   |  209  | -3.1551e+01 | 4.87e+00 | 1.84e-04 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  66   |  66   |  215  | -3.1551e+01 | 4.94e+00 | 1.00e-01 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  67   |  67   |  216  | -3.1551e+01 | 4.94e+00 | 9.34e-04 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  68   |  68   |  218  | -3.1551e+01 | 4.94e+00 | 9.08e-05 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  69   |  69   |  224  | -3.1551e+01 | 4.94e+00 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  70   |  70   |  225  | -3.1551e+01 | 4.94e-01 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  71   |  71   |  226  | -3.1551e+01 | 4.94e-02 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  72   |  72   |  232  | -3.1551e+01 | 4.94e-03 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  73   |  73   |  234  | -3.1551e+01 | 4.94e-04 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  74   |  74   |  236  | -3.1551e+01 | 4.94e-05 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  75   |  74   |  238  | -3.1551e+01 | 4.94e-06 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  76   |  74   |  240  | -3.1551e+01 | 4.94e-07 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   4   |\n",
      "|  77   |  75   |  242  | -3.1551e+01 | 2.47e-07 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  78   |  76   |  244  | -3.1551e+01 | 1.23e-07 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  79   |  77   |  245  | -3.1551e+01 | 6.17e-08 | 4.07e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  80   |  78   |  246  | -3.1551e+01 | 6.17e-08 | 3.60e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  81   |  79   |  247  | -3.1551e+01 | 3.09e-08 | 3.60e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  82   |  80   |  248  | -3.1551e+01 | 1.54e-08 | 3.60e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  83   |  81   |  249  | -3.1551e+01 | 7.72e-09 | 3.60e-03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "\n",
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 83, function evaluations: 81, CG iterations: 249, optimality: 3.60e-03, constraint violation: 0.00e+00, execution time:  6.6 s.\n",
      "⚠️ BFGS no convergió: Desired error not necessarily achieved due to precision loss.\n",
      "      optimizer    loglike   param_0   param_1   param_2       param_3  \\\n",
      "0  trust-constr  31.550821  0.054073  0.002774  0.628218  1.276469e-05   \n",
      "1     Marquardt  31.550865  0.054073  0.002774  0.628225  2.143449e-07   \n",
      "\n",
      "    param_4   param_5  \n",
      "0  0.095218  0.712056  \n",
      "1  0.095218  0.712061  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize, approx_fprime\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import solve\n",
    "\n",
    "# Log-verosimilitud ARCH(p)\n",
    "def arch_loglike(theta, y, p=4, lam=0.7):\n",
    "    mu = theta[0]\n",
    "    omega = np.exp(theta[1])\n",
    "    alphas = np.exp(theta[2:2 + p])\n",
    "\n",
    "    n = len(y)\n",
    "    e = y - mu\n",
    "\n",
    "    sigma2_bar = np.mean(e**2)\n",
    "    w = (1 - lam) * lam ** np.arange(n)\n",
    "    h0 = np.dot(w[::-1], e**2) + lam**n * sigma2_bar\n",
    "\n",
    "    h = np.empty(n)\n",
    "    h[:p] = np.full(p, h0)\n",
    "    for t in range(p, n):\n",
    "        e_lags = e[t - p:t][::-1]\n",
    "        h[t] = omega + np.dot(alphas, e_lags**2)\n",
    "        h[t] = np.maximum(h[t], 1e-8)\n",
    "\n",
    "    ll = -0.5 * np.sum(np.log(2 * np.pi) + np.log(h) + e**2 / h)\n",
    "    return -ll\n",
    "\n",
    "# Inicialización\n",
    "def init_params(y, p):\n",
    "    mu0 = np.mean(y)\n",
    "    eps = y - mu0\n",
    "    sq = eps**2\n",
    "    lags = np.column_stack([np.roll(sq, i) for i in range(1, p+1)])\n",
    "    Yv = sq[p:]\n",
    "    Xv = np.column_stack([np.ones(len(Yv)), lags[p:]])\n",
    "    params = np.linalg.lstsq(Xv, Yv, rcond=None)[0]\n",
    "    omega0 = max(params[0], 1e-6)\n",
    "    alpha0 = np.maximum(params[1:], 1e-4)\n",
    "    return np.r_[mu0, np.log(omega0), np.log(alpha0)]\n",
    "\n",
    "\n",
    "def fit_bfgs(y, p=4, lam=0.7):\n",
    "    start = init_params(y, p)\n",
    "    res = minimize(\n",
    "        fun=arch_loglike,\n",
    "        x0=start,\n",
    "        args=(y, p, lam),\n",
    "        method='BFGS',\n",
    "        options={'disp': True, 'gtol': 1e-9, 'maxiter': 2000}\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def fit_trust(y, p=4, lam=0.7):\n",
    "    start = init_params(y, p)\n",
    "    \n",
    "    def jac(theta, y, p, lam):\n",
    "        return approx_fprime(theta, lambda th: arch_loglike(th, y, p, lam), epsilon=1e-6)\n",
    "\n",
    "    res = minimize(\n",
    "        fun=arch_loglike,\n",
    "        x0=start,\n",
    "        args=(y, p, lam),\n",
    "        method='trust-constr',\n",
    "        jac=jac,\n",
    "        bounds=None,\n",
    "        options={'verbose': 3}\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def fit_marquardt(y, p=4, lam=0.7, max_iter=100, tol=1e-6):\n",
    "    theta = init_params(y, p)\n",
    "    n_params = len(theta)\n",
    "    I = np.eye(n_params)\n",
    "    λ = 1.0  # factor de damping\n",
    "\n",
    "    def grad(theta):\n",
    "        return approx_fprime(theta, lambda th: arch_loglike(th, y, p, lam), epsilon=1e-6)\n",
    "\n",
    "    def hess(theta):\n",
    "        eps = 1e-4\n",
    "        g0 = grad(theta)\n",
    "        H = np.zeros((n_params, n_params))\n",
    "        for i in range(n_params):\n",
    "            d = np.zeros_like(theta)\n",
    "            d[i] = eps\n",
    "            g1 = grad(theta + d)\n",
    "            H[:, i] = (g1 - g0) / eps\n",
    "        return H\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        g = grad(theta)\n",
    "        H = hess(theta)\n",
    "        try:\n",
    "            step = solve(H + λ * I, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            λ *= 10\n",
    "            continue\n",
    "\n",
    "        theta_new = theta - step\n",
    "        ll_old = arch_loglike(theta, y, p, lam)\n",
    "        ll_new = arch_loglike(theta_new, y, p, lam)\n",
    "\n",
    "        if ll_new < ll_old:\n",
    "            theta = theta_new\n",
    "            λ = max(λ / 2, 1e-6)\n",
    "        else:\n",
    "            λ *= 10\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "    class Result:\n",
    "        def __init__(self, x):\n",
    "            self.x = x\n",
    "            self.fun = arch_loglike(x, y, p, lam)\n",
    "            self.success = True\n",
    "            self.message = \"Converged (custom Marquardt)\"\n",
    "    return Result(theta)\n",
    "\n",
    "\n",
    "def decode_params(theta, p):\n",
    "    mu = theta[0]\n",
    "    omega = np.exp(theta[1])\n",
    "    alphas = np.exp(theta[2:2 + p])\n",
    "    return [mu, omega] + alphas.tolist()\n",
    "\n",
    "\n",
    "def summarize_optimizers(y, results_dict, p):\n",
    "    summary = []\n",
    "    for name, res in results_dict.items():\n",
    "        if not res.success:\n",
    "            print(f\"⚠️ {name} no convergió: {res.message}\")\n",
    "            continue\n",
    "        decoded = decode_params(res.x, p)\n",
    "        summary.append({\n",
    "            'optimizer': name,\n",
    "            'loglike': -res.fun,\n",
    "            **{f'param_{i}': val for i, val in enumerate(decoded)}\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "y = df_clean['y'].astype(float)\n",
    "p = 4\n",
    "\n",
    "res_bfgs = fit_bfgs(y, p)\n",
    "res_trust = fit_trust(y, p)\n",
    "res_marquardt = fit_marquardt(y, p)\n",
    "\n",
    "results = {\n",
    "    'BFGS': res_bfgs,\n",
    "    'trust-constr': res_trust,\n",
    "    'Marquardt': res_marquardt\n",
    "}\n",
    "\n",
    "df_summary = summarize_optimizers(y, results, p)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2474d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sticks = [1,2,3,4,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbff1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n",
      "1 2 4\n",
      "1 2 5\n",
      "1 2 10\n",
      "1 3 4\n",
      "1 3 5\n",
      "1 3 10\n",
      "1 4 5\n",
      "1 4 10\n",
      "1 5 10\n",
      "2 3 4\n",
      "2 3 5\n",
      "2 3 10\n",
      "2 4 5\n",
      "2 4 10\n",
      "2 5 10\n",
      "3 4 5\n",
      "3 4 10\n",
      "3 5 10\n",
      "4 5 10\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(sticks)):\n",
    "    for i in range(s+1, len(sticks)): \n",
    "        for j in range(i+1, len(sticks)):\n",
    "            print(sticks[s], sticks[i], sticks[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d7bfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [1,3,4,2]\n",
    "sorted(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "(2, 4, 5)\n",
      "(3, 4, 5)\n",
      "Hay varias combinaciones válidas, la de mayor suma es: (3, 4, 5) .\n"
     ]
    }
   ],
   "source": [
    "# encontrar combinaciones de sticks\n",
    "import itertools\n",
    "comb = list(itertools.combinations(sorted(sticks), 3))\n",
    "\n",
    "comb_options = []\n",
    "for c in comb:\n",
    "    if c[0] + c[1] > c[2] and c[0] + c[2] > c[1] and c[1] + c[2] > c[0]:\n",
    "        print(c)\n",
    "        comb_options.append(c)\n",
    "\n",
    "if len(comb_options) == 0:\n",
    "    print(\"No hay combinaciones válidas\")\n",
    "elif len(comb_options) == 1:\n",
    "    print(\"Hay una combinación válida:\", comb_options[0])\n",
    "elif len(comb_options) > 1:\n",
    "    comb_options_max = []\n",
    "    for c in comb_options:\n",
    "        comb_options_max.append(sum(c))\n",
    "    max_comb = max(comb_options_max)\n",
    "    print(\"Hay varias combinaciones válidas, la de mayor suma es:\", comb_options[comb_options_max.index(max_comb)],\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b156cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "p = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6480616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from_front = p // 2\n",
    "from_back = (n // 2) - (p // 2)\n",
    "print(min(from_front, from_back))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b1c7521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918ea795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(i, i+1) for i in range(0, n+1, 2)]\n",
    "a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edb9fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a)):\n",
    "    if p in a[i]:\n",
    "        print(i)\n",
    "        break\n",
    "if i < (len(a) - 1) - i:\n",
    "    print(i)\n",
    "else:\n",
    "    print((len(a) - 1) - i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c872002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número 3 está en la tupla (2, 3) en la posición 2, dentro de un total de 3 posiciones.\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    if p in a[i]:\n",
    "        print(f\"El número {p} está en la tupla {a[i]} en la posición {i+1}, dentro de un total de {len(a)} posiciones.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4fd4903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "4 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 4]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "n = len(a)\n",
    "a.sort()\n",
    "mid = int((n + 1)/2)\n",
    "a[mid], a[n-1] = a[n-1], a[mid]\n",
    "st = mid + 1\n",
    "ed = n - 1\n",
    "print(st, ed)\n",
    "while(st <= ed):\n",
    "    a[st], a[ed] = a[ed], a[st]\n",
    "    st = st\n",
    "    ed = ed - 1\n",
    "    print(st, ed)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1c9db01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 4]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "n = len(a)\n",
    "a.sort()\n",
    "mid = int((n + 1)/2)\n",
    "a[mid], a[n-1] = a[n-1], a[mid]\n",
    "\n",
    "st = mid + 1\n",
    "ed = n - 2\n",
    "while st <= ed:\n",
    "    a[st], a[ed] = a[ed], a[st]\n",
    "    st += 1\n",
    "    ed -= 1\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f857363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a82f1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 12]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,6]\n",
    "b = [24,36]\n",
    "\n",
    "a_b = a + b\n",
    "a_b_max = max(a_b)\n",
    "btws = []\n",
    "\n",
    "for i in range(1, a_b_max + 1):\n",
    "    ok_a = all(i % ai == 0 for ai in a) \n",
    "    ok_b = all(bi % i == 0 for bi in b)  \n",
    "\n",
    "    if ok_a and ok_b:\n",
    "        btws.append(i)\n",
    "btws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b669b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateid01</th>\n",
       "      <th>dateid</th>\n",
       "      <th>tb3ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1934-01-01</td>\n",
       "      <td>1934-03-31 23:59:59.999</td>\n",
       "      <td>0.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1934-04-01</td>\n",
       "      <td>1934-06-30 23:59:59.999</td>\n",
       "      <td>0.153333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1934-07-01</td>\n",
       "      <td>1934-09-30 23:59:59.999</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1934-10-01</td>\n",
       "      <td>1934-12-31 23:59:59.999</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1935-01-01</td>\n",
       "      <td>1935-03-31 23:59:59.999</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dateid01                   dateid     tb3ms\n",
       "0  1934-01-01  1934-03-31 23:59:59.999  0.526667\n",
       "1  1934-04-01  1934-06-30 23:59:59.999  0.153333\n",
       "2  1934-07-01  1934-09-30 23:59:59.999  0.183333\n",
       "3  1934-10-01  1934-12-31 23:59:59.999  0.250000\n",
       "4  1935-01-01  1935-03-31 23:59:59.999  0.180000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\HP\\OneDrive\\Escritorio\\David Guzzi\\DiTella\\MEC\\Materias\\2025\\2025 1T\\[MT10] Series de Tiempo\\Clases prácticas\\Prácticas\\Práctica 5-20250620\\rate.txt\"\n",
    "df = pd.read_csv(path, delimiter=\"\\t\", decimal=\".\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa146a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 345 entries, 0 to 344\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   dateid01  345 non-null    object \n",
      " 1   dateid    345 non-null    object \n",
      " 2   tb3ms     345 non-null    float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251e9925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dateid'] = pd.to_datetime(df['dateid'])\n",
    "df_test = df.copy()\n",
    "df_test[\"period\"] = df[\"dateid\"].dt.to_period(\"Q\")\n",
    "df_test.set_index(\"period\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff3e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs en muestra: 202\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_test.loc[\"1955Q1\":\"2005Q2\"].copy()\n",
    "print(\"Obs en muestra:\", len(df_sub))   # debe salir 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78188d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateid01</th>\n",
       "      <th>dateid</th>\n",
       "      <th>tb3ms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1955Q1</th>\n",
       "      <td>1955-01-01</td>\n",
       "      <td>1955-03-31 23:59:59.999</td>\n",
       "      <td>1.223333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955Q2</th>\n",
       "      <td>1955-04-01</td>\n",
       "      <td>1955-06-30 23:59:59.999</td>\n",
       "      <td>1.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955Q3</th>\n",
       "      <td>1955-07-01</td>\n",
       "      <td>1955-09-30 23:59:59.999</td>\n",
       "      <td>1.856667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955Q4</th>\n",
       "      <td>1955-10-01</td>\n",
       "      <td>1955-12-31 23:59:59.999</td>\n",
       "      <td>2.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956Q1</th>\n",
       "      <td>1956-01-01</td>\n",
       "      <td>1956-03-31 23:59:59.999</td>\n",
       "      <td>2.326667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004Q2</th>\n",
       "      <td>2004-04-01</td>\n",
       "      <td>2004-06-30 23:59:59.999</td>\n",
       "      <td>1.076667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004Q3</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2004-09-30 23:59:59.999</td>\n",
       "      <td>1.486667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004Q4</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2004-12-31 23:59:59.999</td>\n",
       "      <td>2.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005Q1</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>2005-03-31 23:59:59.999</td>\n",
       "      <td>2.536667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005Q2</th>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>2005-06-30 23:59:59.999</td>\n",
       "      <td>2.863333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dateid01                  dateid     tb3ms\n",
       "period                                              \n",
       "1955Q1  1955-01-01 1955-03-31 23:59:59.999  1.223333\n",
       "1955Q2  1955-04-01 1955-06-30 23:59:59.999  1.483333\n",
       "1955Q3  1955-07-01 1955-09-30 23:59:59.999  1.856667\n",
       "1955Q4  1955-10-01 1955-12-31 23:59:59.999  2.336667\n",
       "1956Q1  1956-01-01 1956-03-31 23:59:59.999  2.326667\n",
       "...            ...                     ...       ...\n",
       "2004Q2  2004-04-01 2004-06-30 23:59:59.999  1.076667\n",
       "2004Q3  2004-07-01 2004-09-30 23:59:59.999  1.486667\n",
       "2004Q4  2004-10-01 2004-12-31 23:59:59.999  2.006667\n",
       "2005Q1  2005-01-01 2005-03-31 23:59:59.999  2.536667\n",
       "2005Q2  2005-04-01 2005-06-30 23:59:59.999  2.863333\n",
       "\n",
       "[202 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.regime_switching.threshold_regression import ThresholdRegression\n",
    "\n",
    "# 1. Load your data (assumes a DataFrame df with a datetime index)\n",
    "#    and columns 'TB3MS'.\n",
    "y = df[\"TB3MS\"].astype(float)\n",
    "\n",
    "# 2. Construct the lagged regressors TB3MS(-1) … TB3MS(-4)\n",
    "#    and the constant.\n",
    "X = pd.concat([y.shift(i) for i in range(1, 5)], axis=1)\n",
    "X.columns = [f\"TB3MS(-{i})\" for i in range(1,5)]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# 3. Specify the STAR model:\n",
    "#    - k_lags: number of lags in the linear part (4)\n",
    "#    - trend='c': include a constant\n",
    "#    - smooth_transition=True: STAR vs. discrete TAR\n",
    "#    - method='logistic': logistic transition\n",
    "#    - exog_s: threshold variable (lag-1)\n",
    "#    - trans_exog_lags=0: use exog_s unlagged as transition variable\n",
    "mod = ThresholdRegression(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    k_lags=4,\n",
    "    trend=\"c\",\n",
    "    smooth_transition=True,\n",
    "    method=\"logistic\",\n",
    "    exog_s=y.shift(1),\n",
    "    trans_exog_lags=0,\n",
    ")\n",
    "\n",
    "# 4. Fit with a grid search on starting values and Marquardt‐BFGS\n",
    "res = mod.fit(\n",
    "    start_params=None,\n",
    "    search=\"grid\",\n",
    "    concentrated=True,\n",
    "    method=\"bfgs\",\n",
    "    maxiter=500,\n",
    "    tol=1e-4,\n",
    ")\n",
    "\n",
    "# 5. Display the results\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60b4f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold Variables (linear part)\n",
      "  Variable  Coefficient  Std. Error  t-Statistic        Prob.\n",
      "        C     0.007647    0.100594     0.076015 9.394072e-01\n",
      "TB3MS(-1)     1.460275    0.065057    22.446197 0.000000e+00\n",
      "TB3MS(-2)    -0.901047    0.105363    -8.551836 0.000000e+00\n",
      "TB3MS(-3)     0.829675    0.109036     7.609207 2.753353e-14\n",
      "TB3MS(-4)    -0.380525    0.067446    -5.641947 1.681377e-08\n",
      "\n",
      "Threshold Variables (nonlinear part)\n",
      "  Variable  Coefficient  Std. Error  t-Statistic    Prob.\n",
      "        C  -114.186938   74.378735    -1.535209 0.124732\n",
      "TB3MS(-1)    11.205016    6.674287     1.678833 0.093185\n",
      "TB3MS(-2)    -1.841348    1.237071    -1.488475 0.136626\n",
      "TB3MS(-3)    -1.856590    0.602216    -3.082930 0.002050\n",
      "TB3MS(-4)    -0.286474    0.190993    -1.499914 0.133637\n",
      "\n",
      "Slopes & Threshold\n",
      "      Variable  Coefficient  Std. Error  t-Statistic    Prob.\n",
      "    Slope (γ)     4.134086    1.506369     2.744405 0.006062\n",
      "Threshold (c)    13.327049    0.333907    39.912435 0.000000\n",
      "\n",
      "Log likelihood   -166.8797\n",
      "AIC              359.7595\n",
      "BIC              402.5070\n",
      "Durbin–Watson    1.747648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:2748: UserWarning: df_model + k_constant + k_extra differs from k_params\n",
      "  warnings.warn(\"df_model + k_constant + k_extra \"\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:2752: UserWarning: df_resid differs from nobs - k_params\n",
      "  warnings.warn(\"df_resid differs from nobs - k_params\")\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from statsmodels.tools.numdiff import approx_hess\n",
    "from numpy.linalg import inv\n",
    "from scipy.stats import norm\n",
    "from scipy.special import expit\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "class STAR(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog_lin, exog_thr, **kwds):\n",
    "        super().__init__(endog, exog_lin, **kwds)\n",
    "        self.exog_lin = exog_lin\n",
    "        self.exog_thr = exog_thr\n",
    "\n",
    "    def loglike(self, params):\n",
    "        p = self.exog_lin.shape[1]\n",
    "        β = params[0:p]\n",
    "        α = params[p:2*p]\n",
    "        γ, c = params[2*p:2*p+2]\n",
    "        logs = params[-1]\n",
    "        σ = np.exp(logs)\n",
    "\n",
    "        X = self.exog_lin          # (n×p)\n",
    "        Z = self.exog_thr          # (n,)\n",
    "        G = expit(γ*(Z - c))       # (n,)\n",
    "\n",
    "        μ = X.dot(β) + G * (X.dot(α))  # (n,)\n",
    "        resid = self.endog - μ\n",
    "        ll = -0.5*np.log(2*np.pi*σ**2) - 0.5*(resid**2)/σ**2\n",
    "        return ll.sum()\n",
    "\n",
    "    def fit(self, start_params=None, **kwds):\n",
    "        if start_params is None:\n",
    "            Z = self.exog_thr\n",
    "            y = self.endog\n",
    "            X = self.exog_lin\n",
    "\n",
    "            # 1) Broader gamma grid (more points, larger range)\n",
    "            gammas = np.logspace(0, 4, 20)    # from 1        to 10,000\n",
    "            # 2) c over full support of Z\n",
    "            cs = np.linspace(Z.min(), Z.max(), 50)\n",
    "\n",
    "            best_ll = -np.inf\n",
    "            best = None\n",
    "            for γ0 in gammas:\n",
    "                for c0 in cs:\n",
    "                    # stable logistic\n",
    "                    G0 = expit(γ0*(Z - c0))\n",
    "                    Xstar = np.hstack([X, G0[:,None]*X])\n",
    "                    βα0, *_ = np.linalg.lstsq(Xstar, y, rcond=None)\n",
    "                    resid0 = y - Xstar.dot(βα0)\n",
    "                    σ2_0 = (resid0**2).mean()\n",
    "                    # exact concentrated LL\n",
    "                    ll0 = -0.5*len(y)*(np.log(2*np.pi*σ2_0)+1)\n",
    "                    if ll0 > best_ll:\n",
    "                        best_ll = ll0\n",
    "                        best = (γ0, c0, βα0, σ2_0)\n",
    "\n",
    "            γ0, c0, βα0, σ2_0 = best\n",
    "            p = X.shape[1]\n",
    "            β0, α0 = βα0[:p], βα0[p:]\n",
    "            logs0 = 0.5*np.log(σ2_0)\n",
    "            start_params = np.r_[β0, α0, γ0, c0, logs0]\n",
    "\n",
    "        return super().fit(start_params=start_params,\n",
    "                           method=\"bfgs\",\n",
    "                           maxiter=500,\n",
    "                           disp=False,\n",
    "                           **kwds)\n",
    "\n",
    "# -------------------\n",
    "# 0) Carga y preparación de datos\n",
    "# Ajusta a tu caso si el nombre de la columna es distinto:\n",
    "y = df_sub[\"tb3ms\"].astype(float).dropna()\n",
    "lags = pd.concat([y.shift(i) for i in range(1,5)], axis=1).dropna()\n",
    "lags.columns = [f\"TB3MS(-{i})\" for i in range(1,5)]\n",
    "common_idx = y.index.intersection(lags.index)\n",
    "y2 = y.loc[common_idx].values               # (n,)\n",
    "X_lin = sm.add_constant(lags.loc[common_idx]).values   # (n×5)\n",
    "X_thr = lags.loc[common_idx, \"TB3MS(-1)\"].values       # (n,)\n",
    "\n",
    "# 1) Estimación: grid + BFGS\n",
    "mod = STAR(y2, X_lin, X_thr)\n",
    "res = mod.fit()\n",
    "\n",
    "# 2) Hessiana numérica → covarianza\n",
    "hess = approx_hess(res.params, lambda p: -mod.loglike(p))\n",
    "cov = inv(hess)\n",
    "se = np.sqrt(np.diag(cov))\n",
    "\n",
    "# 3) Partición de parámetros y errores\n",
    "p = X_lin.shape[1]\n",
    "β_hat    = res.params[0:p]\n",
    "α_hat    = res.params[p:2*p]\n",
    "γ_hat, c_hat = res.params[2*p:2*p+2]\n",
    "\n",
    "se_lin   = se[0:p]\n",
    "se_non   = se[p:2*p]\n",
    "se_gamma = se[2*p]\n",
    "se_c     = se[2*p+1]\n",
    "\n",
    "# 4) Construcción de tablas\n",
    "lin_idx = [\"C\"] + [f\"TB3MS(-{i})\" for i in range(1,5)]\n",
    "non_idx = lin_idx.copy()\n",
    "\n",
    "def make_df(names, coefs, sts):\n",
    "    dfc = pd.DataFrame({\n",
    "        \"Variable\": names,\n",
    "        \"Coefficient\": coefs,\n",
    "        \"Std. Error\": sts\n",
    "    })\n",
    "    dfc[\"t-Statistic\"] = dfc[\"Coefficient\"] / dfc[\"Std. Error\"]\n",
    "    dfc[\"Prob.\"] = 2 * (1 - norm.cdf(np.abs(dfc[\"t-Statistic\"])))\n",
    "    return dfc\n",
    "\n",
    "tbl_lin = make_df(lin_idx, β_hat, se_lin)\n",
    "tbl_non = make_df(non_idx, α_hat, se_non)\n",
    "tbl_sl  = make_df(\n",
    "    [\"Slope (γ)\", \"Threshold (c)\"],\n",
    "    [γ_hat, c_hat],\n",
    "    [se_gamma, se_c]\n",
    ")\n",
    "\n",
    "print(\"\\nThreshold Variables (linear part)\\n\", tbl_lin.to_string(index=False))\n",
    "print(\"\\nThreshold Variables (nonlinear part)\\n\", tbl_non.to_string(index=False))\n",
    "print(\"\\nSlopes & Threshold\\n\", tbl_sl.to_string(index=False))\n",
    "\n",
    "# 5) Cálculo de residuos y Durbin–Watson\n",
    "G_hat = expit(γ_hat * (X_thr - c_hat))\n",
    "μ_hat = X_lin.dot(β_hat) + G_hat * (X_lin.dot(α_hat))\n",
    "resid = y2 - μ_hat\n",
    "dw = durbin_watson(resid)\n",
    "print(f\"\\nLog likelihood   {res.llf:.4f}\")\n",
    "print(f\"AIC              {res.aic:.4f}\")\n",
    "print(f\"BIC              {res.bic:.4f}\")\n",
    "print(f\"Durbin–Watson    {dw:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SmoothThresholdRegression:\n",
    "    \"\"\"\n",
    "    Clase para estimación de modelos de regresión de umbral suave (STR)\n",
    "    con función de transición logística, replicando la salida de EViews.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "        self.results_ = {}\n",
    "        \n",
    "    def _logistic_transition(self, z, gamma, c):\n",
    "        \"\"\"\n",
    "        Función de transición logística: G(z; gamma, c) = 1 / (1 + exp(-gamma * (z - c)))\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : array-like\n",
    "            Variable de transición\n",
    "        gamma : float\n",
    "            Parámetro de pendiente (slope)\n",
    "        c : float\n",
    "            Parámetro de umbral (threshold)\n",
    "        \"\"\"\n",
    "        # Evitar overflow en exp\n",
    "        arg = -gamma * (z - c)\n",
    "        arg = np.clip(arg, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(arg))\n",
    "    \n",
    "    def _objective_function(self, params, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Función objetivo para minimizar (suma de residuos cuadrados)\n",
    "        \"\"\"\n",
    "        n_linear = X_linear.shape[1]\n",
    "        n_nonlinear = X_nonlinear.shape[1]\n",
    "        \n",
    "        # Separar parámetros\n",
    "        beta_linear = params[:n_linear]\n",
    "        beta_nonlinear = params[n_linear:n_linear + n_nonlinear]\n",
    "        gamma = abs(params[-2])  # Slope (debe ser positivo)\n",
    "        c = params[-1]  # Threshold\n",
    "        \n",
    "        # Calcular función de transición\n",
    "        G = self._logistic_transition(z, gamma, c)\n",
    "        \n",
    "        # Modelo STR: y = X_linear @ beta_linear + G * X_nonlinear @ beta_nonlinear + error\n",
    "        y_pred = X_linear @ beta_linear + G * (X_nonlinear @ beta_nonlinear)\n",
    "        \n",
    "        # Suma de residuos cuadrados\n",
    "        residuals = y - y_pred\n",
    "        ssr = np.sum(residuals**2)\n",
    "        \n",
    "        return ssr\n",
    "    \n",
    "    def _grid_search_initial_values(self, y, X_linear, X_nonlinear, z, n_grid=20):\n",
    "        \"\"\"\n",
    "        Búsqueda en grilla para valores iniciales del threshold y slope\n",
    "        \"\"\"\n",
    "        z_min, z_max = np.percentile(z, [10, 90])\n",
    "        c_grid = np.linspace(z_min, z_max, n_grid)\n",
    "        gamma_grid = np.logspace(0, 3, n_grid)  # De 1 a 1000\n",
    "        \n",
    "        best_ssr = np.inf\n",
    "        best_params = None\n",
    "        \n",
    "        print(\"Realizando búsqueda en grilla para valores iniciales...\")\n",
    "        \n",
    "        for c in c_grid:\n",
    "            for gamma in gamma_grid:\n",
    "                try:\n",
    "                    # Calcular función de transición\n",
    "                    G = self._logistic_transition(z, gamma, c)\n",
    "                    \n",
    "                    # Regresión concentrada: estimar betas dados gamma y c\n",
    "                    X_combined = np.column_stack([X_linear, G[:, np.newaxis] * X_nonlinear])\n",
    "                    \n",
    "                    # OLS\n",
    "                    try:\n",
    "                        beta_combined = np.linalg.solve(X_combined.T @ X_combined, X_combined.T @ y)\n",
    "                        y_pred = X_combined @ beta_combined\n",
    "                        ssr = np.sum((y - y_pred)**2)\n",
    "                        \n",
    "                        if ssr < best_ssr:\n",
    "                            best_ssr = ssr\n",
    "                            n_linear = X_linear.shape[1]\n",
    "                            best_params = np.concatenate([\n",
    "                                beta_combined[:n_linear],     # beta_linear\n",
    "                                beta_combined[n_linear:],     # beta_nonlinear\n",
    "                                [gamma, c]                    # gamma, threshold\n",
    "                            ])\n",
    "                    except np.linalg.LinAlgError:\n",
    "                        continue\n",
    "                        \n",
    "                except (OverflowError, RuntimeWarning):\n",
    "                    continue\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def fit(self, y, X_linear, X_nonlinear, z, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Ajustar el modelo STR\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like\n",
    "            Variable dependiente\n",
    "        X_linear : array-like\n",
    "            Variables explicativas para la parte lineal (incluyendo constante)\n",
    "        X_nonlinear : array-like  \n",
    "            Variables explicativas para la parte no lineal (incluyendo constante)\n",
    "        z : array-like\n",
    "            Variable de transición (threshold variable)\n",
    "        max_iter : int\n",
    "            Número máximo de iteraciones\n",
    "        tol : float\n",
    "            Tolerancia para convergencia\n",
    "        \"\"\"\n",
    "        # Convertir a numpy arrays\n",
    "        y = np.asarray(y).flatten()\n",
    "        X_linear = np.asarray(X_linear)\n",
    "        X_nonlinear = np.asarray(X_nonlinear)\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        # Verificar dimensiones\n",
    "        n = len(y)\n",
    "        if X_linear.shape[0] != n or X_nonlinear.shape[0] != n or len(z) != n:\n",
    "            raise ValueError(\"Las dimensiones de y, X_linear, X_nonlinear y z deben coincidir\")\n",
    "        \n",
    "        # Búsqueda en grilla para valores iniciales\n",
    "        initial_params = self._grid_search_initial_values(y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        if initial_params is None:\n",
    "            raise ValueError(\"No se pudieron encontrar valores iniciales válidos\")\n",
    "        \n",
    "        print(f\"Valores iniciales encontrados. Comenzando optimización...\")\n",
    "        \n",
    "        # Optimización\n",
    "        result = minimize(\n",
    "            self._objective_function,\n",
    "            initial_params,\n",
    "            args=(y, X_linear, X_nonlinear, z),\n",
    "            method='BFGS',\n",
    "            options={'maxiter': max_iter, 'gtol': tol}\n",
    "        )\n",
    "        \n",
    "        if not result.success:\n",
    "            print(f\"Advertencia: La optimización no convergió. Mensaje: {result.message}\")\n",
    "        else:\n",
    "            print(f\"Convergencia alcanzada después de {result.nit} iteraciones\")\n",
    "        \n",
    "        # Extraer parámetros estimados\n",
    "        n_linear = X_linear.shape[1]\n",
    "        n_nonlinear = X_nonlinear.shape[1]\n",
    "        \n",
    "        self.beta_linear_ = result.x[:n_linear]\n",
    "        self.beta_nonlinear_ = result.x[n_linear:n_linear + n_nonlinear]\n",
    "        self.gamma_ = abs(result.x[-2])  # Slope\n",
    "        self.threshold_ = result.x[-1]\n",
    "        \n",
    "        # Calcular estadísticos del modelo\n",
    "        self._calculate_statistics(y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _calculate_statistics(self, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Calcular estadísticos del modelo ajustado\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        \n",
    "        # Función de transición estimada\n",
    "        G = self._logistic_transition(z, self.gamma_, self.threshold_)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = X_linear @ self.beta_linear_ + G * (X_nonlinear @ self.beta_nonlinear_)\n",
    "        \n",
    "        # Residuos\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        # Estadísticos básicos\n",
    "        ssr = np.sum(residuals**2)  # Suma de residuos cuadrados\n",
    "        tss = np.sum((y - np.mean(y))**2)  # Suma total de cuadrados\n",
    "        \n",
    "        # Grados de libertad\n",
    "        k = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2  # +2 por gamma y threshold\n",
    "        df_resid = n - k\n",
    "        df_model = k - 1\n",
    "        \n",
    "        # Estadísticos del modelo\n",
    "        self.r_squared_ = 1 - ssr/tss\n",
    "        self.adj_r_squared_ = 1 - (ssr/df_resid)/(tss/(n-1))\n",
    "        self.se_regression_ = np.sqrt(ssr/df_resid)\n",
    "        self.f_statistic_ = (tss - ssr)/df_model / (ssr/df_resid) if ssr > 0 else np.inf\n",
    "        self.f_pvalue_ = 1 - stats.f.cdf(self.f_statistic_, df_model, df_resid)\n",
    "        \n",
    "        # Criterios de información\n",
    "        self.log_likelihood_ = -n/2 * (np.log(2*np.pi) + np.log(ssr/n) + 1)\n",
    "        self.aic_ = 2*k - 2*self.log_likelihood_\n",
    "        self.bic_ = k*np.log(n) - 2*self.log_likelihood_\n",
    "        self.hqic_ = 2*k*np.log(np.log(n)) - 2*self.log_likelihood_\n",
    "        \n",
    "        # Durbin-Watson\n",
    "        diff_resid = np.diff(residuals)\n",
    "        self.durbin_watson_ = np.sum(diff_resid**2) / ssr\n",
    "        \n",
    "        # Variable dependiente\n",
    "        self.mean_dependent_ = np.mean(y)\n",
    "        self.std_dependent_ = np.std(y, ddof=1)\n",
    "        \n",
    "        # Calcular errores estándar usando gradientes numéricos\n",
    "        self._calculate_standard_errors(y, X_linear, X_nonlinear, z)\n",
    "    \n",
    "    def _calculate_standard_errors(self, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Calcular errores estándar usando el producto exterior de gradientes\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        k_total = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "        \n",
    "        # Calcular matriz Hessiana numéricamente\n",
    "        eps = 1e-6\n",
    "        hessian = np.zeros((k_total, k_total))\n",
    "        \n",
    "        # Parámetros actuales\n",
    "        params = np.concatenate([self.beta_linear_, self.beta_nonlinear_, [self.gamma_, self.threshold_]])\n",
    "        \n",
    "        # Calcular gradientes numéricos\n",
    "        gradients = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            grad_i = np.zeros(k_total)\n",
    "            \n",
    "            for j in range(k_total):\n",
    "                params_plus = params.copy()\n",
    "                params_minus = params.copy()\n",
    "                params_plus[j] += eps\n",
    "                params_minus[j] -= eps\n",
    "                \n",
    "                # Calcular residuo para obs i con parámetros perturbados\n",
    "                residual_plus = self._calculate_residual_i(i, params_plus, y, X_linear, X_nonlinear, z)\n",
    "                residual_minus = self._calculate_residual_i(i, params_minus, y, X_linear, X_nonlinear, z)\n",
    "                \n",
    "                grad_i[j] = (residual_plus - residual_minus) / (2 * eps)\n",
    "            \n",
    "            gradients.append(grad_i)\n",
    "        \n",
    "        gradients = np.array(gradients)\n",
    "        \n",
    "        # Matriz de información usando producto exterior de gradientes\n",
    "        info_matrix = gradients.T @ gradients\n",
    "        \n",
    "        try:\n",
    "            # Inversa de la matriz de información\n",
    "            cov_matrix = np.linalg.inv(info_matrix)\n",
    "            self.std_errors_ = np.sqrt(np.diag(cov_matrix))\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Si la matriz no es invertible, usar aproximación diagonal\n",
    "            print(\"Advertencia: Matriz de información singular. Usando aproximación para errores estándar.\")\n",
    "            self.std_errors_ = np.ones(k_total) * 0.1  # Valores por defecto\n",
    "    \n",
    "    def _calculate_residual_i(self, i, params, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Calcular residuo para observación i dados los parámetros\n",
    "        \"\"\"\n",
    "        n_linear = X_linear.shape[1]\n",
    "        n_nonlinear = X_nonlinear.shape[1]\n",
    "        \n",
    "        beta_linear = params[:n_linear]\n",
    "        beta_nonlinear = params[n_linear:n_linear + n_nonlinear]\n",
    "        gamma = abs(params[-2])\n",
    "        c = params[-1]\n",
    "        \n",
    "        G_i = self._logistic_transition(z[i], gamma, c)\n",
    "        y_pred_i = X_linear[i] @ beta_linear + G_i * (X_nonlinear[i] @ beta_nonlinear)\n",
    "        \n",
    "        return y[i] - y_pred_i\n",
    "    \n",
    "    def summary(self, variable_names_linear=None, variable_names_nonlinear=None):\n",
    "        \"\"\"\n",
    "        Mostrar resumen de resultados al estilo EViews\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"El modelo debe ser ajustado primero\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"Dependent Variable: Y\")\n",
    "        print(\"Method: Smooth Threshold Regression\")\n",
    "        print(\"Transition function: Logistic\")\n",
    "        print(\"Date: 06/20/25   Time: 19:16\")\n",
    "        print(f\"Included observations: {len(self.beta_linear_) + len(self.beta_nonlinear_) + 2}\")\n",
    "        print(\"Threshold variable: Z(-1)\")\n",
    "        print(\"Starting values: Grid search with concentrated regression coefficients\")\n",
    "        print(\"Ordinary standard errors & covariance using outer product of gradients\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Crear nombres por defecto si no se proporcionan\n",
    "        if variable_names_linear is None:\n",
    "            variable_names_linear = ['C'] + [f'X{i}' for i in range(1, len(self.beta_linear_))]\n",
    "        if variable_names_nonlinear is None:\n",
    "            variable_names_nonlinear = ['C'] + [f'X{i}' for i in range(1, len(self.beta_nonlinear_))]\n",
    "        \n",
    "        # Calcular estadísticos t y p-values\n",
    "        n_total = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "        df = n_total - len(self.beta_linear_) - len(self.beta_nonlinear_) - 2\n",
    "        \n",
    "        # Parte lineal\n",
    "        print(\"Threshold Variables (linear part)\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        for i, (name, coef) in enumerate(zip(variable_names_linear, self.beta_linear_)):\n",
    "            if i < len(self.std_errors_):\n",
    "                std_err = self.std_errors_[i]\n",
    "                t_stat = coef / std_err if std_err != 0 else 0\n",
    "                p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if df > 0 else 0\n",
    "                print(f\"{name:<15} {coef:<12.6f} {std_err:<12.6f} {t_stat:<12.6f} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Threshold Variables (nonlinear part)\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        offset = len(self.beta_linear_)\n",
    "        for i, (name, coef) in enumerate(zip(variable_names_nonlinear, self.beta_nonlinear_)):\n",
    "            idx = offset + i\n",
    "            if idx < len(self.std_errors_):\n",
    "                std_err = self.std_errors_[idx]\n",
    "                t_stat = coef / std_err if std_err != 0 else 0\n",
    "                p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if df > 0 else 0\n",
    "                print(f\"{name:<15} {coef:<12.6f} {std_err:<12.6f} {t_stat:<12.6f} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Slopes\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        # Slope (gamma)\n",
    "        idx = len(self.beta_linear_) + len(self.beta_nonlinear_)\n",
    "        if idx < len(self.std_errors_):\n",
    "            std_err = self.std_errors_[idx]\n",
    "            t_stat = self.gamma_ / std_err if std_err != 0 else 0\n",
    "            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if df > 0 else 0\n",
    "            print(f\"{'SLOPE':<15} {self.gamma_:<12.6f} {std_err:<12.6f} {t_stat:<12.6e} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Thresholds\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        # Threshold\n",
    "        idx = len(self.beta_linear_) + len(self.beta_nonlinear_) + 1\n",
    "        if idx < len(self.std_errors_):\n",
    "            std_err = self.std_errors_[idx]\n",
    "            t_stat = self.threshold_ / std_err if std_err != 0 else 0\n",
    "            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if df > 0 else 0\n",
    "            print(f\"{'THRESHOLD':<15} {self.threshold_:<12.6f} {std_err:<12.6f} {t_stat:<12.6f} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"R-squared         {self.r_squared_:<12.6f}    Mean dependent var    {self.mean_dependent_:<12.6f}\")\n",
    "        print(f\"Adjusted R-squared{self.adj_r_squared_:<12.6f}    S.D. dependent var    {self.std_dependent_:<12.6f}\")\n",
    "        print(f\"S.E. of regression{self.se_regression_:<12.6f}    Akaike info criterion {self.aic_:<12.6f}\")\n",
    "        print(f\"Sum squared resid {getattr(self, 'ssr_', 0):<12.6f}    Schwarz criterion     {self.bic_:<12.6f}\")\n",
    "        print(f\"Log likelihood    {self.log_likelihood_:<12.6f}    Hannan-Quinn criter.  {self.hqic_:<12.6f}\")\n",
    "        print(f\"F-statistic       {self.f_statistic_:<12.6f}    Durbin-Watson stat    {self.durbin_watson_:<12.6f}\")\n",
    "        print(f\"Prob(F-statistic) {self.f_pvalue_:<12.6f}\")\n",
    "    \n",
    "    def predict(self, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Realizar predicciones usando el modelo ajustado\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"El modelo debe ser ajustado primero\")\n",
    "        \n",
    "        X_linear = np.asarray(X_linear)\n",
    "        X_nonlinear = np.asarray(X_nonlinear)\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        G = self._logistic_transition(z, self.gamma_, self.threshold_)\n",
    "        y_pred = X_linear @ self.beta_linear_ + G * (X_nonlinear @ self.beta_nonlinear_)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def prepare_data_from_series(self, y, max_lags=4):\n",
    "        \"\"\"\n",
    "        Preparar datos para STR usando solo la serie de tiempo y,\n",
    "        creando rezagos automáticamente como en EViews\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like\n",
    "            Serie de tiempo (ej: TB3MS)\n",
    "        max_lags : int\n",
    "            Número máximo de rezagos a incluir\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_clean : array\n",
    "            Variable dependiente limpia (sin NaN)\n",
    "        X_linear : array\n",
    "            Matriz de variables para parte lineal [C, y(-1), y(-2), ..., y(-max_lags)]\n",
    "        X_nonlinear : array  \n",
    "            Matriz de variables para parte no lineal [C, y(-1), y(-2), ..., y(-max_lags)]\n",
    "        z : array\n",
    "            Variable de transición y(-1)\n",
    "        \"\"\"\n",
    "        y = np.asarray(y).flatten()\n",
    "        n = len(y)\n",
    "        \n",
    "        # Crear matriz con rezagos\n",
    "        data_matrix = np.zeros((n - max_lags, max_lags + 2))  # +2 para y actual y constante\n",
    "        \n",
    "        for i in range(max_lags, n):\n",
    "            data_matrix[i - max_lags, 0] = y[i]  # y actual\n",
    "            data_matrix[i - max_lags, 1] = 1.0   # constante\n",
    "            for lag in range(1, max_lags + 1):\n",
    "                data_matrix[i - max_lags, lag + 1] = y[i - lag]  # y(-lag)\n",
    "        \n",
    "        # Separar variables\n",
    "        y_clean = data_matrix[:, 0]  # Variable dependiente\n",
    "        X_vars = data_matrix[:, 1:]   # [C, y(-1), y(-2), ..., y(-max_lags)]\n",
    "        z = data_matrix[:, 2]         # Variable de transición = y(-1)\n",
    "        \n",
    "        # Las mismas variables para parte lineal y no lineal\n",
    "        X_linear = X_vars.copy()\n",
    "        X_nonlinear = X_vars.copy()\n",
    "        \n",
    "        return y_clean, X_linear, X_nonlinear, z\n",
    "    \n",
    "    def fit_from_series(self, y, max_lags=4, **kwargs):\n",
    "        \"\"\"\n",
    "        Ajustar modelo STR directamente desde una serie de tiempo\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like\n",
    "            Serie de tiempo\n",
    "        max_lags : int\n",
    "            Número de rezagos a incluir\n",
    "        **kwargs : dict\n",
    "            Argumentos adicionales para fit()\n",
    "        \"\"\"\n",
    "        # Preparar datos\n",
    "        y_clean, X_linear, X_nonlinear, z = self.prepare_data_from_series(y, max_lags)\n",
    "        \n",
    "        # Guardar información para el summary\n",
    "        self.max_lags_ = max_lags\n",
    "        self.n_original_ = len(y)\n",
    "        self.n_used_ = len(y_clean)\n",
    "        \n",
    "        # Ajustar modelo\n",
    "        return self.fit(y_clean, X_linear, X_nonlinear, z, **kwargs)\n",
    "\n",
    "# Función auxiliar para uso fácil\n",
    "def fit_str_model(y, max_lags=4):\n",
    "    \"\"\"\n",
    "    Función conveniente para ajustar STR con solo una serie\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Serie de tiempo\n",
    "    max_lags : int\n",
    "        Número de rezagos\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : SmoothThresholdRegression\n",
    "        Modelo ajustado\n",
    "    \"\"\"\n",
    "    model = SmoothThresholdRegression()\n",
    "    model.fit_from_series(y, max_lags=max_lags)\n",
    "    \n",
    "    # Mostrar resultados automáticamente\n",
    "    var_names = ['C'] + [f'Y(-{i})' for i in range(1, max_lags + 1)]\n",
    "    model.summary(var_names, var_names)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f54bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando búsqueda en grilla para valores iniciales...\n",
      "Valores iniciales encontrados. Comenzando optimización...\n",
      "Advertencia: La optimización no convergió. Mensaje: Desired error not necessarily achieved due to precision loss.\n",
      "================================================================================\n",
      "Dependent Variable: Y\n",
      "Method: Smooth Threshold Regression\n",
      "Transition function: Logistic\n",
      "Date: 06/20/25   Time: 19:16\n",
      "Included observations: 12\n",
      "Threshold variable: Z(-1)\n",
      "Starting values: Grid search with concentrated regression coefficients\n",
      "Ordinary standard errors & covariance using outer product of gradients\n",
      "================================================================================\n",
      "\n",
      "Threshold Variables (linear part)\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "C               0.192169     0.202489     0.949033     0.0000  \n",
      "Y(-1)           1.541094     0.167759     9.186356     0.0000  \n",
      "Y(-2)           -0.843957    0.283244     -2.979616    0.0000  \n",
      "Y(-3)           0.423560     0.262933     1.610906     0.0000  \n",
      "Y(-4)           -0.158507    0.145418     -1.090008    0.0000  \n",
      "\n",
      "Threshold Variables (nonlinear part)\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "C               3.325182     1.401066     2.373323     0.0000  \n",
      "Y(-1)           -0.582852    0.227764     -2.559016    0.0000  \n",
      "Y(-2)           0.221086     0.341559     0.647284     0.0000  \n",
      "Y(-3)           0.181759     0.330352     0.550197     0.0000  \n",
      "Y(-4)           -0.106117    0.209311     -0.506983    0.0000  \n",
      "\n",
      "Slopes\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "SLOPE           21.329863    787.342979   2.709094e-02 0.0000  \n",
      "\n",
      "Thresholds\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "THRESHOLD       8.793376     0.350908     25.058915    0.0000  \n",
      "\n",
      "R-squared         0.945101        Mean dependent var    5.354697    \n",
      "Adjusted R-squared0.941855        S.D. dependent var    2.769797    \n",
      "S.E. of regression0.667891        Akaike info criterion 413.682975  \n",
      "Sum squared resid 0.000000        Schwarz criterion     453.142179  \n",
      "Log likelihood    -194.841488     Hannan-Quinn criter.  429.654749  \n",
      "F-statistic       291.096356      Durbin-Watson stat    2.001977    \n",
      "Prob(F-statistic) 0.000000    \n"
     ]
    }
   ],
   "source": [
    "y = df_sub[\"tb3ms\"].astype(float).dropna()\n",
    "\n",
    "str_model = SmoothThresholdRegression()\n",
    "str_model.fit_from_series(y, max_lags=4)\n",
    "var_names = ['C', 'Y(-1)', 'Y(-2)', 'Y(-3)', 'Y(-4)']\n",
    "str_model.summary(var_names, var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7704260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SmoothThresholdRegression:\n",
    "    \"\"\"\n",
    "    Clase para estimación de modelos de regresión de umbral suave (STR)\n",
    "    con función de transición logística, replicando la salida de EViews.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "        self.results_ = {}\n",
    "        \n",
    "    def _logistic_transition(self, z, gamma, c):\n",
    "        \"\"\"\n",
    "        Función de transición logística: G(z; gamma, c) = 1 / (1 + exp(-gamma * (z - c)))\n",
    "        \"\"\"\n",
    "        # Evitar overflow/underflow\n",
    "        arg = gamma * (z - c)\n",
    "        arg = np.clip(arg, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-arg))\n",
    "    \n",
    "    def _concentrated_regression(self, gamma, c, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Regresión concentrada: estima betas dados gamma y c\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calcular función de transición\n",
    "            G = self._logistic_transition(z, gamma, c)\n",
    "            \n",
    "            # Construir matriz de regresores\n",
    "            X_combined = np.column_stack([X_linear, G.reshape(-1, 1) * X_nonlinear])\n",
    "            \n",
    "            # Resolver sistema de ecuaciones normales\n",
    "            XtX = X_combined.T @ X_combined\n",
    "            Xty = X_combined.T @ y\n",
    "            \n",
    "            # Usar SVD para mayor estabilidad numérica\n",
    "            U, s, Vt = np.linalg.svd(XtX, full_matrices=False)\n",
    "            \n",
    "            # Eliminar valores singulares muy pequeños\n",
    "            tol = 1e-12\n",
    "            s_inv = np.where(s > tol, 1.0/s, 0.0)\n",
    "            \n",
    "            # Calcular coeficientes\n",
    "            beta_combined = Vt.T @ np.diag(s_inv) @ U.T @ Xty\n",
    "            \n",
    "            # Predicciones y residuos\n",
    "            y_pred = X_combined @ beta_combined\n",
    "            residuals = y - y_pred\n",
    "            ssr = np.sum(residuals**2)\n",
    "            \n",
    "            return beta_combined, ssr, residuals\n",
    "            \n",
    "        except (np.linalg.LinAlgError, ValueError):\n",
    "            return None, np.inf, None\n",
    "    \n",
    "    def _objective_function(self, nonlinear_params, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Función objetivo para optimización (SSR)\n",
    "        \"\"\"\n",
    "        if len(nonlinear_params) == 2:\n",
    "            gamma, c = nonlinear_params\n",
    "        else:\n",
    "            gamma = abs(nonlinear_params[0])  # Asegurar gamma > 0\n",
    "            c = nonlinear_params[1]\n",
    "        \n",
    "        # Regresión concentrada\n",
    "        beta_combined, ssr, _ = self._concentrated_regression(gamma, c, y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        if beta_combined is None:\n",
    "            return 1e10\n",
    "            \n",
    "        return ssr\n",
    "    \n",
    "    def _grid_search_initial_values(self, y, X_linear, X_nonlinear, z, n_grid=25):\n",
    "        \"\"\"\n",
    "        Búsqueda en grilla mejorada para valores iniciales\n",
    "        \"\"\"\n",
    "        print(\"Realizando búsqueda en grilla para valores iniciales...\")\n",
    "        \n",
    "        # Rangos más amplios para la búsqueda\n",
    "        z_sorted = np.sort(z)\n",
    "        z_min, z_max = z_sorted[int(0.1*len(z))], z_sorted[int(0.9*len(z))]\n",
    "        c_grid = np.linspace(z_min, z_max, n_grid)\n",
    "        \n",
    "        # Rango logarítmico para gamma\n",
    "        gamma_grid = np.logspace(-1, 3, n_grid)  # De 0.1 a 1000\n",
    "        \n",
    "        best_ssr = np.inf\n",
    "        best_gamma = None\n",
    "        best_c = None\n",
    "        best_beta = None\n",
    "        \n",
    "        for c in c_grid:\n",
    "            for gamma in gamma_grid:\n",
    "                beta_combined, ssr, _ = self._concentrated_regression(gamma, c, y, X_linear, X_nonlinear, z)\n",
    "                \n",
    "                if beta_combined is not None and ssr < best_ssr:\n",
    "                    best_ssr = ssr\n",
    "                    best_gamma = gamma\n",
    "                    best_c = c\n",
    "                    best_beta = beta_combined\n",
    "        \n",
    "        if best_beta is None:\n",
    "            raise ValueError(\"No se pudieron encontrar valores iniciales válidos\")\n",
    "        \n",
    "        print(f\"Mejores valores iniciales: gamma={best_gamma:.4f}, threshold={best_c:.4f}, SSR={best_ssr:.6f}\")\n",
    "        return best_gamma, best_c, best_beta\n",
    "    \n",
    "    def fit(self, y, X_linear, X_nonlinear, z, max_iter=200, tol=1e-8):\n",
    "        \"\"\"\n",
    "        Ajustar el modelo STR\n",
    "        \"\"\"\n",
    "        # Convertir a numpy arrays y validar\n",
    "        y = np.asarray(y).flatten()\n",
    "        X_linear = np.asarray(X_linear)\n",
    "        X_nonlinear = np.asarray(X_nonlinear)\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        n = len(y)\n",
    "        if X_linear.shape[0] != n or X_nonlinear.shape[0] != n or len(z) != n:\n",
    "            raise ValueError(\"Las dimensiones no coinciden\")\n",
    "        \n",
    "        # Guardar datos originales\n",
    "        self.y_ = y\n",
    "        self.X_linear_ = X_linear\n",
    "        self.X_nonlinear_ = X_nonlinear\n",
    "        self.z_ = z\n",
    "        self.n_obs_ = n\n",
    "        \n",
    "        # Búsqueda en grilla para valores iniciales\n",
    "        initial_gamma, initial_c, initial_beta = self._grid_search_initial_values(\n",
    "            y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        print(\"Comenzando optimización...\")\n",
    "        \n",
    "        # Múltiples intentos de optimización\n",
    "        best_result = None\n",
    "        best_ssr = np.inf\n",
    "        \n",
    "        methods = ['L-BFGS-B', 'SLSQP', 'TNC']\n",
    "        \n",
    "        for method in methods:\n",
    "            try:\n",
    "                # Configurar bounds para gamma > 0\n",
    "                bounds = [(1e-6, 1000), (z.min(), z.max())]\n",
    "                \n",
    "                result = minimize(\n",
    "                    self._objective_function,\n",
    "                    [initial_gamma, initial_c],\n",
    "                    args=(y, X_linear, X_nonlinear, z),\n",
    "                    method=method,\n",
    "                    bounds=bounds,\n",
    "                    options={'maxiter': max_iter, 'ftol': tol}\n",
    "                )\n",
    "                \n",
    "                if result.fun < best_ssr:\n",
    "                    best_ssr = result.fun\n",
    "                    best_result = result\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Usar evolución diferencial como respaldo\n",
    "        if best_result is None or not best_result.success:\n",
    "            print(\"Optimización local fallida, probando evolución diferencial...\")\n",
    "            bounds = [(1e-6, 1000), (z.min(), z.max())]\n",
    "            \n",
    "            result = differential_evolution(\n",
    "                self._objective_function,\n",
    "                bounds,\n",
    "                args=(y, X_linear, X_nonlinear, z),\n",
    "                maxiter=max_iter,\n",
    "                tol=tol,\n",
    "                seed=42\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        if best_result is None:\n",
    "            raise RuntimeError(\"La optimización falló completamente\")\n",
    "        \n",
    "        # Extraer parámetros finales\n",
    "        final_gamma = abs(best_result.x[0])\n",
    "        final_c = best_result.x[1]\n",
    "        \n",
    "        # Regresión final\n",
    "        final_beta, final_ssr, final_residuals = self._concentrated_regression(\n",
    "            final_gamma, final_c, y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        if final_beta is None:\n",
    "            raise RuntimeError(\"Error en regresión final\")\n",
    "        \n",
    "        # Guardar parámetros\n",
    "        n_linear = X_linear.shape[1]\n",
    "        self.beta_linear_ = final_beta[:n_linear]\n",
    "        self.beta_nonlinear_ = final_beta[n_linear:]\n",
    "        self.gamma_ = final_gamma\n",
    "        self.threshold_ = final_c\n",
    "        self.residuals_ = final_residuals\n",
    "        self.ssr_ = final_ssr\n",
    "        \n",
    "        # Calcular estadísticos\n",
    "        self._calculate_statistics()\n",
    "        \n",
    "        iterations = getattr(best_result, 'nit', 0)\n",
    "        print(f\"Convergencia alcanzada después de {iterations} iteraciones\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _calculate_statistics(self):\n",
    "        \"\"\"\n",
    "        Calcular todos los estadísticos del modelo\n",
    "        \"\"\"\n",
    "        y = self.y_\n",
    "        n = self.n_obs_\n",
    "        \n",
    "        # Función de transición final\n",
    "        G = self._logistic_transition(self.z_, self.gamma_, self.threshold_)\n",
    "        \n",
    "        # Predicciones\n",
    "        self.fitted_values_ = (self.X_linear_ @ self.beta_linear_ + \n",
    "                              G * (self.X_nonlinear_ @ self.beta_nonlinear_))\n",
    "        \n",
    "        # Estadísticos básicos\n",
    "        tss = np.sum((y - np.mean(y))**2)\n",
    "        \n",
    "        # Grados de libertad\n",
    "        k = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "        self.df_resid_ = n - k\n",
    "        self.df_model_ = k - 1\n",
    "        \n",
    "        # R-squared y ajustado\n",
    "        self.r_squared_ = 1 - self.ssr_/tss\n",
    "        self.adj_r_squared_ = 1 - (self.ssr_/self.df_resid_)/(tss/(n-1))\n",
    "        \n",
    "        # Error estándar de regresión\n",
    "        self.se_regression_ = np.sqrt(self.ssr_/self.df_resid_)\n",
    "        \n",
    "        # Estadístico F\n",
    "        msr = (tss - self.ssr_)/self.df_model_\n",
    "        mse = self.ssr_/self.df_resid_\n",
    "        self.f_statistic_ = msr/mse if mse > 0 else np.inf\n",
    "        self.f_pvalue_ = 1 - stats.f.cdf(self.f_statistic_, self.df_model_, self.df_resid_)\n",
    "        \n",
    "        # Log-likelihood\n",
    "        self.log_likelihood_ = -n/2 * (np.log(2*np.pi) + np.log(self.ssr_/n) + 1)\n",
    "        \n",
    "        # Criterios de información\n",
    "        self.aic_ = 2*k - 2*self.log_likelihood_\n",
    "        self.bic_ = k*np.log(n) - 2*self.log_likelihood_\n",
    "        self.hqic_ = 2*k*np.log(np.log(n)) - 2*self.log_likelihood_\n",
    "        \n",
    "        # Durbin-Watson\n",
    "        diff_resid = np.diff(self.residuals_)\n",
    "        self.durbin_watson_ = np.sum(diff_resid**2) / self.ssr_\n",
    "        \n",
    "        # Variable dependiente\n",
    "        self.mean_dependent_ = np.mean(y)\n",
    "        self.std_dependent_ = np.std(y, ddof=1)\n",
    "        \n",
    "        # Calcular errores estándar\n",
    "        self._calculate_standard_errors()\n",
    "    \n",
    "    def _calculate_standard_errors(self):\n",
    "        \"\"\"\n",
    "        Calcular errores estándar usando matriz Hessiana numérica\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Parámetros completos\n",
    "            all_params = np.concatenate([\n",
    "                self.beta_linear_, \n",
    "                self.beta_nonlinear_, \n",
    "                [self.gamma_, self.threshold_]\n",
    "            ])\n",
    "            \n",
    "            # Calcular Hessiana numérica\n",
    "            hessian = self._numerical_hessian(all_params)\n",
    "            \n",
    "            # Matriz de covarianza\n",
    "            cov_matrix = np.linalg.inv(hessian) * (self.ssr_ / self.df_resid_)\n",
    "            \n",
    "            self.std_errors_ = np.sqrt(np.diag(cov_matrix))\n",
    "            \n",
    "        except (np.linalg.LinAlgError, ValueError):\n",
    "            print(\"Advertencia: No se pudieron calcular errores estándar precisos\")\n",
    "            k_total = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "            self.std_errors_ = np.full(k_total, self.se_regression_)\n",
    "    \n",
    "    def _numerical_hessian(self, params, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Calcular Hessiana numérica de la función objetivo\n",
    "        \"\"\"\n",
    "        n_params = len(params)\n",
    "        hessian = np.zeros((n_params, n_params))\n",
    "        \n",
    "        def obj_func(p):\n",
    "            return self._full_objective_function(p)\n",
    "        \n",
    "        f0 = obj_func(params)\n",
    "        \n",
    "        for i in range(n_params):\n",
    "            for j in range(i, n_params):\n",
    "                # Perturbaciones\n",
    "                params_pp = params.copy()\n",
    "                params_pm = params.copy()\n",
    "                params_mp = params.copy()\n",
    "                params_mm = params.copy()\n",
    "                \n",
    "                params_pp[i] += eps\n",
    "                params_pp[j] += eps\n",
    "                \n",
    "                params_pm[i] += eps\n",
    "                params_pm[j] -= eps\n",
    "                \n",
    "                params_mp[i] -= eps\n",
    "                params_mp[j] += eps\n",
    "                \n",
    "                params_mm[i] -= eps\n",
    "                params_mm[j] -= eps\n",
    "                \n",
    "                # Aproximación de segunda derivada\n",
    "                fpp = obj_func(params_pp)\n",
    "                fpm = obj_func(params_pm)\n",
    "                fmp = obj_func(params_mp)\n",
    "                fmm = obj_func(params_mm)\n",
    "                \n",
    "                hessian[i, j] = (fpp - fpm - fmp + fmm) / (4 * eps**2)\n",
    "                hessian[j, i] = hessian[i, j]  # Simétrica\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def _full_objective_function(self, params):\n",
    "        \"\"\"\n",
    "        Función objetivo completa para cálculo de Hessiana\n",
    "        \"\"\"\n",
    "        n_linear = self.X_linear_.shape[1]\n",
    "        n_nonlinear = self.X_nonlinear_.shape[1]\n",
    "        \n",
    "        beta_linear = params[:n_linear]\n",
    "        beta_nonlinear = params[n_linear:n_linear + n_nonlinear]\n",
    "        gamma = abs(params[-2])\n",
    "        c = params[-1]\n",
    "        \n",
    "        # Calcular predicciones\n",
    "        G = self._logistic_transition(self.z_, gamma, c)\n",
    "        y_pred = (self.X_linear_ @ beta_linear + \n",
    "                 G * (self.X_nonlinear_ @ beta_nonlinear))\n",
    "        \n",
    "        # SSR\n",
    "        residuals = self.y_ - y_pred\n",
    "        return np.sum(residuals**2)\n",
    "    \n",
    "    def prepare_data_from_series(self, y, max_lags=4):\n",
    "        \"\"\"\n",
    "        Preparar datos para STR usando solo la serie de tiempo y\n",
    "        \"\"\"\n",
    "        y = np.asarray(y).flatten()\n",
    "        n = len(y)\n",
    "        \n",
    "        # Crear matriz con rezagos\n",
    "        data_matrix = np.zeros((n - max_lags, max_lags + 2))\n",
    "        \n",
    "        for i in range(max_lags, n):\n",
    "            data_matrix[i - max_lags, 0] = y[i]  # y actual\n",
    "            data_matrix[i - max_lags, 1] = 1.0   # constante\n",
    "            for lag in range(1, max_lags + 1):\n",
    "                data_matrix[i - max_lags, lag + 1] = y[i - lag]\n",
    "        \n",
    "        # Separar variables\n",
    "        y_clean = data_matrix[:, 0]\n",
    "        X_vars = data_matrix[:, 1:]\n",
    "        z = data_matrix[:, 2]  # y(-1) como variable de transición\n",
    "        \n",
    "        X_linear = X_vars.copy()\n",
    "        X_nonlinear = X_vars.copy()\n",
    "        \n",
    "        return y_clean, X_linear, X_nonlinear, z\n",
    "    \n",
    "    def fit_from_series(self, y, max_lags=4, **kwargs):\n",
    "        \"\"\"\n",
    "        Ajustar modelo STR directamente desde una serie de tiempo\n",
    "        \"\"\"\n",
    "        y_clean, X_linear, X_nonlinear, z = self.prepare_data_from_series(y, max_lags)\n",
    "        \n",
    "        self.max_lags_ = max_lags\n",
    "        self.n_original_ = len(y)\n",
    "        self.n_used_ = len(y_clean)\n",
    "        \n",
    "        return self.fit(y_clean, X_linear, X_nonlinear, z, **kwargs)\n",
    "    \n",
    "    def summary(self, variable_names_linear=None, variable_names_nonlinear=None):\n",
    "        \"\"\"\n",
    "        Mostrar resumen estilo EViews\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"El modelo debe ser ajustado primero\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"Dependent Variable: Y\")\n",
    "        print(\"Method: Smooth Threshold Regression\")\n",
    "        print(\"Transition function: Logistic\")\n",
    "        print(\"Date: 06/20/25   Time: 19:16\")\n",
    "        print(f\"Included observations: {self.n_obs_}\")\n",
    "        print(\"Threshold variable: Y(-1)\")\n",
    "        print(\"Starting values: Grid search with concentrated regression coefficients\")\n",
    "        print(\"Ordinary standard errors & covariance using outer product of gradients\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Nombres por defecto\n",
    "        if variable_names_linear is None:\n",
    "            variable_names_linear = ['C'] + [f'Y(-{i})' for i in range(1, len(self.beta_linear_))]\n",
    "        if variable_names_nonlinear is None:\n",
    "            variable_names_nonlinear = ['C'] + [f'Y(-{i})' for i in range(1, len(self.beta_nonlinear_))]\n",
    "        \n",
    "        # Parte lineal\n",
    "        print(\"Threshold Variables (linear part)\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        for i, (name, coef) in enumerate(zip(variable_names_linear, self.beta_linear_)):\n",
    "            std_err = self.std_errors_[i]\n",
    "            t_stat = coef / std_err if std_err != 0 else 0\n",
    "            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_))\n",
    "            print(f\"{name:<15} {coef:<12.6f} {std_err:<12.6f} {t_stat:<12.6f} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Threshold Variables (nonlinear part)\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        offset = len(self.beta_linear_)\n",
    "        for i, (name, coef) in enumerate(zip(variable_names_nonlinear, self.beta_nonlinear_)):\n",
    "            idx = offset + i\n",
    "            std_err = self.std_errors_[idx]\n",
    "            t_stat = coef / std_err if std_err != 0 else 0\n",
    "            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_))\n",
    "            print(f\"{name:<15} {coef:<12.6f} {std_err:<12.6f} {t_stat:<12.6f} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Slopes\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        idx = len(self.beta_linear_) + len(self.beta_nonlinear_)\n",
    "        std_err = self.std_errors_[idx]\n",
    "        t_stat = self.gamma_ / std_err if std_err != 0 else 0\n",
    "        p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_))\n",
    "        print(f\"{'SLOPE':<15} {self.gamma_:<12.6f} {std_err:<12.6f} {t_stat:<12.6e} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Thresholds\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        idx = len(self.beta_linear_) + len(self.beta_nonlinear_) + 1\n",
    "        std_err = self.std_errors_[idx]\n",
    "        t_stat = self.threshold_ / std_err if std_err != 0 else 0\n",
    "        p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_))\n",
    "        print(f\"{'THRESHOLD':<15} {self.threshold_:<12.6f} {std_err:<12.6f} {t_stat:<12.6f} {p_val:<8.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"R-squared         {self.r_squared_:<12.6f}    Mean dependent var    {self.mean_dependent_:<12.6f}\")\n",
    "        print(f\"Adjusted R-squared{self.adj_r_squared_:<12.6f}    S.D. dependent var    {self.std_dependent_:<12.6f}\")\n",
    "        print(f\"S.E. of regression{self.se_regression_:<12.6f}    Akaike info criterion {self.aic_:<12.6f}\")\n",
    "        print(f\"Sum squared resid {self.ssr_:<12.6f}    Schwarz criterion     {self.bic_:<12.6f}\")\n",
    "        print(f\"Log likelihood    {self.log_likelihood_:<12.6f}    Hannan-Quinn criter.  {self.hqic_:<12.6f}\")\n",
    "        print(f\"F-statistic       {self.f_statistic_:<12.6f}    Durbin-Watson stat    {self.durbin_watson_:<12.6f}\")\n",
    "        print(f\"Prob(F-statistic) {self.f_pvalue_:<12.6f}\")\n",
    "    \n",
    "    def predict(self, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Realizar predicciones\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"El modelo debe ser ajustado primero\")\n",
    "        \n",
    "        X_linear = np.asarray(X_linear)\n",
    "        X_nonlinear = np.asarray(X_nonlinear)\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        G = self._logistic_transition(z, self.gamma_, self.threshold_)\n",
    "        y_pred = X_linear @ self.beta_linear_ + G * (X_nonlinear @ self.beta_nonlinear_)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "# Función auxiliar para uso fácil\n",
    "def fit_str_model(y, max_lags=4):\n",
    "    \"\"\"\n",
    "    Función conveniente para ajustar STR con solo una serie\n",
    "    \"\"\"\n",
    "    model = SmoothThresholdRegression()\n",
    "    model.fit_from_series(y, max_lags=max_lags)\n",
    "    \n",
    "    var_names = ['C'] + [f'Y(-{i})' for i in range(1, max_lags + 1)]\n",
    "    model.summary(var_names, var_names)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd513d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando búsqueda en grilla para valores iniciales...\n",
      "Mejores valores iniciales: gamma=21.5443, threshold=8.8033, SSR=82.976626\n",
      "Comenzando optimización...\n",
      "Convergencia alcanzada después de 5 iteraciones\n",
      "================================================================================\n",
      "Dependent Variable: Y\n",
      "Method: Smooth Threshold Regression\n",
      "Transition function: Logistic\n",
      "Date: 06/20/25   Time: 19:16\n",
      "Included observations: 198\n",
      "Threshold variable: Y(-1)\n",
      "Starting values: Grid search with concentrated regression coefficients\n",
      "Ordinary standard errors & covariance using outer product of gradients\n",
      "================================================================================\n",
      "\n",
      "Threshold Variables (linear part)\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "C               0.192167     0.094779     2.027542     0.0440  \n",
      "Y(-1)           1.541094     0.077671     19.841330    0.0000  \n",
      "Y(-2)           -0.843956    0.133370     -6.327948    0.0000  \n",
      "Y(-3)           0.423558     0.124240     3.409209     0.0008  \n",
      "Y(-4)           -0.158507    0.068653     -2.308810    0.0221  \n",
      "\n",
      "Threshold Variables (nonlinear part)\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "C               3.325180     0.660148     5.037024     0.0000  \n",
      "Y(-1)           -0.582853    0.106996     -5.447404    0.0000  \n",
      "Y(-2)           0.221085     0.160624     1.376406     0.1704  \n",
      "Y(-3)           0.181761     0.155894     1.165930     0.2451  \n",
      "Y(-4)           -0.106118    0.098789     -1.074182    0.2841  \n",
      "\n",
      "Slopes\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "SLOPE           21.345575    nan          nan          nan     \n",
      "\n",
      "Thresholds\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "THRESHOLD       8.793386     0.060704     144.856169   0.0000  \n",
      "\n",
      "R-squared         0.945101        Mean dependent var    5.354697    \n",
      "Adjusted R-squared0.941855        S.D. dependent var    2.769797    \n",
      "S.E. of regression0.667891        Akaike info criterion 413.682975  \n",
      "Sum squared resid 82.970556       Schwarz criterion     453.142180  \n",
      "Log likelihood    -194.841488     Hannan-Quinn criter.  429.654750  \n",
      "F-statistic       291.096356      Durbin-Watson stat    2.001978    \n",
      "Prob(F-statistic) 0.000000    \n"
     ]
    }
   ],
   "source": [
    "y = df_sub[\"tb3ms\"].astype(float).dropna()\n",
    "# Ajustar modelo\n",
    "model = fit_str_model(y, max_lags=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de64a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SmoothThresholdRegression:\n",
    "    \"\"\"\n",
    "    Clase mejorada para estimación de modelos STR compatible con EViews\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "        self.results_ = {}\n",
    "        \n",
    "    def _logistic_transition(self, z, gamma, c):\n",
    "        \"\"\"\n",
    "        Función de transición logística mejorada\n",
    "        \"\"\"\n",
    "        # Usar mayor precisión para evitar overflow\n",
    "        arg = gamma * (z - c)\n",
    "        arg = np.clip(arg, -700, 700)  # Rango más amplio\n",
    "        return 1.0 / (1.0 + np.exp(-arg))\n",
    "    \n",
    "    def _concentrated_regression(self, gamma, c, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Regresión concentrada con mejor manejo numérico\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calcular función de transición\n",
    "            G = self._logistic_transition(z, gamma, c)\n",
    "            \n",
    "            # Construir matriz de regresores\n",
    "            X_combined = np.column_stack([X_linear, G.reshape(-1, 1) * X_nonlinear])\n",
    "            \n",
    "            # Usar QR decomposition para mayor estabilidad\n",
    "            Q, R = np.linalg.qr(X_combined)\n",
    "            \n",
    "            # Resolver sistema usando QR\n",
    "            beta_combined = np.linalg.solve(R, Q.T @ y)\n",
    "            \n",
    "            # Predicciones y residuos\n",
    "            y_pred = X_combined @ beta_combined\n",
    "            residuals = y - y_pred\n",
    "            ssr = np.sum(residuals**2)\n",
    "            \n",
    "            return beta_combined, ssr, residuals, X_combined\n",
    "            \n",
    "        except (np.linalg.LinAlgError, ValueError):\n",
    "            return None, np.inf, None, None\n",
    "    \n",
    "    def _objective_function(self, nonlinear_params, y, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Función objetivo mejorada\n",
    "        \"\"\"\n",
    "        gamma = abs(nonlinear_params[0]) + 1e-6  # Evitar gamma = 0\n",
    "        c = nonlinear_params[1]\n",
    "        \n",
    "        beta_combined, ssr, _, _ = self._concentrated_regression(gamma, c, y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        if beta_combined is None or not np.isfinite(ssr):\n",
    "            return 1e12\n",
    "            \n",
    "        return ssr\n",
    "    \n",
    "    def _grid_search_initial_values(self, y, X_linear, X_nonlinear, z, n_grid=30):\n",
    "        \"\"\"\n",
    "        Búsqueda en grilla más exhaustiva\n",
    "        \"\"\"\n",
    "        print(\"Realizando búsqueda en grilla para valores iniciales...\")\n",
    "        \n",
    "        # Percentiles para threshold más robustos\n",
    "        z_sorted = np.sort(z)\n",
    "        c_min = np.percentile(z_sorted, 15)\n",
    "        c_max = np.percentile(z_sorted, 85)\n",
    "        c_grid = np.linspace(c_min, c_max, n_grid)\n",
    "        \n",
    "        # Rango más amplio para gamma con distribución logarítmica\n",
    "        gamma_grid = np.logspace(-1, 4, n_grid)  # De 0.1 a 10000\n",
    "        \n",
    "        best_ssr = np.inf\n",
    "        best_params = None\n",
    "        \n",
    "        for c in c_grid:\n",
    "            for gamma in gamma_grid:\n",
    "                beta_combined, ssr, _, _ = self._concentrated_regression(gamma, c, y, X_linear, X_nonlinear, z)\n",
    "                \n",
    "                if beta_combined is not None and ssr < best_ssr and np.isfinite(ssr):\n",
    "                    best_ssr = ssr\n",
    "                    best_params = (gamma, c, beta_combined)\n",
    "        \n",
    "        if best_params is None:\n",
    "            raise ValueError(\"No se pudieron encontrar valores iniciales válidos\")\n",
    "        \n",
    "        gamma, c, beta = best_params\n",
    "        print(f\"Mejores valores iniciales: gamma={gamma:.4f}, threshold={c:.4f}, SSR={best_ssr:.6f}\")\n",
    "        return gamma, c, beta\n",
    "    \n",
    "    def fit(self, y, X_linear, X_nonlinear, z, max_iter=500, tol=1e-12):\n",
    "        \"\"\"\n",
    "        Ajustar el modelo STR con optimización mejorada\n",
    "        \"\"\"\n",
    "        # Validar y convertir datos\n",
    "        y = np.asarray(y).flatten()\n",
    "        X_linear = np.asarray(X_linear)\n",
    "        X_nonlinear = np.asarray(X_nonlinear)\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        n = len(y)\n",
    "        if X_linear.shape[0] != n or X_nonlinear.shape[0] != n or len(z) != n:\n",
    "            raise ValueError(\"Las dimensiones no coinciden\")\n",
    "        \n",
    "        # Guardar datos\n",
    "        self.y_ = y\n",
    "        self.X_linear_ = X_linear\n",
    "        self.X_nonlinear_ = X_nonlinear\n",
    "        self.z_ = z\n",
    "        self.n_obs_ = n\n",
    "        \n",
    "        # Búsqueda inicial\n",
    "        initial_gamma, initial_c, initial_beta = self._grid_search_initial_values(\n",
    "            y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        print(\"Comenzando optimización...\")\n",
    "        \n",
    "        # Estrategia de optimización múltiple\n",
    "        best_result = None\n",
    "        best_ssr = np.inf\n",
    "        \n",
    "        # Bounds más restrictivos pero realistas\n",
    "        z_range = z.max() - z.min()\n",
    "        bounds = [\n",
    "            (1e-3, 10000),  # gamma\n",
    "            (z.min() - 0.1*z_range, z.max() + 0.1*z_range)  # threshold\n",
    "        ]\n",
    "        \n",
    "        # Múltiples puntos de inicio\n",
    "        starting_points = [\n",
    "            [initial_gamma, initial_c],\n",
    "            [initial_gamma * 0.1, initial_c],\n",
    "            [initial_gamma * 10, initial_c],\n",
    "            [initial_gamma, np.median(z)],\n",
    "            [100, np.percentile(z, 25)],\n",
    "            [100, np.percentile(z, 75)]\n",
    "        ]\n",
    "        \n",
    "        methods = ['L-BFGS-B', 'SLSQP', 'TNC']\n",
    "        \n",
    "        for start_point in starting_points:\n",
    "            for method in methods:\n",
    "                try:\n",
    "                    result = minimize(\n",
    "                        self._objective_function,\n",
    "                        start_point,\n",
    "                        args=(y, X_linear, X_nonlinear, z),\n",
    "                        method=method,\n",
    "                        bounds=bounds,\n",
    "                        options={\n",
    "                            'maxiter': max_iter, \n",
    "                            'ftol': tol,\n",
    "                            'gtol': tol\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    if result.fun < best_ssr and result.success:\n",
    "                        best_ssr = result.fun\n",
    "                        best_result = result\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        # Evolución diferencial como respaldo\n",
    "        if best_result is None:\n",
    "            print(\"Optimización local fallida, usando evolución diferencial...\")\n",
    "            result = differential_evolution(\n",
    "                self._objective_function,\n",
    "                bounds,\n",
    "                args=(y, X_linear, X_nonlinear, z),\n",
    "                maxiter=max_iter,\n",
    "                tol=tol,\n",
    "                seed=42,\n",
    "                popsize=20\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        if best_result is None:\n",
    "            raise RuntimeError(\"La optimización falló\")\n",
    "        \n",
    "        # Parámetros finales\n",
    "        final_gamma = abs(best_result.x[0])\n",
    "        final_c = best_result.x[1]\n",
    "        \n",
    "        # Regresión final\n",
    "        final_beta, final_ssr, final_residuals, X_combined = self._concentrated_regression(\n",
    "            final_gamma, final_c, y, X_linear, X_nonlinear, z)\n",
    "        \n",
    "        if final_beta is None:\n",
    "            raise RuntimeError(\"Error en regresión final\")\n",
    "        \n",
    "        # Guardar resultados\n",
    "        n_linear = X_linear.shape[1]\n",
    "        self.beta_linear_ = final_beta[:n_linear]\n",
    "        self.beta_nonlinear_ = final_beta[n_linear:]\n",
    "        self.gamma_ = final_gamma\n",
    "        self.threshold_ = final_c\n",
    "        self.residuals_ = final_residuals\n",
    "        self.ssr_ = final_ssr\n",
    "        self.X_combined_ = X_combined\n",
    "        \n",
    "        # Calcular estadísticos\n",
    "        self._calculate_statistics()\n",
    "        \n",
    "        iterations = getattr(best_result, 'nit', 0)\n",
    "        print(f\"Convergencia alcanzada después de {iterations} iteraciones\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _calculate_statistics(self):\n",
    "        \"\"\"\n",
    "        Calcular estadísticos con mayor precisión\n",
    "        \"\"\"\n",
    "        y = self.y_\n",
    "        n = self.n_obs_\n",
    "        \n",
    "        # Función de transición final\n",
    "        G = self._logistic_transition(self.z_, self.gamma_, self.threshold_)\n",
    "        \n",
    "        # Predicciones\n",
    "        self.fitted_values_ = (self.X_linear_ @ self.beta_linear_ + \n",
    "                              G * (self.X_nonlinear_ @ self.beta_nonlinear_))\n",
    "        \n",
    "        # Estadísticos básicos\n",
    "        tss = np.sum((y - np.mean(y))**2)\n",
    "        \n",
    "        # Grados de libertad\n",
    "        k = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "        self.df_resid_ = n - k\n",
    "        self.df_model_ = k - 1\n",
    "        \n",
    "        # R-squared\n",
    "        self.r_squared_ = 1 - self.ssr_/tss\n",
    "        self.adj_r_squared_ = 1 - (self.ssr_/self.df_resid_)/(tss/(n-1))\n",
    "        \n",
    "        # Error estándar de regresión\n",
    "        self.se_regression_ = np.sqrt(self.ssr_/self.df_resid_)\n",
    "        \n",
    "        # Estadístico F\n",
    "        if self.df_model_ > 0:\n",
    "            msr = (tss - self.ssr_)/self.df_model_\n",
    "            mse = self.ssr_/self.df_resid_\n",
    "            self.f_statistic_ = msr/mse if mse > 0 else np.inf\n",
    "            self.f_pvalue_ = 1 - stats.f.cdf(self.f_statistic_, self.df_model_, self.df_resid_)\n",
    "        else:\n",
    "            self.f_statistic_ = np.nan\n",
    "            self.f_pvalue_ = np.nan\n",
    "        \n",
    "        # Log-likelihood\n",
    "        self.log_likelihood_ = -n/2 * (np.log(2*np.pi) + np.log(self.ssr_/n) + 1)\n",
    "        \n",
    "        # Criterios de información\n",
    "        self.aic_ = 2*k - 2*self.log_likelihood_\n",
    "        self.bic_ = k*np.log(n) - 2*self.log_likelihood_\n",
    "        self.hqic_ = 2*k*np.log(np.log(n)) - 2*self.log_likelihood_\n",
    "        \n",
    "        # Durbin-Watson\n",
    "        if len(self.residuals_) > 1:\n",
    "            diff_resid = np.diff(self.residuals_)\n",
    "            self.durbin_watson_ = np.sum(diff_resid**2) / self.ssr_\n",
    "        else:\n",
    "            self.durbin_watson_ = np.nan\n",
    "        \n",
    "        # Variable dependiente\n",
    "        self.mean_dependent_ = np.mean(y)\n",
    "        self.std_dependent_ = np.std(y, ddof=1)\n",
    "        \n",
    "        # Calcular errores estándar mejorados\n",
    "        self._calculate_standard_errors_improved()\n",
    "    \n",
    "    def _calculate_standard_errors_improved(self):\n",
    "        \"\"\"\n",
    "        Cálculo mejorado de errores estándar usando matriz de información\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n = len(self.y_)\n",
    "            \n",
    "            # Matriz de información usando producto exterior de gradientes\n",
    "            # como hace EViews\n",
    "            gradients = self._compute_score_vectors()\n",
    "            \n",
    "            if gradients is not None:\n",
    "                # Matriz de información\n",
    "                info_matrix = gradients.T @ gradients / n\n",
    "                \n",
    "                # Matriz de covarianza\n",
    "                cov_matrix = np.linalg.inv(info_matrix)\n",
    "                \n",
    "                # Errores estándar\n",
    "                self.std_errors_ = np.sqrt(np.diag(cov_matrix))\n",
    "                \n",
    "                # Verificar si hay errores estándar inválidos\n",
    "                if np.any(~np.isfinite(self.std_errors_)):\n",
    "                    raise np.linalg.LinAlgError(\"Errores estándar no finitos\")\n",
    "                    \n",
    "            else:\n",
    "                raise ValueError(\"No se pudieron calcular gradientes\")\n",
    "                \n",
    "        except (np.linalg.LinAlgError, ValueError):\n",
    "            print(\"Advertencia: Usando aproximación para errores estándar\")\n",
    "            # Aproximación simple\n",
    "            k_total = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "            se_approx = self.se_regression_ / np.sqrt(n)\n",
    "            self.std_errors_ = np.full(k_total, se_approx)\n",
    "    \n",
    "    def _compute_score_vectors(self):\n",
    "        \"\"\"\n",
    "        Calcular vectores de score (gradientes) para cada observación\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n = len(self.y_)\n",
    "            k_total = len(self.beta_linear_) + len(self.beta_nonlinear_) + 2\n",
    "            \n",
    "            # Matriz de gradientes\n",
    "            gradients = np.zeros((n, k_total))\n",
    "            \n",
    "            # Parámetros actuales\n",
    "            G = self._logistic_transition(self.z_, self.gamma_, self.threshold_)\n",
    "            \n",
    "            # Residuos\n",
    "            residuals = self.residuals_\n",
    "            \n",
    "            # Gradientes respecto a betas lineales\n",
    "            for i in range(len(self.beta_linear_)):\n",
    "                gradients[:, i] = residuals * self.X_linear_[:, i]\n",
    "            \n",
    "            # Gradientes respecto a betas no lineales\n",
    "            offset = len(self.beta_linear_)\n",
    "            for i in range(len(self.beta_nonlinear_)):\n",
    "                gradients[:, offset + i] = residuals * G * self.X_nonlinear_[:, i]\n",
    "            \n",
    "            # Gradientes respecto a gamma y threshold (aproximación numérica)\n",
    "            eps = 1e-6\n",
    "            \n",
    "            # Gradient respecto a gamma\n",
    "            _, ssr_plus, _, _ = self._concentrated_regression(\n",
    "                self.gamma_ + eps, self.threshold_, self.y_, \n",
    "                self.X_linear_, self.X_nonlinear_, self.z_)\n",
    "            _, ssr_minus, _, _ = self._concentrated_regression(\n",
    "                self.gamma_ - eps, self.threshold_, self.y_, \n",
    "                self.X_linear_, self.X_nonlinear_, self.z_)\n",
    "            \n",
    "            if ssr_plus is not None and ssr_minus is not None:\n",
    "                grad_gamma = (ssr_plus - ssr_minus) / (2 * eps)\n",
    "                gradients[:, -2] = grad_gamma / n\n",
    "            \n",
    "            # Gradient respecto a threshold\n",
    "            _, ssr_plus, _, _ = self._concentrated_regression(\n",
    "                self.gamma_, self.threshold_ + eps, self.y_, \n",
    "                self.X_linear_, self.X_nonlinear_, self.z_)\n",
    "            _, ssr_minus, _, _ = self._concentrated_regression(\n",
    "                self.gamma_, self.threshold_ - eps, self.y_, \n",
    "                self.X_linear_, self.X_nonlinear_, self.z_)\n",
    "            \n",
    "            if ssr_plus is not None and ssr_minus is not None:\n",
    "                grad_threshold = (ssr_plus - ssr_minus) / (2 * eps)\n",
    "                gradients[:, -1] = grad_threshold / n\n",
    "            \n",
    "            return gradients\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def prepare_data_from_series(self, y, max_lags=4, drop_first=True):\n",
    "        \"\"\"\n",
    "        Preparar datos manteniendo más observaciones como EViews\n",
    "        \"\"\"\n",
    "        y = np.asarray(y).flatten()\n",
    "        n = len(y)\n",
    "        \n",
    "        if drop_first:\n",
    "            # Mantener más observaciones eliminando solo las necesarias\n",
    "            start_idx = max_lags\n",
    "        else:\n",
    "            start_idx = max_lags\n",
    "        \n",
    "        data_matrix = np.zeros((n - start_idx, max_lags + 2))\n",
    "        \n",
    "        for i in range(start_idx, n):\n",
    "            row_idx = i - start_idx\n",
    "            data_matrix[row_idx, 0] = y[i]  # y actual\n",
    "            data_matrix[row_idx, 1] = 1.0   # constante\n",
    "            for lag in range(1, max_lags + 1):\n",
    "                data_matrix[row_idx, lag + 1] = y[i - lag]\n",
    "        \n",
    "        # Separar variables\n",
    "        y_clean = data_matrix[:, 0]\n",
    "        X_vars = data_matrix[:, 1:]\n",
    "        z = data_matrix[:, 2]  # y(-1) como variable de transición\n",
    "        \n",
    "        X_linear = X_vars.copy()\n",
    "        X_nonlinear = X_vars.copy()\n",
    "        \n",
    "        return y_clean, X_linear, X_nonlinear, z\n",
    "    \n",
    "    def fit_from_series(self, y, max_lags=4, **kwargs):\n",
    "        \"\"\"\n",
    "        Ajustar modelo STR desde serie de tiempo\n",
    "        \"\"\"\n",
    "        y_clean, X_linear, X_nonlinear, z = self.prepare_data_from_series(y, max_lags)\n",
    "        \n",
    "        self.max_lags_ = max_lags\n",
    "        self.n_original_ = len(y)\n",
    "        self.n_used_ = len(y_clean)\n",
    "        \n",
    "        print(f\"Observaciones originales: {self.n_original_}\")\n",
    "        print(f\"Observaciones utilizadas: {self.n_used_}\")\n",
    "        \n",
    "        return self.fit(y_clean, X_linear, X_nonlinear, z, **kwargs)\n",
    "    \n",
    "    def summary(self, variable_names_linear=None, variable_names_nonlinear=None, \n",
    "                variable_name='Y', threshold_var=None):\n",
    "        \"\"\"\n",
    "        Resumen mejorado estilo EViews\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"El modelo debe ser ajustado primero\")\n",
    "        \n",
    "        if threshold_var is None:\n",
    "            threshold_var = f\"{variable_name}(-1)\"\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Dependent Variable: {variable_name}\")\n",
    "        print(\"Method: Smooth Threshold Regression\")\n",
    "        print(\"Transition function: Logistic\")\n",
    "        print(\"Date: 06/20/25   Time: 19:16\")\n",
    "        print(f\"Included observations: {self.n_obs_}\")\n",
    "        print(f\"Threshold variable: {threshold_var}\")\n",
    "        print(\"Starting values: Grid search with concentrated regression coefficients\")\n",
    "        print(\"Ordinary standard errors & covariance using outer product of gradients\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Nombres por defecto\n",
    "        if variable_names_linear is None:\n",
    "            variable_names_linear = ['C'] + [f'{variable_name}(-{i})' for i in range(1, len(self.beta_linear_))]\n",
    "        if variable_names_nonlinear is None:\n",
    "            variable_names_nonlinear = ['C'] + [f'{variable_name}(-{i})' for i in range(1, len(self.beta_nonlinear_))]\n",
    "        \n",
    "        # Formato de salida mejorado\n",
    "        def format_number(num, width=12, decimals=6):\n",
    "            if np.isnan(num) or np.isinf(num):\n",
    "                return \"nan\".ljust(width)\n",
    "            elif abs(num) < 1e-4 and num != 0:\n",
    "                return f\"{num:.2e}\".ljust(width)\n",
    "            else:\n",
    "                return f\"{num:.{decimals}f}\".ljust(width)\n",
    "        \n",
    "        # Parte lineal\n",
    "        print(\"Threshold Variables (linear part)\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        for i, (name, coef) in enumerate(zip(variable_names_linear, self.beta_linear_)):\n",
    "            if i < len(self.std_errors_):\n",
    "                std_err = self.std_errors_[i]\n",
    "                t_stat = coef / std_err if std_err != 0 and np.isfinite(std_err) else np.nan\n",
    "                p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_)) if np.isfinite(t_stat) else np.nan\n",
    "            else:\n",
    "                std_err = np.nan\n",
    "                t_stat = np.nan\n",
    "                p_val = np.nan\n",
    "                \n",
    "            print(f\"{name:<15} {format_number(coef)} {format_number(std_err)} {format_number(t_stat)} {format_number(p_val, 8, 4)}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Threshold Variables (nonlinear part)\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        offset = len(self.beta_linear_)\n",
    "        for i, (name, coef) in enumerate(zip(variable_names_nonlinear, self.beta_nonlinear_)):\n",
    "            idx = offset + i\n",
    "            if idx < len(self.std_errors_):\n",
    "                std_err = self.std_errors_[idx]\n",
    "                t_stat = coef / std_err if std_err != 0 and np.isfinite(std_err) else np.nan\n",
    "                p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_)) if np.isfinite(t_stat) else np.nan\n",
    "            else:\n",
    "                std_err = np.nan\n",
    "                t_stat = np.nan\n",
    "                p_val = np.nan\n",
    "                \n",
    "            print(f\"{name:<15} {format_number(coef)} {format_number(std_err)} {format_number(t_stat)} {format_number(p_val, 8, 4)}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Slopes\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        idx = len(self.beta_linear_) + len(self.beta_nonlinear_)\n",
    "        if idx < len(self.std_errors_):\n",
    "            std_err = self.std_errors_[idx]\n",
    "            t_stat = self.gamma_ / std_err if std_err != 0 and np.isfinite(std_err) else np.nan\n",
    "            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_)) if np.isfinite(t_stat) else np.nan\n",
    "        else:\n",
    "            std_err = np.nan\n",
    "            t_stat = np.nan\n",
    "            p_val = np.nan\n",
    "            \n",
    "        print(f\"{'SLOPE':<15} {format_number(self.gamma_)} {format_number(std_err)} {format_number(t_stat)} {format_number(p_val, 8, 4)}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Thresholds\")\n",
    "        print()\n",
    "        print(f\"{'Variable':<15} {'Coefficient':<12} {'Std. Error':<12} {'t-Statistic':<12} {'Prob.':<8}\")\n",
    "        print()\n",
    "        \n",
    "        idx = len(self.beta_linear_) + len(self.beta_nonlinear_) + 1\n",
    "        if idx < len(self.std_errors_):\n",
    "            std_err = self.std_errors_[idx]\n",
    "            t_stat = self.threshold_ / std_err if std_err != 0 and np.isfinite(std_err) else np.nan\n",
    "            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), self.df_resid_)) if np.isfinite(t_stat) else np.nan\n",
    "        else:\n",
    "            std_err = np.nan\n",
    "            t_stat = np.nan\n",
    "            p_val = np.nan\n",
    "            \n",
    "        print(f\"{'THRESHOLD':<15} {format_number(self.threshold_)} {format_number(std_err)} {format_number(t_stat)} {format_number(p_val, 8, 4)}\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"R-squared         {self.r_squared_:<12.6f}    Mean dependent var    {self.mean_dependent_:<12.6f}\")\n",
    "        print(f\"Adjusted R-squared{self.adj_r_squared_:<12.6f}    S.D. dependent var    {self.std_dependent_:<12.6f}\")\n",
    "        print(f\"S.E. of regression{self.se_regression_:<12.6f}    Akaike info criterion {self.aic_:<12.6f}\")\n",
    "        print(f\"Sum squared resid {self.ssr_:<12.6f}    Schwarz criterion     {self.bic_:<12.6f}\")\n",
    "        print(f\"Log likelihood    {self.log_likelihood_:<12.6f}    Hannan-Quinn criter.  {self.hqic_:<12.6f}\")\n",
    "        print(f\"F-statistic       {self.f_statistic_:<12.6f}    Durbin-Watson stat    {self.durbin_watson_:<12.6f}\")\n",
    "        print(f\"Prob(F-statistic) {self.f_pvalue_:<12.6f}\")\n",
    "    \n",
    "    def predict(self, X_linear, X_nonlinear, z):\n",
    "        \"\"\"\n",
    "        Realizar predicciones\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"El modelo debe ser ajustado primero\")\n",
    "        \n",
    "        X_linear = np.asarray(X_linear)\n",
    "        X_nonlinear = np.asarray(X_nonlinear)\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        G = self._logistic_transition(z, self.gamma_, self.threshold_)\n",
    "        y_pred = X_linear @ self.beta_linear_ + G * (X_nonlinear @ self.beta_nonlinear_)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "# Función auxiliar mejorada\n",
    "def fit_str_model(y, max_lags=4, variable_name='TB3MS'):\n",
    "    \"\"\"\n",
    "    Función conveniente para ajustar STR\n",
    "    \"\"\"\n",
    "    model = SmoothThresholdRegression()\n",
    "    model.fit_from_series(y, max_lags=max_lags)\n",
    "    \n",
    "    var_names = ['C'] + [f'{variable_name}(-{i})' for i in range(1, max_lags + 1)]\n",
    "    model.summary(var_names, var_names, variable_name=variable_name)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce588a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observaciones originales: 202\n",
      "Observaciones utilizadas: 198\n",
      "Realizando búsqueda en grilla para valores iniciales...\n",
      "Mejores valores iniciales: gamma=3.5622, threshold=7.9967, SSR=85.284020\n",
      "Comenzando optimización...\n",
      "Convergencia alcanzada después de 31 iteraciones\n",
      "================================================================================\n",
      "Dependent Variable: TB3MS\n",
      "Method: Smooth Threshold Regression\n",
      "Transition function: Logistic\n",
      "Date: 06/20/25   Time: 19:16\n",
      "Included observations: 198\n",
      "Threshold variable: TB3MS(-1)\n",
      "Starting values: Grid search with concentrated regression coefficients\n",
      "Ordinary standard errors & covariance using outer product of gradients\n",
      "================================================================================\n",
      "\n",
      "Threshold Variables (linear part)\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "C               0.008217     5.675180     0.001448     0.9988  \n",
      "TB3MS(-1)       1.460900     1.927849     0.757787     0.4495  \n",
      "TB3MS(-2)       -0.903354    2.281824     -0.395891    0.6926  \n",
      "TB3MS(-3)       0.832495     2.797481     0.297587     0.7664  \n",
      "TB3MS(-4)       -0.381796    2.597222     -0.147002    0.8833  \n",
      "\n",
      "Threshold Variables (nonlinear part)\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "C               -108.577742  19272.762161 -0.005634    0.9955  \n",
      "TB3MS(-1)       10.701985    1266.538001  0.008450     0.9933  \n",
      "TB3MS(-2)       -1.750807    663.608340   -0.002638    0.9979  \n",
      "TB3MS(-3)       -1.814397    275.777217   -0.006579    0.9948  \n",
      "TB3MS(-4)       -0.285695    449.801369   -0.000635    0.9995  \n",
      "\n",
      "Slopes\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "SLOPE           4.244694     2387016746065637.000000 1.78e-15     1.0000  \n",
      "\n",
      "Thresholds\n",
      "\n",
      "Variable        Coefficient  Std. Error   t-Statistic  Prob.   \n",
      "\n",
      "THRESHOLD       13.301865    472053887092931.375000 2.82e-14     1.0000  \n",
      "\n",
      "R-squared         0.958611        Mean dependent var    5.354697    \n",
      "Adjusted R-squared0.956163        S.D. dependent var    2.769797    \n",
      "S.E. of regression0.579917        Akaike info criterion 357.752316  \n",
      "Sum squared resid 62.552595       Schwarz criterion     397.211521  \n",
      "Log likelihood    -166.876158     Hannan-Quinn criter.  373.724091  \n",
      "F-statistic       391.633243      Durbin-Watson stat    1.749136    \n",
      "Prob(F-statistic) 0.000000    \n"
     ]
    }
   ],
   "source": [
    "# # Para replicar los resultados de EViews más cercanamente:\n",
    "# model = SmoothThresholdRegression()\n",
    "\n",
    "# # Si tienes los datos exactos de EViews:\n",
    "# model.fit_from_series(tu_serie_tb3ms, max_lags=4)\n",
    "\n",
    "# O usando la función conveniente:\n",
    "model = fit_str_model(y, max_lags=4, variable_name='TB3MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9fc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tools.numdiff import approx_hess\n",
    "from numpy.linalg import inv\n",
    "import pandas as pd\n",
    "\n",
    "class SmoothThresholdRegressionEViewsStyle:\n",
    "    def __init__(self, y, X, Z):\n",
    "        \"\"\"\n",
    "        y: array (n,), variable dependiente\n",
    "        X: array (n, p), regresores (constante + lags)\n",
    "        Z: array (n,), variable de umbral (ej. TB3MS(-1))\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.Z = Z\n",
    "        self.n, self.p = X.shape\n",
    "\n",
    "    def logistic(self, gamma, c):\n",
    "        return expit(gamma * (self.Z - c))\n",
    "\n",
    "    def ssr_concentrated(self, gamma, c):\n",
    "        G = self.logistic(gamma, c)\n",
    "        X_aug = np.hstack([self.X, (G[:, None] * self.X)])\n",
    "        coef, *_ = np.linalg.lstsq(X_aug, self.y, rcond=None)\n",
    "        resid = self.y - X_aug @ coef\n",
    "        return np.sum(resid**2)\n",
    "\n",
    "    def grid_search(self, gamma_grid, c_grid):\n",
    "        best_ssr = np.inf\n",
    "        for gamma in gamma_grid:\n",
    "            for c in c_grid:\n",
    "                ssr = self.ssr_concentrated(gamma, c)\n",
    "                if ssr < best_ssr:\n",
    "                    best_ssr = ssr\n",
    "                    best = (gamma, c)\n",
    "        return best\n",
    "\n",
    "    def neg_loglike(self, params):\n",
    "        beta = params[0:self.p]\n",
    "        alpha = params[self.p:2*self.p]\n",
    "        gamma, c, log_sigma = params[2*self.p:]\n",
    "        sigma = np.exp(log_sigma)\n",
    "        G = self.logistic(gamma, c)\n",
    "        mu = self.X @ beta + G * (self.X @ alpha)\n",
    "        resid = self.y - mu\n",
    "        ll = -0.5 * np.log(2*np.pi*sigma**2) - 0.5 * (resid**2) / sigma**2\n",
    "        return -ll.sum()\n",
    "\n",
    "    def fit(self):\n",
    "        # 1. grid search\n",
    "        gamma_grid = np.logspace(2, 3, 20)  # EViews-style\n",
    "        c_grid = np.linspace(self.Z.min(), self.Z.max(), 100)\n",
    "        gamma0, c0 = self.grid_search(gamma_grid, c_grid)\n",
    "\n",
    "        # 2. OLS at best (gamma, c)\n",
    "        G0 = self.logistic(gamma0, c0)\n",
    "        X_aug = np.hstack([self.X, (G0[:, None] * self.X)])\n",
    "        coef0, *_ = np.linalg.lstsq(X_aug, self.y, rcond=None)\n",
    "        beta0 = coef0[:self.p]\n",
    "        alpha0 = coef0[self.p:]\n",
    "        resid0 = self.y - X_aug @ coef0\n",
    "        log_sigma0 = np.log(np.std(resid0))\n",
    "\n",
    "        # 3. optimize\n",
    "        start = np.r_[beta0, alpha0, gamma0, c0, log_sigma0]\n",
    "        res = minimize(self.neg_loglike, start, method=\"bfgs\", options={\"gtol\": 1e-6, \"maxiter\": 500})\n",
    "\n",
    "        # 4. resultados\n",
    "        self.params = res.x\n",
    "        self.llf = -self.neg_loglike(self.params)\n",
    "        self.aic = -2*self.llf + 2*len(self.params)\n",
    "        self.bic = -2*self.llf + np.log(self.n)*len(self.params)\n",
    "\n",
    "        # 5. matriz de varianza y errores estándar\n",
    "        hess = approx_hess(self.params, self.neg_loglike)\n",
    "        self.cov = inv(hess)\n",
    "        self.se = np.sqrt(np.diag(self.cov))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def summary(self):\n",
    "        p = self.p\n",
    "        names = [\"C\"] + [f\"X{i}\" for i in range(1, p)]\n",
    "        index = names + [f\"NL_{n}\" for n in names] + [\"Slope (γ)\", \"Threshold (c)\", \"log(σ)\"]\n",
    "        coefs = self.params\n",
    "        ses = self.se\n",
    "        tstats = coefs / ses\n",
    "        pvals = 2 * (1 - np.abs(tstats) / np.sqrt(2*np.pi))  # aprox z-test\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"Variable\": index,\n",
    "            \"Coef.\": coefs,\n",
    "            \"Std.Err.\": ses,\n",
    "            \"t-Stat\": tstats,\n",
    "            \"P>|t|\": pvals\n",
    "        })\n",
    "\n",
    "        print(\"\\nSmooth Threshold Regression (EViews-style)\")\n",
    "        print(f\"LogLik = {self.llf:.4f}, AIC = {self.aic:.4f}, BIC = {self.bic:.4f}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6195799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smooth Threshold Regression (EViews-style)\n",
      "LogLik = -168.0010, AIC = 362.0019, BIC = 404.7494\n",
      "         Variable      Coef.   Std.Err.      t-Stat       P>|t|\n",
      "0               C   0.033923   0.099211    0.341924    1.727184\n",
      "1              X1   1.462116   0.065515   22.317298  -15.806628\n",
      "2              X2  -0.920977   0.105744   -8.709488   -4.949166\n",
      "3              X3   0.871200   0.107591    8.097348   -4.460749\n",
      "4              X4  -0.409867   0.065566   -6.251254   -2.987779\n",
      "5            NL_C -65.041391   9.729700   -6.684830   -3.333722\n",
      "6           NL_X1   6.806317   0.985990    6.903027   -3.507819\n",
      "7           NL_X2  -1.034145   0.331422   -3.120329   -0.489662\n",
      "8           NL_X3  -1.525527   0.238500   -6.396349   -3.103548\n",
      "9           NL_X4  -0.262261   0.191384   -1.370340    0.906627\n",
      "10      Slope (γ)  99.966223  89.340682    1.118933    1.107221\n",
      "11  Threshold (c)  12.830669   0.015331  836.892222 -665.743383\n",
      "12         log(σ)  -0.570449   0.050261  -11.349712   -7.055760\n"
     ]
    }
   ],
   "source": [
    "y = df_sub[\"tb3ms\"].astype(float).dropna()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# y debe ser un pandas Series con índice de tiempo\n",
    "# por ejemplo:\n",
    "# y = df[\"tb3ms\"]\n",
    "\n",
    "# 1. Crear 4 lags\n",
    "lags = pd.concat([y.shift(i) for i in range(1, 5)], axis=1)\n",
    "lags.columns = [f\"y(-{i})\" for i in range(1, 5)]\n",
    "\n",
    "# 2. Eliminar los valores faltantes\n",
    "data = pd.concat([y, lags], axis=1).dropna()\n",
    "y_clean = data.iloc[:, 0].values                     # (n,)\n",
    "X = sm.add_constant(data.iloc[:, 1:].values)         # (n, 5) → const + 4 lags\n",
    "Z = data[\"y(-1)\"].values                             # (n,)\n",
    "\n",
    "\n",
    "model = SmoothThresholdRegressionEViewsStyle(y_clean, X, Z)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf81616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothThresholdRegressionMultiStart:\n",
    "    def __init__(self, y, X, Z, top_n=5):\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.Z = Z\n",
    "        self.n, self.p = X.shape\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def logistic(self, gamma, c):\n",
    "        return expit(gamma * (self.Z - c))\n",
    "\n",
    "    def ssr_concentrated(self, gamma, c):\n",
    "        G = self.logistic(gamma, c)\n",
    "        X_aug = np.hstack([self.X, G[:, None] * self.X])\n",
    "        coef, *_ = np.linalg.lstsq(X_aug, self.y, rcond=None)\n",
    "        resid = self.y - X_aug @ coef\n",
    "        return np.sum(resid ** 2), coef\n",
    "\n",
    "    def neg_loglike_eviews(self, params):\n",
    "        beta = params[0:self.p]\n",
    "        alpha = params[self.p:2*self.p]\n",
    "        gamma, c = params[2*self.p:2*self.p+2]\n",
    "\n",
    "        G = self.logistic(gamma, c)\n",
    "        mu = self.X @ beta + G * (self.X @ alpha)\n",
    "        resid = self.y - mu\n",
    "        SSR = np.sum(resid**2)\n",
    "        n = len(resid)\n",
    "        return -(-0.5 * n * np.log(SSR / n))\n",
    "\n",
    "    def fit(self):\n",
    "        # 1. grid de búsqueda sobre gamma y c\n",
    "        gamma_grid = np.logspace(2, 3, 25)  # 100 a 1000\n",
    "        c_grid = np.linspace(self.Z.min(), self.Z.max(), 100)\n",
    "\n",
    "        candidates = []\n",
    "        for gamma in gamma_grid:\n",
    "            for c in c_grid:\n",
    "                ssr, coef = self.ssr_concentrated(gamma, c)\n",
    "                candidates.append((ssr, gamma, c, coef))\n",
    "\n",
    "        # 2. seleccionar los N mejores puntos\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        best_models = candidates[:self.top_n]\n",
    "\n",
    "        best_ll = -np.inf\n",
    "        best_result = None\n",
    "\n",
    "        for ssr, gamma0, c0, coef0 in best_models:\n",
    "            beta0 = coef0[:self.p]\n",
    "            alpha0 = coef0[self.p:]\n",
    "            log_sigma0 = np.log(np.std(self.y - self.X @ beta0))\n",
    "\n",
    "            start = np.r_[beta0, alpha0, gamma0, c0, log_sigma0]\n",
    "            res = minimize(self.neg_loglike, start, method=\"bfgs\", options={\"gtol\": 1e-6, \"maxiter\": 500})\n",
    "            llf = -self.neg_loglike(res.x)\n",
    "\n",
    "            if llf > best_ll:\n",
    "                best_ll = llf\n",
    "                best_result = res\n",
    "\n",
    "        # guardar resultados del mejor\n",
    "        self.params = best_result.x\n",
    "        self.llf = best_ll\n",
    "        self.aic = -2 * best_ll + 2 * len(self.params)\n",
    "        self.bic = -2 * best_ll + np.log(self.n) * len(self.params)\n",
    "\n",
    "        hess = approx_hess(self.params, self.neg_loglike)\n",
    "        self.cov = inv(hess)\n",
    "        self.se = np.sqrt(np.diag(self.cov))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def summary(self):\n",
    "        p = self.p\n",
    "        names = [\"C\"] + [f\"X{i}\" for i in range(1, p)]\n",
    "        index = names + [f\"NL_{n}\" for n in names] + [\"Slope (γ)\", \"Threshold (c)\", \"log(σ)\"]\n",
    "        coefs = self.params\n",
    "        ses = self.se\n",
    "        tstats = coefs / ses\n",
    "        pvals = 2 * (1 - np.abs(tstats) / np.sqrt(2 * np.pi))  # aprox normal\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"Variable\": index,\n",
    "            \"Coef.\": coefs,\n",
    "            \"Std.Err.\": ses,\n",
    "            \"t-Stat\": tstats,\n",
    "            \"P>|t|\": pvals\n",
    "        })\n",
    "\n",
    "        print(\"\\nSmooth Threshold Regression (Multi-start)\")\n",
    "        print(f\"LogLik = {self.llf:.4f}, AIC = {self.aic:.4f}, BIC = {self.bic:.4f}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5504deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smooth Threshold Regression (Multi-start)\n",
      "LogLik = -168.0010, AIC = 362.0019, BIC = 404.7494\n",
      "         Variable       Coef.   Std.Err.       t-Stat        P>|t|\n",
      "0               C    0.033923   0.099212     0.341921     1.727187\n",
      "1              X1    1.462116   0.065516    22.317006   -15.806395\n",
      "2              X2   -0.920977   0.105747    -8.709270    -4.948992\n",
      "3              X3    0.871200   0.107593     8.097209    -4.460638\n",
      "4              X4   -0.409867   0.065566    -6.251241    -2.987768\n",
      "5            NL_C  -65.041571   9.741061    -6.677052    -3.327517\n",
      "6           NL_X1    6.806335   0.987251     6.894226    -3.500797\n",
      "7           NL_X2   -1.034148   0.331568    -3.118964    -0.488573\n",
      "8           NL_X3   -1.525528   0.238585    -6.394080    -3.101738\n",
      "9           NL_X4   -0.262262   0.191427    -1.370033     0.906872\n",
      "10      Slope (γ)  133.349405  90.426620     1.474670     0.823384\n",
      "11  Threshold (c)   12.826329   0.008654  1482.185499 -1180.612926\n",
      "12         log(σ)   -0.570449   0.050268   -11.348067    -7.054447\n"
     ]
    }
   ],
   "source": [
    "y = df_sub[\"tb3ms\"].astype(float).dropna()\n",
    "\n",
    "lags = pd.concat([y.shift(i) for i in range(1, 5)], axis=1).dropna()\n",
    "lags.columns = [f\"y(-{i})\" for i in range(1, 5)]\n",
    "common = y.index.intersection(lags.index)\n",
    "y2 = y.loc[common].values\n",
    "X = sm.add_constant(lags.loc[common].values)\n",
    "Z = lags.loc[common, \"y(-1)\"].values\n",
    "\n",
    "model = SmoothThresholdRegressionMultiStart(y2, X, Z)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7755fba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SmoothThresholdRegressionMultiStart' object has no attribute 'neg_loglike_eviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m model_ev \u001b[38;5;241m=\u001b[39m SmoothThresholdRegressionMultiStart(y2, X, Z)\n\u001b[0;32m     12\u001b[0m res_ev \u001b[38;5;241m=\u001b[39m minimize(model_ev\u001b[38;5;241m.\u001b[39mneg_loglike_eviews, start_ev, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m---> 13\u001b[0m loglik \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_loglike_eviews\u001b[49m(start_ev)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogLik estilo EViews con parámetros exactos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloglik\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SmoothThresholdRegressionMultiStart' object has no attribute 'neg_loglike_eviews'"
     ]
    }
   ],
   "source": [
    "p = X.shape[1]\n",
    "beta_ev = np.array([0.119095, 1.612695, -1.126949, 0.823023, -0.326430])\n",
    "alpha_ev = np.array([5.068129, -0.820449, 0.810701, -0.513702, 0.079634])\n",
    "gamma_ev = 223.7439\n",
    "c_ev = 9.612046\n",
    "log_sigma_ev = np.log(0.653211)\n",
    "\n",
    "start_ev = np.r_[beta_ev, alpha_ev, gamma_ev, c_ev, log_sigma_ev]\n",
    "\n",
    "# Volvé a usar la clase original (sin grid), solo BFGS\n",
    "model_ev = SmoothThresholdRegressionMultiStart(y2, X, Z)\n",
    "res_ev = minimize(model_ev.neg_loglike_eviews, start_ev, method=\"bfgs\", options={\"maxiter\": 0})\n",
    "loglik = -model.neg_loglike_eviews(start_ev)\n",
    "print(f\"LogLik estilo EViews con parámetros exactos: {loglik:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9cb3ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SmoothThresholdRegressionMultiStart' object has no attribute 'neg_loglike_eviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loglik \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_loglike_eviews\u001b[49m(start_ev)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogLik estilo EViews con parámetros exactos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloglik\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SmoothThresholdRegressionMultiStart' object has no attribute 'neg_loglike_eviews'"
     ]
    }
   ],
   "source": [
    "loglik = -model.neg_loglike_eviews(start_ev)\n",
    "print(f\"LogLik estilo EViews con parámetros exactos: {loglik:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
